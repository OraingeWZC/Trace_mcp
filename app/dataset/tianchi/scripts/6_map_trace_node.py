# scripts/6_map_trace_nodes.py
# -*- coding: utf-8 -*-
"""
Trace èŠ‚ç‚¹æ˜ å°„æ¸…æ´—å·¥å…·
åŠŸèƒ½ï¼š
1. è¯»å–çŽ°æœ‰çš„ Trace CSV æ–‡ä»¶ã€‚
2. åŠ è½½ ecs_mapping_index.json æ˜ å°„è¡¨ã€‚
3. å°† Trace ä¸­çš„ NodeName (IP/K8sName) ç»Ÿä¸€æ›¿æ¢ä¸ºç‰©ç† Instance IDã€‚
4. åŽŸæœ‰çš„ NodeName ä¼šè¢«å¤‡ä»½åˆ°æ–°åˆ— RawNodeName ä¸­ã€‚
"""

import os
import csv
import json
import argparse
import sys
from collections import defaultdict

# # å¢žå¤§ CSV å­—æ®µé™åˆ¶ï¼Œé˜²æ­¢ Trace è¿‡é•¿æŠ¥é”™
# csv.field_size_limit(sys.maxsize)

def load_mapping(json_path):
    """åŠ è½½æ˜ å°„æ–‡ä»¶ï¼Œæž„å»ºæŸ¥æ‰¾è¡¨"""
    if not os.path.exists(json_path):
        print(f"âŒ é”™è¯¯: æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨ {json_path}")
        return None
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # åˆå¹¶ lookup table
    # ä¼˜å…ˆåŒ¹é…ç²¾ç¡®çš„ key (IP æˆ– Hostname)
    lookup = {}
    if "ip_to_id" in data:
        lookup.update(data["ip_to_id"])
    if "name_to_id" in data:
        lookup.update(data["name_to_id"])
    
    print(f"âœ… å·²åŠ è½½æ˜ å°„è¡¨ï¼ŒåŒ…å« {len(lookup)} ä¸ªæ¡ç›®")
    return lookup

def process_file(input_path, output_path, mapping):
    """å¤„ç†å•ä¸ª CSV æ–‡ä»¶"""
    if not os.path.exists(input_path):
        print(f"âš ï¸ è·³è¿‡: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ {input_path}")
        return

    print(f"ðŸ”„ æ­£åœ¨å¤„ç†: {input_path} ...")
    
    mapped_count = 0
    total_count = 0
    missed_nodes = set()

    with open(input_path, 'r', encoding='utf-8', newline='') as f_in, \
         open(output_path, 'w', encoding='utf-8', newline='') as f_out:
        
        reader = csv.DictReader(f_in)
        # 1. ä¿®æ”¹è¡¨å¤´ï¼šæŠŠ NodeName æ”¾åˆ°åŽŸæ¥çš„ä½ç½®ï¼Œæ–°å¢ž RawNodeName
        fieldnames = list(reader.fieldnames)
        if "RawNodeName" not in fieldnames:
            # æ’å…¥åˆ° NodeName åŽé¢ï¼Œæˆ–è€…æœ€åŽ
            if "NodeName" in fieldnames:
                idx = fieldnames.index("NodeName")
                fieldnames.insert(idx + 1, "RawNodeName")
            else:
                fieldnames.append("RawNodeName")
        
        writer = csv.DictWriter(f_out, fieldnames=fieldnames)
        writer.writeheader()
        
        for row in reader:
            total_count += 1
            original_name = row.get("NodeName", "").strip()
            
            # å¤‡ä»½åŽŸå§‹åå­—
            row["RawNodeName"] = original_name
            
            # æŸ¥æ‰¾æ˜ å°„
            # 1. å°è¯•ç›´æŽ¥åŒ¹é…
            target_id = mapping.get(original_name)
            
            # 2. å°è¯•åŽ»æŽ‰ 'cn-qingdao.' å‰ç¼€åŒ¹é…
            if not target_id and original_name.startswith("cn-qingdao."):
                short_name = original_name.replace("cn-qingdao.", "")
                target_id = mapping.get(short_name)
            
            # 3. å°è¯•ä½œä¸º IP åŒ¹é… (å¦‚æžœåŒ…å«åœ¨ HostName é‡Œ)
            # (è¿™ä¸€æ­¥è§†æƒ…å†µè€Œå®šï¼Œå¦‚æžœä½ çš„ mapping è¶³å¤Ÿå…¨ï¼Œé€šå¸¸ä¸éœ€è¦æ¨¡ç³ŠåŒ¹é…)
            
            if target_id:
                row["NodeName"] = target_id
                mapped_count += 1
            else:
                if original_name and original_name.lower() != "none":
                    missed_nodes.add(original_name)
            
            writer.writerow(row)
            
            if total_count % 50000 == 0:
                print(f"   å·²å¤„ç† {total_count} è¡Œ...", end='\r')

    print(f"\n   âœ… å®Œæˆ! æ˜ å°„æˆåŠŸçŽ‡: {mapped_count}/{total_count} ({mapped_count/total_count*100:.1f}%)")
    if missed_nodes:
        print(f"   âš ï¸ æœªå‘½ä¸­æ˜ å°„çš„èŠ‚ç‚¹ (Top 5): {list(missed_nodes)[:5]}")
        # å¯ä»¥æŠŠ missed_nodes å†™å…¥æ—¥å¿—æ–¹ä¾¿æŽ’æŸ¥

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mapping", default="data/ecs_mapping_index.json", help="æ˜ å°„æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--inputs", nargs="+", 
                        default=[
                            # "data/NormalData/normal_traces.csv",
                            # "data/ServiceFault/all_fault_traces.csv",
                            # "data/NodeFault/all_fault_traces.csv"
                            "data/NormalData/normal_traces2e5_30s_6h.csv"
                        ],
                        help="éœ€è¦å¤„ç†çš„ CSV æ–‡ä»¶åˆ—è¡¨")
    parser.add_argument("--suffix", default="_mapped", help="è¾“å‡ºæ–‡ä»¶åŽç¼€ (ä¾‹å¦‚ _mapped)")
    args = parser.parse_args()

    mapping = load_mapping(args.mapping)
    if not mapping:
        return

    for input_file in args.inputs:
        # æž„é€ è¾“å‡ºæ–‡ä»¶å: data/xxx.csv -> data/xxx_mapped.csv
        base, ext = os.path.splitext(input_file)
        output_file = f"{base}{args.suffix}{ext}"
        
        process_file(input_file, output_file, mapping)

if __name__ == "__main__":
    main()