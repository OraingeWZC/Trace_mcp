import warnings
warnings.filterwarnings("ignore")

import os
import argparse
import shutil
import sqlite3
import pandas as pd
import numpy as np
from tqdm import tqdm

# å¼•å…¥é¡¹ç›®ä¾èµ–
from tracegnn.data.trace_graph import df_to_trace_graphs, TraceGraphIDManager
from tracegnn.data.trace_graph_db import TraceGraphDB, BytesSqliteDB
from tracegnn.utils.host_state import host_state_vector, DEFAULT_METRICS, DISK_METRICS

# ================= é…ç½®åŒºåŸŸ =================
# é»˜è®¤çš„æ•°æ®é›†æ ¹ç›®å½•ï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹ï¼Œæˆ–è€…é€šè¿‡å‘½ä»¤è¡Œå‚æ•° --root ä¼ å…¥
DEFAULT_DATASET_ROOT = 'dataset/dataset_topo' 
INFRA_FILENAME = 'merged_all_infra.csv'
# ===========================================

def flexible_load_trace_csv(input_path: str) -> pd.DataFrame:
    """æ›´çµæ´»åœ°åŠ è½½CSVæ–‡ä»¶"""
    if not os.path.exists(input_path):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return pd.DataFrame()
        
    try:
        df = pd.read_csv(input_path)
        # ç±»å‹è½¬æ¢å…œåº•
        if 'Duration' in df.columns:
            df['Duration'] = pd.to_numeric(df['Duration'], errors='coerce')
        if 'StartTimeMs' in df.columns:
            df['StartTime'] = pd.to_numeric(df['StartTimeMs'], errors='coerce')
        if 'Anomaly' in df.columns:
            df['Anomaly'] = df['Anomaly'].astype(bool)
        return df
    except Exception as e:
        print(f"åŠ è½½CSVå‡ºé”™ {input_path}: {e}")
        return pd.DataFrame()

def load_infra_data_from_parent(dataset_root: str):
    """
    ä»æ•°æ®é›†æ ¹ç›®å½•çš„ä¸Šä¸€çº§æŸ¥æ‰¾æŒ‡æ ‡æ•°æ®
    ä¾‹å¦‚: root = 'dataset/dataset_demo' -> æŸ¥æ‰¾ 'dataset/merged_all_infra.csv'
    """
    parent_dir = os.path.dirname(dataset_root.rstrip(os.path.sep))
    infra_path = os.path.join(parent_dir, INFRA_FILENAME)
    
    # å¦‚æœä¸Šä¸€çº§æ‰¾ä¸åˆ°ï¼Œå°è¯•ä¸Šä¸€çº§çš„ infra ç›®å½•
    if not os.path.exists(infra_path):
        infra_path_alt = os.path.join(parent_dir, 'infra', INFRA_FILENAME)
        if os.path.exists(infra_path_alt):
            infra_path = infra_path_alt
    
    if not os.path.exists(infra_path):
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æŒ‡æ ‡æ•°æ®æ–‡ä»¶ã€‚æœŸæœ›è·¯å¾„: {infra_path}")
        return None
    
    print(f"âœ… å·²åŠ è½½æŒ‡æ ‡æ•°æ®: {infra_path}")
    
    # åŠ è½½å¹¶æ„å»ºç´¢å¼•
    try:
        df = pd.read_csv(infra_path)
        if 'timeMs' not in df.columns or 'kubernetes_node' not in df.columns:
            return None
            
        # ç¡®ä¿åŒ…å«éœ€è¦çš„æŒ‡æ ‡åˆ—
        all_metrics = list(set(DEFAULT_METRICS + DISK_METRICS))
        for m in all_metrics:
            if m not in df.columns:
                df[m] = np.nan
        
        try:
            df['timeMs'] = df['timeMs'].astype(np.int64)
        except:
            if 'time' in df.columns:
                df['timeMs'] = pd.to_datetime(df['time']).astype('int64') // 10**6
        
        cols = ['timeMs', 'kubernetes_node'] + [c for c in all_metrics if c in df.columns]
        df = df[cols].dropna(subset=['timeMs', 'kubernetes_node'])
        
        host_idx = {}
        for host, g in df.groupby('kubernetes_node'):
            lg = g.sort_values('timeMs')
            host_idx[str(host)] = {
                'timeMs': lg['timeMs'].to_numpy(dtype=np.int64),
                'metrics': {m: lg[m].to_numpy(dtype=np.float64) for m in lg.columns if m not in ('timeMs', 'kubernetes_node')}
            }
        return host_idx
    except Exception as e:
        print(f"è§£ææŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
        return None

def precompute_host_states(trace_graphs, infra_index, id_manager, W=3):
    """é¢„è®¡ç®— SInfra æ•°æ®å¹¶æ³¨å…¥åˆ°å›¾å¯¹è±¡ä¸­"""
    if infra_index is None:
        return

    metrics = list(DEFAULT_METRICS)
    # å¦‚æœéœ€è¦ç£ç›˜æŒ‡æ ‡ï¼Œå–æ¶ˆæ³¨é‡Š
    # for m in DISK_METRICS:
    #     if m not in metrics: metrics.append(m)
    per_metric_dims = 3

    success_cnt = 0
    for graph in tqdm(trace_graphs, desc="é¢„è®¡ç®— SInfra (HostState)"):
        try:
            # 1. è®¡ç®— t0
            st = graph.root.spans[0].start_time if (graph.root and graph.root.spans) else None
            if isinstance(st, (int, float)):
                v = float(st)
                t0_ms = int(v if v > 1e12 else v * 1000.0)
            else:
                t0_ms = 0
            t0_min_ms = (t0_ms // 60000) * 60000

            # 2. æŸ¥æ‰¾æ¶‰åŠçš„ä¸»æœº
            host_ids = set(node.host_id for _, node in graph.iter_bfs() if node.host_id and node.host_id > 0)
            
            # 3. è®¡ç®—å‘é‡
            host_state_map = {}
            for hid in host_ids:
                hname = id_manager.host_id.rev(int(hid))
                if hname:
                    vec = host_state_vector(hname, infra_index, t0_min_ms, metrics=metrics, W=W, per_metric_dims=per_metric_dims)
                    if vec is not None:
                        host_state_map[hid] = vec
            
            if host_state_map:
                graph.data['precomputed_host_state'] = host_state_map
                success_cnt += 1
        except Exception:
            continue
    
    print(f"  -> SInfra é¢„è®¡ç®—å®Œæˆ: {success_cnt}/{len(trace_graphs)} ä¸ª Trace åŒ…å«ä¸»æœºæŒ‡æ ‡æ•°æ®")

def process_split(split_name, dataset_root, id_manager, infra_index, processed_df=None):
    """å¤„ç†å•ä¸ªæ•°æ®é›†åˆ†ç‰‡ (train/val/test)"""
    raw_csv = os.path.join(dataset_root, 'raw', f'{split_name}.csv')
    out_dir = os.path.join(dataset_root, 'processed', split_name)
    
    if not os.path.exists(raw_csv):
        print(f"è·³è¿‡ {split_name}: æ–‡ä»¶ä¸å­˜åœ¨ {raw_csv}")
        return

    print(f"\n=== å¤„ç† {split_name} é›† ===")
    os.makedirs(out_dir, exist_ok=True)
    
    # 1. åŠ è½½æ•°æ®
    if processed_df is not None:
        df = processed_df
        print(f"  ä½¿ç”¨é¢„å¤„ç†åçš„ DataFrame ({len(df)} è¡Œ)")
    else:
        df = flexible_load_trace_csv(raw_csv)
        print(f"  åŠ è½½ CSV: {len(df)} è¡Œ")

    if df.empty:
        return

    # 2. è½¬æ¢ä¸ºå›¾
    trace_graphs = df_to_trace_graphs(
        df=df,
        id_manager=id_manager,
        min_node_count=2,
        max_node_count=100,
        summary_file=None,
        merge_spans=False
    )
    
    if not trace_graphs:
        print("  æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ Trace Graph")
        return

    # 3. é¢„è®¡ç®— SInfra
    precompute_host_states(trace_graphs, infra_index, id_manager)

    # 4. å†™å…¥æ•°æ®åº“
    db_path = os.path.join(out_dir, "_bytes.db")
    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(db_path):
        open(db_path, 'a').close()
        
    db = TraceGraphDB(BytesSqliteDB(out_dir, write=True))
    try:
        with db.write_batch():
            for graph in trace_graphs:
                # ç¡®ä¿ç±»å‹å®‰å…¨
                if hasattr(graph, 'root_cause') and graph.root_cause is None: graph.root_cause = 0
                if hasattr(graph, 'fault_category') and graph.fault_category is None: graph.fault_category = 0
                db.add(graph)
        db.commit()
        print(f"  âœ… æˆåŠŸå†™å…¥ {len(trace_graphs)} ä¸ªå›¾åˆ° {split_name} æ•°æ®åº“")
    finally:
        db.close()

def main():
    parser = argparse.ArgumentParser(description="Traceæ•°æ®å¤„ç†æµæ°´çº¿ (v2)")
    parser.add_argument('--root', type=str, default=DEFAULT_DATASET_ROOT, 
                        help='æ•°æ®é›†æ ¹ç›®å½• (ä¾‹å¦‚: dataset/dataset_demo)')
    args = parser.parse_args()
    
    dataset_root = args.root
    print(f"ğŸš€ å¼€å§‹å¤„ç†æ•°æ®æµï¼Œæ ¹ç›®å½•: {dataset_root}")
    
    # 0. å‡†å¤‡ç›®å½•
    processed_root = os.path.join(dataset_root, 'processed')
    os.makedirs(processed_root, exist_ok=True)
    
    # 1. åŠ è½½æŒ‡æ ‡æ•°æ® (ä¼˜åŒ–ç‚¹: åªåŠ è½½ä¸€æ¬¡)
    infra_index = load_infra_data_from_parent(dataset_root)
    
    # 2. å»ºç«‹ç»Ÿä¸€çš„ ID æ˜ å°„ (Train + Val + Test)
    print("\n[æ­¥éª¤ 1/4] å»ºç«‹ç»Ÿä¸€ ID æ˜ å°„...")
    combined_dfs = []
    for split in ['train', 'val', 'test']:
        path = os.path.join(dataset_root, 'raw', f'{split}.csv')
        df = flexible_load_trace_csv(path)
        if not df.empty:
            combined_dfs.append(df)
            
    if not combined_dfs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• CSV æ•°æ®ï¼Œé€€å‡ºã€‚")
        return

    # ä½¿ç”¨ä¸´æ—¶ç›®å½•ç”Ÿæˆ IDï¼Œç„¶åç§»åŠ¨åˆ° processed
    temp_id_dir = os.path.join(dataset_root, 'temp_ids')
    os.makedirs(temp_id_dir, exist_ok=True)
    id_manager = TraceGraphIDManager(temp_id_dir)
    
    with id_manager:
        full_df = pd.concat(combined_dfs, ignore_index=True)
        for row in tqdm(full_df.itertuples(), total=len(full_df), desc="ç”Ÿæˆ ID"):
            id_manager.service_id.get_or_assign(getattr(row, 'ServiceName', '') or '')
            id_manager.operation_id.get_or_assign(getattr(row, 'OperationName', '') or '')
            id_manager.status_id.get_or_assign(str(getattr(row, 'StatusCode', '')) or '')
            # æ³¨æ„: FaultCategory å’Œ HostID ä¹Ÿä¼šåœ¨ df_to_trace_graphs ä¸­åŠ¨æ€æ·»åŠ 

    # å°† ID æ–‡ä»¶ä¿å­˜åˆ°æœ€ç»ˆç›®å½•
    id_manager.dump_to(processed_root)
    # é‡æ–°åˆå§‹åŒ–æŒ‡å‘æœ€ç»ˆç›®å½•çš„ manager
    id_manager = TraceGraphIDManager(processed_root)
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    if os.path.exists(temp_id_dir):
        shutil.rmtree(temp_id_dir)

    # 3. å¤„ç† Train å’Œ Val
    print("\n[æ­¥éª¤ 2/4] å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
    process_split('train', dataset_root, id_manager, infra_index)
    process_split('val', dataset_root, id_manager, infra_index)

    # 4. ç‰¹æ®Šå¤„ç†æµ‹è¯•é›† (æ˜ å°„ RootCause å’Œ FaultCategory)
    print("\n[æ­¥éª¤ 3/4] å¤„ç†æµ‹è¯•é›† (åŒ…å«æ•…éšœæ˜ å°„)...")
    test_csv_path = os.path.join(dataset_root, 'raw', 'test.csv')
    test_df = flexible_load_trace_csv(test_csv_path)
    
    if not test_df.empty:
        # å¤„ç†æ•…éšœæ˜ å°„é€»è¾‘
        print("  æ­£åœ¨æ‰§è¡Œæµ‹è¯•é›†æ•…éšœæ–‡æœ¬æ˜ å°„...")
        # ç¡®ä¿åˆ—å­˜åœ¨
        for col in ['RootCause', 'FaultCategory']:
            if col not in test_df.columns: test_df[col] = ''
            
        for idx, row in test_df.iterrows():
            if row.get('Anomaly'):
                rc_text = str(row.get('RootCause', '')).strip()
                fc_text = str(row.get('FaultCategory', '')).strip()
                
                # æ˜ å°„ RootCause -> ID
                mapped_id = None
                if fc_text.lower().startswith('node'):
                    rc_text = rc_text.replace('_', '-')
                    mapped_id = id_manager.host_id.get(rc_text)
                else:
                    rc_svc = rc_text.split('-')[0] if '-' in rc_text else rc_text
                    mapped_id = id_manager.service_id.get(rc_svc)
                
                test_df.at[idx, 'RootCause'] = mapped_id if mapped_id is not None else 0
                
                # æ˜ å°„ FaultCategory -> ID
                if fc_text:
                    fc_id = id_manager.fault_category.get_or_assign(fc_text)
                    test_df.at[idx, 'FaultCategory'] = fc_id
                else:
                    test_df.at[idx, 'FaultCategory'] = 0

        # å¤„ç†å¹¶å†™å…¥æµ‹è¯•é›†
        process_split('test', dataset_root, id_manager, infra_index, processed_df=test_df)

    # 5. æ”¶å°¾
    print("\n[æ­¥éª¤ 4/4] ä¿å­˜æœ€ç»ˆæ˜ å°„æ–‡ä»¶...")
    id_manager.dump_to(processed_root)
    
    print(f"\nâœ¨ æ‰€æœ‰å¤„ç†å®Œæˆï¼è¾“å‡ºç›®å½•: {processed_root}")

if __name__ == '__main__':
    main()