# train_sv_b.py (Fixed F1 Early Stop & Save)
# -*- coding: utf-8 -*-
import os, argparse, torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

# å¯¼å…¥ Scheme B çš„æ¨¡å—
from tc_utils import (
    set_seed, TraceDataset, collate_multi, vocab_sizes_from_meta,
    derive_keep_types, evaluate_detailed, print_per_class_reports,
    collect_per_class_reports, save_run_summary, save_ckpt
)
from tc_model import TraceClassifier

def main():
    ap = argparse.ArgumentParser("SV Scheme B Training (No Host Graph)")
    ap.add_argument("--data-root", default="dataset/tianchi/processed_0111")
    # ä¿®æ”¹ä¿å­˜è·¯å¾„ï¼Œæ–¹ä¾¿ç®¡ç†
    ap.add_argument("--save-dir",  default="dataset/tianchi/processed_0111/f1earlystop")
    ap.add_argument("--save_pt",   default="dataset/tianchi/processed_0111/f1earlystop/best_model.pt")
    
    ap.add_argument("--type_min_support", type=int, default=10)
    ap.add_argument("--epochs", type=int, default=20)
    ap.add_argument("--batch", type=int, default=64)
    ap.add_argument("--lr", type=float, default=1e-3)
    ap.add_argument("--seed", type=int, default=2025)
    ap.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    ap.add_argument("--no-progress", action="store_true", default=False)
    ap.add_argument("--early_stop_patience", type=int, default=10) # å»ºè®®ç¨å¾®è°ƒå¤§ä¸€ç‚¹ patience
    ap.add_argument("--early_stop_min_delta", type=float, default=1e-4)
    args = ap.parse_args()

    set_seed(args.seed)
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.save_dir, exist_ok=True)
    os.makedirs(os.path.dirname(args.save_pt), exist_ok=True)

    tr = os.path.join(args.data_root, "train.jsonl")
    va = os.path.join(args.data_root, "val.jsonl")
    te = os.path.join(args.data_root, "test.jsonl")
    
    api_sz, st_sz, node_sz, type_names, ctx_dim = vocab_sizes_from_meta(args.data_root)
    print(f"[Init] Vocab: API={api_sz}, Status={st_sz}, Node={node_sz}, Types={len(type_names)}, Ctx={ctx_dim}")

    ds_fit = TraceDataset(tr, task="multihead", fit_stats=True)
    stats  = ds_fit.stats
    keep_types = derive_keep_types(ds_fit.items, args.type_min_support)
    
    ds_tr = TraceDataset(tr, task="multihead", fit_stats=False, stats=stats, keep_types=keep_types)
    ds_va = TraceDataset(va, task="multihead", fit_stats=False, stats=stats, keep_types=keep_types)
    ds_te = TraceDataset(te, task="multihead", fit_stats=False, stats=stats, keep_types=keep_types)
    
    mk = lambda ds, shuf: DataLoader(ds, batch_size=args.batch, shuffle=shuf, collate_fn=collate_multi, num_workers=4)
    tr_loader, va_loader, te_loader = mk(ds_tr, True), mk(ds_va, False), mk(ds_te, False)

    device = torch.device(args.device)
    model = TraceClassifier(
        api_sz, st_sz, node_sz, 
        n_types=len(type_names), 
        ctx_dim=ctx_dim
    ).to(device)
    
    opt = torch.optim.AdamW(model.parameters(), lr=args.lr)
    bce = nn.BCEWithLogitsLoss()
    ce  = nn.CrossEntropyLoss()

    def run_epoch(loader, train=True):
        model.train(train)
        tot = 0.0
        iterator = tqdm(loader, leave=False) if not args.no_progress else loader
        for g, lab, _, _ in iterator:
            g = g.to(device)
            yb = lab["y_bin"].float().to(device)
            yc = lab["y_c3"].to(device)
            yt = lab["y_type"].to(device)
            mt = lab["m_type"].to(device)

            out = model(g)
            l1 = bce(out["logit_bin"], yb)
            # åœ¨ Scheme B ä¸­ y_c3 åªæœ‰ 0/1ï¼Œè¿™é‡Œå®é™…ä¸Šé‡å¤äº† l1ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™
            l2 = ce(out["logits_c3"], yc) 
            
            if mt.sum() > 0:
                l3 = (ce(out["logits_type"], yt) * mt).sum() / (mt.sum() + 1e-6)
            else:
                l3 = l1 * 0.0
            
            loss = 0.1 * l1 + 0.2 * l2 + 0.7 * l3

            if train:
                opt.zero_grad(); loss.backward(); 
                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
                opt.step()

            tot += float(loss.item())
        return tot / max(1, len(loader))

    # === [ä¿®å¤ 1] F1 æ˜¯è¶Šå¤§è¶Šå¥½ï¼Œæ‰€ä»¥åˆå§‹ best è¦è®¾ä¸º 0.0 (ä¸æ˜¯æ— ç©·å¤§) ===
    best = 0.0 
    best_state = None
    no_improve = 0

    print("ğŸš€ Start Training...")
    for ep in range(1, args.epochs + 1):
        trL = run_epoch(tr_loader, train=True)
        vaL = run_epoch(va_loader, train=False) # è¿™é‡Œçš„ vaL ä»…ä½œå‚è€ƒæ—¥å¿—
        
        # è®¡ç®— F1
        metrics = evaluate_detailed(model, va_loader, device, type_names, keep_types=keep_types)
        current_f1 = metrics["type_f1"]
        
        print(f"[Epoch {ep:02d}] Loss: {trL:.4f} | Val F1: {current_f1:.4f} (Best: {best:.4f})")

        # === [ä¿®å¤ 2] æ­£ç¡®çš„ F1 æ—©åœé€»è¾‘ ===
        if current_f1 > best + args.early_stop_min_delta:
            best = current_f1
            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}
            no_improve = 0
            print(f"   âœ¨ New Best F1! ({best:.4f})")
        else:
            no_improve += 1
            if args.early_stop_patience > 0 and no_improve >= args.early_stop_patience:
                print(f"ğŸ›‘ [EarlyStop] No F1 improvement for {no_improve} epochs. Stopping.")
                break

    # åŠ è½½æœ€ä½³æƒé‡è¿›è¡Œæµ‹è¯•
    if best_state is not None:
        print("ğŸ”™ Loading best model state for testing...")
        model.load_state_dict(best_state)
    else:
        print("âš ï¸ Warning: No best state found (maybe F1 never > 0?). Using last epoch state.")

    print("\n>>> Final Test <<<")
    metrics = evaluate_detailed(model, te_loader, device, type_names, keep_types=keep_types)
    print_per_class_reports(model, te_loader, device, type_names, keep_types=keep_types)

    # ä¿å­˜ç»“æœ
    reports = collect_per_class_reports(model, te_loader, device, type_names, keep_types=keep_types)
    
    save_run_summary(save_dir=args.save_dir, args=vars(args), data_root=args.data_root,
                     metrics_dict=metrics, reports_dict=reports,
                     stats=stats, type_names=type_names, keep_types=keep_types)
    
    # === [ä¿®å¤ 3] æ˜¾å¼ä¿å­˜æ¨¡å‹æƒé‡ ===
    save_ckpt(
        path=args.save_pt, 
        model_state=model.state_dict(), 
        stats=stats, 
        args_dict=vars(args), 
        type_names=type_names, 
        keep_types=keep_types
    )
    
    print(f"âœ… Training Finished. Model saved to: {args.save_pt}")

if __name__ == "__main__":
    main()