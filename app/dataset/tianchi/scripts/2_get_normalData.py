#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ­£å¸¸æ—¶æ®µæ•°æ®è·å–å·¥å…· (Baseline Data Fetcher) - ä¿®å¤ç‰ˆ

é€»è¾‘ï¼š
1. è¯»å– b_gt.csvï¼Œæ‰¾åˆ°æœ€æ—©çš„æ•…éšœæ—¶é—´ (Min Start Time)ã€‚
2. å®šä¹‰æ­£å¸¸æ—¶é—´çª—ï¼š[æœ€æ—©æ•…éšœæ—¶é—´ - 2å°æ—¶, æœ€æ—©æ•…éšœæ—¶é—´ - 1å°æ—¶]ã€‚
3. Metric: è·å–è¯¥æ—¶æ®µæ‰€æœ‰æ´»è·ƒ ECS çš„æ€§èƒ½æŒ‡æ ‡ (æ”¯æŒ --interval é‡é‡‡æ ·)ã€‚
4. Trace: è·å–è¯¥æ—¶æ®µ try_cast(statusCode as bigint) <= 1 çš„æ­£å¸¸ Traceã€‚
"""

import os
import sys
import json
import csv
import time
import argparse
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

import config

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
# 1. æŒ‡æ ‡å®šä¹‰
TARGET_METRICS = [
    "aggregate_node_net_receive_packages_errors_per_minute",
    "aggregate_node_tcp_inuse_total_num",
    "aggregate_node_tcp_alloc_total_num",
    "aggregate_node_cpu_usage",
    "aggregate_node_memory_usage",
    "aggregate_node_disk_io_usage"
]

# # 2. SLS é…ç½®
PROJECT_NAME = config.SLS_PROJECT_NAME
LOGSTORE_NAME = config.SLS_LOGSTORE_NAME
REGION = config.SLS_REGION

# 3. é‰´æƒé…ç½®
os.environ.setdefault("ALIBABA_CLOUD_ROLE_SESSION_NAME", "normal-data-fetcher")

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# å¯¼å…¥å·¥å…·åº“
try:
    from tools.paas_entity_tools import umodel_get_entities
    from tools.paas_data_tools import umodel_get_golden_metrics
    from tools.common import create_cms_client, execute_cms_query
    from tools.constants import REGION_ID, WORKSPACE_NAME
    from aliyun.log import LogClient, GetLogsRequest
    from alibabacloud_sts20150401.client import Client as StsClient
    from alibabacloud_sts20150401 import models as sts_models
    from alibabacloud_tea_openapi import models as open_api_models
except ImportError as e:
    print(f"âŒ ä¾èµ–ç¼ºå¤±: {e}")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NormalDataFetcher:
    def __init__(self, args):
        self.args = args
        self.output_dir = args.output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.cms_client = create_cms_client(REGION_ID)
        self.sls_client = self._init_sls_client()

    def _init_sls_client(self):
        """åˆå§‹åŒ– SLS å®¢æˆ·ç«¯ (å¸¦ STS)"""
        config = open_api_models.Config(
            access_key_id=os.environ["ALIBABA_CLOUD_ACCESS_KEY_ID"],
            access_key_secret=os.environ["ALIBABA_CLOUD_ACCESS_KEY_SECRET"],
            endpoint=f'sts.{REGION}.aliyuncs.com'
        )
        sts_client = StsClient(config)
        resp = sts_client.assume_role(sts_models.AssumeRoleRequest(
            role_arn=os.environ["ALIBABA_CLOUD_ROLE_ARN"],
            role_session_name="normal-fetcher",
            duration_seconds=3600
        ))
        creds = resp.body.credentials
        return LogClient(
            endpoint=f"{REGION}.log.aliyuncs.com",
            accessKeyId=creds.access_key_id,
            accessKey=creds.access_key_secret,
            securityToken=creds.security_token
        )

    def determine_time_window(self):
        """æ­¥éª¤ 1: ç¡®å®šæ­£å¸¸æ—¶é—´æ®µ"""
        logger.info(f"ğŸ“… æ­£åœ¨æ‰«æ {self.args.csv} è®¡ç®—åŸºå‡†æ—¶é—´...")
        min_ts = float('inf')
        
        with open(self.args.csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    ts = int(datetime.strptime(row['start_time'], '%Y-%m-%d %H:%M:%S').timestamp())
                    if ts < min_ts: min_ts = ts
                except: continue
        
        # å®šä¹‰ï¼šæœ€æ—©æ•…éšœå‰ 2å°æ—¶ ~ å‰ 1å°æ—¶
        end_time = min_ts - 3600
        start_time = end_time - 3600
        
        logger.info(f"âœ… é€‰å®šæ­£å¸¸æ—¶æ®µ: {datetime.fromtimestamp(start_time)} ~ {datetime.fromtimestamp(end_time)}")
        return start_time, end_time

    def fetch_metrics(self, start_ts, end_ts):
        """æ­¥éª¤ 2: è·å– Metric æ•°æ® (æ”¯æŒé‡é‡‡æ ·)"""
        logger.info("ğŸš€ [Metric] å¼€å§‹è·å–æ­£å¸¸æ—¶æ®µçš„èŠ‚ç‚¹æŒ‡æ ‡...")
        
        entity_query = {
            "domain": "acs",
            "entity_set_name": "acs.ecs.instance",
            "from_time": start_ts,
            "to_time": end_ts,
            "limit": 200
        }
        nodes_res = umodel_get_entities.invoke(entity_query)
        if not nodes_res or not nodes_res.data:
            logger.warning("   âš ï¸ æœªå‘ç°æ´»è·ƒèŠ‚ç‚¹")
            return

        nodes = nodes_res.data
        logger.info(f"   å‘ç° {len(nodes)} ä¸ªæ´»è·ƒèŠ‚ç‚¹ï¼Œæ­£åœ¨æ‹‰å–æŒ‡æ ‡...")
        if self.args.interval:
            logger.info(f"   â±ï¸ å·²å¯ç”¨é‡é‡‡æ ·: æ¯ {self.args.interval} ç§’èšåˆä¸€æ¡æ•°æ®")
        
        csv_path = os.path.join(self.output_dir, "normal_metrics.csv")
        headers = ['problem_id', 'fault_type', 'instance_id', 'timestamp'] + sorted(TARGET_METRICS)
        
        rows_to_write = []
        
        for i, node in enumerate(nodes):
            instance_id = node.get('instance_id')
            entity_id = node.get('__entity_id__')
            if not entity_id: continue

            gm_res = umodel_get_golden_metrics.invoke({
                "domain": "acs",
                "entity_set_name": "acs.ecs.instance",
                "entity_ids": [entity_id],
                "from_time": start_ts,
                "to_time": end_ts
            })

            # æ”¶é›†åŸå§‹æ•°æ®: {timestamp(ns): {metric: val}}
            node_data = {} 
            
            if gm_res and gm_res.data:
                for item in gm_res.data:
                    m_name = item.get('metric')
                    if m_name in TARGET_METRICS:
                        import ast
                        vals = ast.literal_eval(item.get('__value__', '[]'))
                        tss = ast.literal_eval(item.get('__ts__', '[]'))
                        for v, t in zip(vals, tss):
                            if t not in node_data: node_data[t] = {}
                            node_data[t][m_name] = v
            
            # ğŸ”¥ é‡é‡‡æ ·é€»è¾‘
            if self.args.interval and self.args.interval > 0 and node_data:
                try:
                    # 1. è½¬ DataFrame
                    df = pd.DataFrame.from_dict(node_data, orient='index')
                    # 2. å¤„ç†æ—¶é—´ç´¢å¼• (çº³ç§’è½¬ datetime)
                    df.index = pd.to_datetime(df.index, unit='ns')
                    # 3. é‡é‡‡æ · (å‡å€¼)
                    df_resampled = df.resample(f'{self.args.interval}s').mean()
                    
                    # 4. å›å¡«æ•°æ®
                    node_data_resampled = {}
                    # å°†æ—¶é—´æˆ³è½¬å›çº³ç§’ int64
                    new_timestamps = df_resampled.index.astype(np.int64).tolist()
                    
                    for idx, ts_val in enumerate(new_timestamps):
                        row_vals = df_resampled.iloc[idx].to_dict()
                        # è¿‡æ»¤æ‰å…¨ç©ºçš„è¡Œ (å¯é€‰ï¼Œè¿™é‡Œä¿ç•™ä»¥ç»´æŒæ—¶é—´è¿ç»­æ€§ï¼Œä½†å¡«å……ç©ºç¼º)
                        node_data_resampled[ts_val] = {k: v for k, v in row_vals.items() if pd.notnull(v)}
                    
                    # æ›¿æ¢åŸå§‹æ•°æ®
                    node_data = node_data_resampled
                except Exception as e:
                    logger.error(f"   [Node {instance_id}] é‡é‡‡æ ·å¤±è´¥: {e}")

            # æ•´ç†ä¸º CSV è¡Œ
            for ts, metrics in node_data.items():
                if not metrics: continue # è·³è¿‡ç©ºè¡Œ
                row = {
                    'problem_id': 'normal_000',
                    'fault_type': 'normal',
                    'instance_id': instance_id,
                    'timestamp': ts
                }
                for m in TARGET_METRICS:
                    row[m] = metrics.get(m, "")
                rows_to_write.append(row)
                
            if (i+1) % 5 == 0: print(f"   å·²å¤„ç† {i+1}/{len(nodes)} ä¸ªèŠ‚ç‚¹...", end='\r')

        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
            writer.writerows(rows_to_write)
        
        logger.info(f"\nâœ… [Metric] å·²ä¿å­˜ {len(rows_to_write)} æ¡æŒ‡æ ‡æ•°æ®è‡³ {csv_path}")

    def fetch_traces(self, start_ts, end_ts):
        """æ­¥éª¤ 3: è·å– Trace æ•°æ® (ä¸¥æ ¼è¿‡æ»¤ç‰ˆ - é€»è¾‘å¯¹é½ build_trace_dataset.py)"""
        logger.info("ğŸš€ [Trace] å¼€å§‹è·å–æ­£å¸¸æ—¶æ®µçš„ Trace...")
        
        # 1. åˆç­›: è·å–åŒ…å«è‡³å°‘ä¸€ä¸ªæ­£å¸¸Spançš„å€™é€‰TraceID
        # (è¿™é‡Œè¿˜æ˜¯ç”¨å®½æ³›æŸ¥è¯¢ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šåœ¨æœ¬åœ°åšäºŒæ¬¡ä¸¥æ ¼æ£€æŸ¥)
        query = "* | where try_cast(statusCode as bigint) <= 1"
        limit = self.args.trace_limit
        
        candidate_trace_ids = set()
        offset = 0
        
        # æ‰¹é‡è·å–å€™é€‰ ID
        while len(candidate_trace_ids) < limit * 1.5: # å¤šè·å–ä¸€ç‚¹ï¼Œå› ä¸ºæœ¬åœ°è¿‡æ»¤ä¼šä¸¢å¼ƒä¸€éƒ¨åˆ†
            req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=query, fromTime=start_ts, toTime=end_ts, line=100, offset=offset)
            try:
                res = self.sls_client.get_logs(req)
                if not res or not res.get_logs(): break
                logs = res.get_logs()
                for log in logs:
                    candidate_trace_ids.add(log.get_contents().get('traceId'))
                offset += len(logs)
                if len(logs) < 100: break
            except Exception as e:
                logger.error(f"SLS Query Error: {e}")
                break
        
        logger.info(f"   å·²è·å– {len(candidate_trace_ids)} ä¸ªå€™é€‰ TraceIDï¼Œæ­£åœ¨è¿›è¡Œä¸¥æ ¼è¿‡æ»¤å’Œæ‹‰å–...")
        
        csv_path = os.path.join(self.output_dir, "normal_traces.csv")
        csv_headers = [
            'TraceID', 'SpanId', 'ParentID', 'ServiceName', 'NodeName', 'PodName', 
            'URL', 'SpanKind', 'StartTimeMs', 'EndTimeMs', 'DurationMs',
            'StatusCode', 'HttpStatusCode', 'fault_type', 'fault_instance', 'problem_id'
        ]
        
        batch_list = list(candidate_trace_ids)
        valid_trace_count = 0
        total_spans = 0
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=csv_headers)
            writer.writeheader()
            
            # æ¯æ¬¡å¤„ç† 20 ä¸ª TraceID
            for i in range(0, len(batch_list), 20):
                if valid_trace_count >= limit: break
                
                batch = batch_list[i:i+20]
                or_query = " OR ".join([f'traceId: "{tid}"' for tid in batch])
                
                # === å†…å­˜èšåˆ: å°†è¿™20ä¸ªTraceçš„æ‰€æœ‰Spanå…ˆå­˜èµ·æ¥ ===
                trace_buffer = {tid: [] for tid in batch} 
                
                sub_offset = 0
                while True:
                    req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=or_query, fromTime=start_ts, toTime=end_ts, line=100, offset=sub_offset)
                    try:
                        res = self.sls_client.get_logs(req)
                        if not res or not res.get_logs(): break
                        logs = res.get_logs()
                        
                        for log in logs:
                            d = log.get_contents()
                            tid = d.get('traceId')
                            if tid in trace_buffer:
                                # è§£æ Span æ•°æ®
                                try: res_obj = json.loads(d.get('resources', '{}'))
                                except: res_obj = {}
                                try:
                                    s_ms = int(d.get('startTime', 0)) / 1e6
                                    d_ms = int(d.get('duration', 0)) / 1e6
                                except: s_ms, d_ms = 0, 0
                                
                                # æš‚å­˜åŸå§‹æ•°æ®å¯¹è±¡
                                span_obj = {
                                    'TraceID': tid,
                                    'SpanId': d.get('spanId'),
                                    'ParentID': d.get('parentSpanId'),
                                    'ServiceName': d.get('serviceName'),
                                    'NodeName': res_obj.get('k8s.node.name'),
                                    'PodName': res_obj.get('k8s.pod.name'),
                                    'URL': d.get('spanName'),
                                    'SpanKind': d.get('kind'),
                                    'StartTimeMs': f"{s_ms:.3f}",
                                    'EndTimeMs': f"{s_ms + d_ms:.3f}",
                                    'DurationMs': f"{d_ms:.3f}",
                                    'StatusCode': d.get('statusCode'), # åŸå§‹çŠ¶æ€ç 
                                    'HttpStatusCode': "",
                                    'fault_type': 'normal',
                                    'fault_instance': 'unknown',
                                    'problem_id': 'normal_000'
                                }
                                trace_buffer[tid].append(span_obj)
                        
                        sub_offset += len(logs)
                        if len(logs) < 100: break
                    except: break
                
                # === ä¸¥æ ¼è¿‡æ»¤: æ£€æŸ¥æ¯ä¸ªTraceæ˜¯å¦çœŸæ­£â€œçº¯å‡€â€ ===
                rows_to_save = []
                for tid, spans in trace_buffer.items():
                    if not spans: continue
                    
                    # 1. è¿‡æ»¤æ‰åŒ…å«å¼‚å¸¸Spançš„Trace (Status > 1)
                    is_dirty = False
                    for span in spans:
                        try:
                            # å…¼å®¹å¤„ç†ï¼šæœ‰äº›statusCodeå¯èƒ½æ˜¯ç©ºæˆ–éæ•°å­—ï¼Œè§†ä½œ0
                            sc = int(span['StatusCode']) if span['StatusCode'] and span['StatusCode'].isdigit() else 0
                            if sc > 1:
                                is_dirty = True
                                break
                        except: pass
                    
                    if is_dirty: continue # ä¸¢å¼ƒæ•´æ¡ Trace
                    
                    # 2. è¿‡æ»¤æ‰è¿‡çŸ­çš„ Trace (å¯é€‰ï¼Œå‚è€ƒ build_trace_dataset é€»è¾‘)
                    if len(spans) < 2: continue

                    # 3. é€šè¿‡æ£€æŸ¥ï¼ŒåŠ å…¥ä¿å­˜é˜Ÿåˆ—
                    rows_to_save.extend(spans)
                    valid_trace_count += 1
                
                # å†™å…¥æ–‡ä»¶
                if rows_to_save:
                    writer.writerows(rows_to_save)
                    total_spans += len(rows_to_save)
                
                print(f"   è¿›åº¦: å·²è·å– {valid_trace_count}/{limit} æ¡çº¯å‡€ Trace...", end='\r')

        logger.info(f"\nâœ… [Trace] å·²ä¿å­˜ {valid_trace_count} æ¡çº¯å‡€ Trace ({total_spans} Spans) è‡³ {csv_path}")

    def run(self):
        s_ts, e_ts = self.determine_time_window()
        self.fetch_metrics(s_ts, e_ts)
        self.fetch_traces(s_ts, e_ts)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", default="dataset/b_gt.csv", help="æ•…éšœåˆ—è¡¨è·¯å¾„")
    parser.add_argument("--output-dir", default="data/NormalData", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--trace-limit", type=int, default=70000, help="è·å–å¤šå°‘æ¡æ­£å¸¸ Trace")
    
    # ğŸ”¥ æ–°å¢å‚æ•°ï¼šé»˜è®¤ä¸ä¼ åˆ™ä¿ç•™åŸå§‹ç²¾åº¦(çº¦10s)ï¼Œä¼  60 åˆ™èšåˆä¸º 1åˆ†é’Ÿ
    parser.add_argument("--interval", type=int, default=10, help="æŒ‡æ ‡é‡é‡‡æ ·é—´éš”(ç§’)ï¼Œä¾‹å¦‚ 60")
    
    args = parser.parse_args()

    fetcher = NormalDataFetcher(args)
    fetcher.run()