#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
1:1 è¿˜åŸ Debug è„šæœ¬
å®Œå…¨å¤ç”¨ 2_get_normalData.py çš„é€»è¾‘ï¼Œä»…é”å®šç‰¹å®šèŠ‚ç‚¹å¹¶æ‰“å°ä¸­é—´æ•°æ®ã€‚
"""

import os
import sys
import argparse
import logging
from datetime import datetime

# === å¿…é¡»å¤ç”¨åŸè„šæœ¬çš„é…ç½® ===
import config

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# å¯¼å…¥åŸè„šæœ¬çš„ç±»ï¼ˆç¡®ä¿æ–‡ä»¶åæ˜¯ 2_get_normalData.py ä¸”åœ¨åŒçº§ç›®å½•ï¼‰
try:
    # from get_normalData import NormalDataFetcher # å‡è®¾åŸæ–‡ä»¶åä¸º normal_data_fetcher.pyï¼Œå¦‚æœä¸æ˜¯è¯·æ”¹å
    # å¦‚æœæ–‡ä»¶åæ˜¯ 2_get_normalData.pyï¼ŒPython import ä¸æ”¯æŒæ•°å­—å¼€å¤´ï¼Œ
    # è¯·ä¸´æ—¶å°† 2_get_normalData.py é‡å‘½åä¸º baseline_fetcher.pyï¼Œæˆ–è€…ä½¿ç”¨ä¸‹é¢çš„åŠ¨æ€å¯¼å…¥æ–¹å¼ï¼š
    import importlib.util
    spec = importlib.util.spec_from_file_location("baseline_fetcher", "scripts/2_get_normalData.py")
    baseline_fetcher = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(baseline_fetcher)
    NormalDataFetcher = baseline_fetcher.NormalDataFetcher
    umodel_get_golden_metrics = baseline_fetcher.umodel_get_golden_metrics
    execute_cms_query = baseline_fetcher.execute_cms_query
    TARGET_METRICS = baseline_fetcher.TARGET_METRICS
    REGION_ID = baseline_fetcher.REGION_ID
    WORKSPACE_NAME = baseline_fetcher.WORKSPACE_NAME
except ImportError as e:
    print(f"âŒ å¯¼å…¥åŸè„šæœ¬å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ '2_get_normalData.py' åœ¨å½“å‰ç›®å½•ä¸‹ã€‚")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class DebugFetcher(NormalDataFetcher):
    def fetch_metrics(self, start_ts, end_ts):
        print(f"\nğŸš€ [Debugæ¨¡å¼] å¼€å§‹é’ˆå¯¹å•èŠ‚ç‚¹è¿›è¡Œ 1:1 é€»è¾‘å¤ç°")
        print(f"   æ—¶é—´çª—: {start_ts} -> {end_ts}")
        
        # =======================================================
        # å¼ºåˆ¶é”å®šç›®æ ‡èŠ‚ç‚¹ (è·³è¿‡ umodel_get_entitiesï¼Œç›´æ¥ç»™ ID)
        # =======================================================
        target_instance_id = "i-m5ec00yjg8kxv34hyr0n"
        # è¿™æ˜¯ä½ æ—¥å¿—é‡ŒæŸ¥å‡ºæ¥çš„ entity_idï¼Œæˆ‘ä»¬ç›´æ¥ç¡¬ç¼–ç ï¼Œæ’é™¤ lookup é”™è¯¯
        target_entity_id = "34016cd1d03b562e39370299e1e83610" 
        
        nodes = [{
            'instance_id': target_instance_id,
            '__entity_id__': target_entity_id
        }]
        
        print(f"   ğŸ”’ å·²é”å®šèŠ‚ç‚¹: {target_instance_id} (EntityID: {target_entity_id})")

        # =======================================================
        # ä¸‹é¢å®Œå…¨å¤åˆ¶åŸè„šæœ¬çš„é€»è¾‘ï¼Œåªå¢åŠ äº† print
        # =======================================================
        CHUNK_SIZE = 1800

        for node in nodes:
            instance_id = node.get('instance_id')
            entity_id = node.get('__entity_id__')
            
            # node_data: { timestamp_ns: { metric_name: value } }
            node_data = {} 

            current_chunk_start = start_ts
            while current_chunk_start < end_ts:
                current_chunk_end = min(current_chunk_start + CHUNK_SIZE, end_ts)
                print(f"   ğŸ” æ­£åœ¨æ‰«æåˆ†ç‰‡: {current_chunk_start} ~ {current_chunk_end} ...")
                
                chunk_found_metrics = set()

                # --- ç­–ç•¥ A: Golden Metrics ---
                try:
                    gm_res = umodel_get_golden_metrics.invoke({
                        "domain": "acs",
                        "entity_set_name": "acs.ecs.instance",
                        "entity_ids": [entity_id],
                        "from_time": current_chunk_start,
                        "to_time": current_chunk_end
                    })
                    
                    if gm_res and gm_res.data:
                        print(f"      âœ… [GM] æ¥å£è¿”å›äº†æ•°æ®å¯¹è±¡")
                        for item in gm_res.data:
                            m_name = item.get('metric')
                            if m_name in TARGET_METRICS:
                                import ast
                                vals = ast.literal_eval(item.get('__value__', '[]'))
                                tss = ast.literal_eval(item.get('__ts__', '[]'))
                                if vals:
                                    print(f"         Found {m_name}: {len(vals)} points")
                                    chunk_found_metrics.add(m_name)
                                    for v, t in zip(vals, tss):
                                        t_int = int(t)
                                        t_ns = t_int * 1000000 if t_int < 1e14 else t_int
                                        if t_ns not in node_data: node_data[t_ns] = {}
                                        node_data[t_ns][m_name] = v
                    else:
                        print(f"      âŒ [GM] æ¥å£è¿”å›ç©º (æˆ– .data ä¸ºç©º)")
                except Exception as e:
                    print(f"      âŒ [GM] æŠ¥é”™: {e}")

                # --- ç­–ç•¥ B: CMS åŸå§‹æ¥å£è¡¥ç¼º ---
                missing = [m for m in TARGET_METRICS if m not in chunk_found_metrics]
                if missing:
                    print(f"      âš ï¸ [CMS] å°è¯•è¡¥å…¨ç¼ºå¤±æŒ‡æ ‡: {len(missing)} ä¸ª")
                    for m in missing:
                        # åŸæ±åŸå‘³çš„æŸ¥è¯¢è¯­å¥
                        query = f".entity_set with(domain='acs', name='acs.ecs.instance', ids=['{entity_id}']) | entity-call get_metric('{m}')"
                        try:
                            # ç›´æ¥å¤ç”¨çˆ¶ç±»çš„ client
                            res = execute_cms_query(self.cms_client, WORKSPACE_NAME, query, current_chunk_start, current_chunk_end)
                            if res and res.data:
                                print(f"         âœ… [CMS] {m} è·å–åˆ° {len(res.data)} æ¡æ•°æ®")
                                for r in res.data:
                                    v = r.get('value') or r.get(m)
                                    t = r.get('timestamp') or r.get('ts')
                                    if v is not None and t is not None:
                                        t_int = int(t)
                                        t_ns = t_int * 1000000 if t_int < 1e14 else t_int
                                        if t_ns not in node_data: node_data[t_ns] = {}
                                        node_data[t_ns][m] = v
                            else:
                                # è¿™é‡Œå¾ˆå…³é”®ï¼šå¦‚æœè¿™é‡Œä¹Ÿç©ºï¼Œé‚£å°±æ˜¯çœŸæ²¡æ•°æ®
                                pass 
                                # print(f"         âŒ [CMS] {m} è¿”å›ç©º")
                        except Exception as e:
                            print(f"         âŒ [CMS] æŸ¥è¯¢æŠ¥é”™: {e}")
                
                current_chunk_start = current_chunk_end

            # === æ‰“å°æœ€ç»ˆæŠ“åˆ°çš„åŸå§‹æ•°æ®æ‘˜è¦ ===
            print(f"\nğŸ“Š === æœ€ç»ˆæ•°æ®æ‘˜è¦ (Instance: {instance_id}) ===")
            if not node_data:
                print("ğŸ”´ ç»“æœ: å­—å…¸ä¸ºç©ºã€‚è¯¥èŠ‚ç‚¹æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæŒ‡æ ‡æ•°æ®ã€‚")
            else:
                print(f"ğŸŸ¢ ç»“æœ: æ•è·åˆ°äº† {len(node_data)} ä¸ªæ—¶é—´ç‚¹çš„æ•°æ®ã€‚")
                sorted_ts = sorted(node_data.keys())
                
                # æ‰“å°å‰ 3 æ¡çœ‹æ ·å­
                print("   [å‰ 3 æ¡åŸå§‹æ•°æ®æ ·ä¾‹]:")
                for ts in sorted_ts[:3]:
                    readable_time = datetime.fromtimestamp(ts / 1e9).strftime('%H:%M:%S')
                    print(f"   Time: {readable_time} ({ts})")
                    for k, v in node_data[ts].items():
                        print(f"      - {k}: {v}")

if __name__ == "__main__":
    # ä½¿ç”¨ä½ æŒ‡å®šçš„å‚æ•°
    # Start: 2025-09-16 16:00:00 -> TS: 1758009600
    # End:   2025-09-16 22:00:00 -> TS: 1758031200
    
    parser = argparse.ArgumentParser()
    # åªè¦è¿™å‡ ä¸ªå‚æ•°å°±å¤Ÿäº†ï¼Œåæ­£æˆ‘ä»¬åœ¨è¿™ä¸ªè„šæœ¬é‡Œä¸çœŸæ­£å†™æ–‡ä»¶
    parser.add_argument("--output-dir", default="data/DebugOutput")
    parser.add_argument("--file-name", default="debug")
    parser.add_argument("--window-hours", type=float, default=6.0) 
    
    args = parser.parse_args()
    
    # ä½ çš„ Start/End æ—¶é—´
    s_ts = 1758009600 
    e_ts = 1758031200

    fetcher = DebugFetcher(args)
    fetcher.fetch_metrics(s_ts, e_ts)