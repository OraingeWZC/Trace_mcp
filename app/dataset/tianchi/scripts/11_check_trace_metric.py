import os
import argparse
import pandas as pd
import numpy as np
import json
from tqdm import tqdm

def load_mapping(mapping_path):
    if not mapping_path or not os.path.exists(mapping_path):
        print(f"âš ï¸  æœªæ‰¾åˆ°æ˜ å°„æ–‡ä»¶: {mapping_path} (å°†è·³è¿‡ IP->ID è½¬æ¢å°è¯•)")
        return {}
    
    with open(mapping_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # åˆå¹¶ ip_to_id å’Œ name_to_id ä¸ºä¸€ä¸ªå¤§å­—å…¸
    lookup = {}
    if "ip_to_id" in data: lookup.update(data["ip_to_id"])
    if "name_to_id" in data: lookup.update(data["name_to_id"])
    
    print(f"âœ… å·²åŠ è½½æ˜ å°„è¡¨ï¼ŒåŒ…å« {len(lookup)} ä¸ªæ˜ å°„è§„åˆ™")
    return lookup

def check_data_quality(trace_file, metric_file, mapping_file=None):
    print(f"ğŸš€ å¼€å§‹è¯Šæ–­æ•°æ®è´¨é‡...")
    print(f"   Trace æ–‡ä»¶: {trace_file}")
    print(f"   Metric æ–‡ä»¶: {metric_file}")
    
    # 1. åŠ è½½ Trace èŠ‚ç‚¹ä¿¡æ¯
    if not os.path.exists(trace_file):
        print("âŒ Trace æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print("â³ æ­£åœ¨è¯»å– Trace æ–‡ä»¶ (å¯èƒ½è¾ƒæ…¢)...")
    # åªè¯»éœ€è¦çš„åˆ—ï¼ŒåŠ å¿«é€Ÿåº¦
    df_trace = pd.read_csv(trace_file, usecols=['TraceID', 'NodeName', 'StartTimeMs', 'EndTimeMs'])
    
    # ç»Ÿè®¡ Trace æ¶‰åŠçš„å”¯ä¸€èŠ‚ç‚¹
    trace_nodes = df_trace['NodeName'].unique()
    trace_nodes = [str(n).strip() for n in trace_nodes if pd.notnull(n) and str(n).strip() != '']
    print(f"   -> Trace ä¸­å…±å‘ç° {len(trace_nodes)} ä¸ªç‹¬ç«‹èŠ‚ç‚¹æ ‡è¯† (NodeName)")

    # 2. åŠ è½½ Metric èŠ‚ç‚¹ä¿¡æ¯
    if not os.path.exists(metric_file):
        print("âŒ Metric æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print("â³ æ­£åœ¨è¯»å– Metric æ–‡ä»¶...")
    df_metric = pd.read_csv(metric_file, usecols=['instance_id', 'timestamp'])
    if 'instanceId' in df_metric.columns: 
        df_metric.rename(columns={'instanceId': 'instance_id'}, inplace=True)
        
    metric_nodes = set(df_metric['instance_id'].astype(str).unique())
    print(f"   -> Metric ä¸­å…±åŒ…å« {len(metric_nodes)} ä¸ªç‰©ç†æœº (Instance ID)")
    
    # 3. åŠ è½½æ˜ å°„è¡¨
    mapping = load_mapping(mapping_file)
    
    # === å¼€å§‹è¯Šæ–­ ===
    print("\nğŸ” === è¯Šæ–­æŠ¥å‘Š ===")
    
    results = {
        "success": [],       # æˆåŠŸï¼šTraceèŠ‚ç‚¹ -> æ˜ å°„ID -> Metricä¸­æœ‰æ•°æ®
        "no_mapping": [],    # å¤±è´¥ï¼šTraceèŠ‚ç‚¹æ˜¯IPï¼Œä¸”æ˜ å°„è¡¨ä¸­æ‰¾ä¸åˆ°ID
        "no_metric": [],     # å¤±è´¥ï¼šTraceèŠ‚ç‚¹(æˆ–æ˜ å°„å)æ˜¯IDï¼Œä½†Metricè¡¨ä¸­æ²¡æ•°æ®
        "time_mismatch": []  # è­¦å‘Šï¼šæœ‰IDä¹Ÿæœ‰Metricï¼Œä½†Traceå‘ç”Ÿæ—¶Metricæ²¡è¦†ç›– (æš‚æœªè¯¦ç»†å®ç°ï¼Œä»…ä½œæç¤º)
    }
    
    for original_name in tqdm(trace_nodes, desc="æ£€æŸ¥èŠ‚ç‚¹"):
        final_id = original_name
        is_mapped = False
        
        # æ­¥éª¤ A: å°è¯•æ˜ å°„
        # å¦‚æœåŸå§‹åå­—å°±åƒä¸€ä¸ª ID (i-å¼€å¤´)ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
        if original_name.startswith('i-'):
            final_id = original_name
        else:
            # å°è¯•ä»æ˜ å°„è¡¨ä¸­æŸ¥æ‰¾
            if original_name in mapping:
                final_id = mapping[original_name]
                is_mapped = True
            else:
                # æ˜ å°„å¤±è´¥
                results["no_mapping"].append(original_name)
                continue
        
        # æ­¥éª¤ B: æ£€æŸ¥ Metric æ˜¯å¦å­˜åœ¨
        if final_id in metric_nodes:
            results["success"].append(f"{original_name} -> {final_id}")
        else:
            results["no_metric"].append(f"{original_name} -> {final_id}")

    # === è¾“å‡ºæ±‡æ€» ===
    total = len(trace_nodes)
    print("\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   âœ… å®Œå…¨åŒ¹é…æˆåŠŸ: {len(results['success'])} / {total} ({(len(results['success'])/total)*100:.1f}%)")
    print(f"   âŒ æ˜ å°„å¤±è´¥ (ç¼ºå­—å…¸): {len(results['no_mapping'])}")
    print(f"   âŒ æŒ‡æ ‡ç¼ºå¤± (æœ‰IDæ— æ•°æ®): {len(results['no_metric'])}")
    
    if results["no_mapping"]:
        print(f"\nâš ï¸  [Top 5] æ˜ å°„å¤±è´¥çš„èŠ‚ç‚¹ (è¯·æ£€æŸ¥ data/ecs_mapping_index.json):")
        for x in results["no_mapping"][:5]: print(f"   - {x}")
        
    if results["no_metric"]:
        print(f"\nâš ï¸  [Top 5] æœ‰IDä½†æ— æŒ‡æ ‡çš„èŠ‚ç‚¹ (è¯·æ£€æŸ¥ Metric æ–‡ä»¶æ˜¯å¦è¦†ç›–äº†å¯¹åº”æœºå™¨):")
        for x in results["no_metric"][:5]: print(f"   - {x}")

    # å»ºè®®
    print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
    if len(results["no_mapping"]) > 0:
        print("   1. ä½ çš„ ecs_mapping_index.json å¯èƒ½è¿‡æœŸäº†ï¼Œæˆ–è€… Trace é‡Œçš„ IP æ˜¯æ–°çš„/ä¸´æ—¶çš„ã€‚")
    if len(results["no_metric"]) > 0:
        print("   2. 2_get_normalData.py å¯èƒ½æ¼æŠ“äº†è¿™äº›èŠ‚ç‚¹ã€‚")
        print("      å»ºè®®ï¼šä¸è¦ä½¿ç”¨ 'Traceå¯¼å‘' (fetch_metricså¸¦target_nodes) çš„æ–¹å¼ï¼Œ")
        print("      è€Œæ˜¯æ”¹å› 'å…¨é‡æŠ“å–' (åªæŒ‡å®šæ—¶é—´çª—)ï¼Œè®©å®ƒæŠŠè¯¥æ—¶é—´æ®µå†…æ‰€æœ‰æ´»è·ƒèŠ‚ç‚¹çš„æ•°æ®éƒ½æŠ“ä¸‹æ¥ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # é»˜è®¤å€¼æ ¹æ®ä½ ä¹‹å‰çš„æ–‡ä»¶åè®¾å®š
    parser.add_argument("--trace", default="data/NormalData/normal_traces1e5_30s.csv", help="Trace CSV è·¯å¾„")
    parser.add_argument("--metric", default="data/NormalData/normal_metrics_1e5_30s.csv", help="Metric CSV è·¯å¾„")
    parser.add_argument("--mapping", default="data/ecs_mapping_index.json", help="æ˜ å°„æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    check_data_quality(args.trace, args.metric, args.mapping)