# -*- coding: utf-8 -*-
"""
Trace æ•°æ®é›†æ ¼å¼è½¬æ¢å·¥å…·
åŠŸèƒ½ï¼šå°†æ—§ç‰ˆ/æ­£å¸¸æ•°æ®é›† CSV è½¬æ¢ä¸ºæ–°ç‰ˆæ ‡å‡†æ ¼å¼
1. åˆ—åé‡å‘½å (å¦‚ SpanID -> SpanId, StartTime -> StartTimeMs)
2. åˆ—é¡ºåºè°ƒæ•´ (å¯¹é½è®­ç»ƒæ•°æ®æ ¼å¼)
3. è‡ªåŠ¨å¡«å……ç©ºæ ‡ç­¾ (fault_type, fault_instance, problem_id)
"""

import csv
import argparse
import os
import sys

# å¢åŠ å­—æ®µå¤§å°é™åˆ¶ï¼Œé˜²æ­¢æŸäº›è¶…é•¿ Trace æŠ¥é”™
csv.field_size_limit(2147483647)

# ================= é…ç½®åŒºåŸŸ =================

# ç›®æ ‡ CSV è¡¨å¤´ (ä¸¥æ ¼å¯¹åº”è®­ç»ƒè„šæœ¬è¦æ±‚çš„æ ¼å¼)
TARGET_HEADERS = [
    'TraceID', 'SpanId', 'ParentID', 
    'ServiceName', 'NodeName', 'PodName', 
    'URL', 'SpanKind', 
    'StartTimeMs', 'EndTimeMs', 'DurationMs',
    'StatusCode', 'HttpStatusCode', 
    'fault_type', 'fault_instance', 'problem_id'
]

# åˆ—åæ˜ å°„å­—å…¸: { "æºåˆ—å": "ç›®æ ‡åˆ—å" }
# å¦‚æœæºåˆ—åå’Œç›®æ ‡ä¸€è‡´ï¼Œå¯ä»¥ä¸å†™ï¼Œä½†ä¸ºäº†æ¸…æ™°å»ºè®®å†™å…¨
COLUMN_MAPPING = {
    'TraceID': 'TraceID',
    'SpanID': 'SpanId',        # æ³¨æ„å¤§å°å†™å˜åŒ–: ID -> Id
    'ParentID': 'ParentID',
    'NodeName': 'NodeName',
    'ServiceName': 'ServiceName',
    'PodName': 'PodName',
    'URL': 'URL',
    'HttpStatusCode': 'HttpStatusCode',
    'StatusCode': 'StatusCode',
    'SpanKind': 'SpanKind',
    'StartTime': 'StartTimeMs', # å‡è®¾æºæ•°æ®å·²ç»æ˜¯æ•°å€¼ï¼Œåªæ”¹å
    'EndTime': 'EndTimeMs',
    'Duration': 'DurationMs'
}

# ===========================================

def convert_csv(input_path, output_path):
    print(f"ğŸš€ å¼€å§‹è½¬æ¢: {input_path} -> {output_path}")
    
    if not os.path.exists(input_path):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return

    success_count = 0
    
    try:
        with open(input_path, 'r', encoding='utf-8', newline='') as f_in, \
             open(output_path, 'w', encoding='utf-8', newline='') as f_out:
            
            # 1. è¯»å–æºæ–‡ä»¶
            reader = csv.DictReader(f_in)
            
            # æ£€æŸ¥æºæ–‡ä»¶è¡¨å¤´æ˜¯å¦åŒ…å«æˆ‘ä»¬éœ€è¦çš„æ‰€æœ‰å…³é”®å­—æ®µ
            # (è¿™é‡Œåªæ‰“å°è­¦å‘Šï¼Œä¸å¼ºåˆ¶é€€å‡ºï¼Œé˜²æ­¢æºæ–‡ä»¶åˆ—åæœ‰ç»†å¾®å·®åˆ«)
            source_fields = reader.fieldnames
            print(f"   â„¹ï¸ æºæ–‡ä»¶åˆ—å: {source_fields}")
            
            # 2. åˆå§‹åŒ–å†™å…¥å™¨
            writer = csv.DictWriter(f_out, fieldnames=TARGET_HEADERS)
            writer.writeheader()
            
            # 3. é€è¡Œå¤„ç†
            for row in reader:
                new_row = {}
                
                # A. æ˜ å°„å·²æœ‰æ•°æ®
                for src_col, target_col in COLUMN_MAPPING.items():
                    # get() é˜²æ­¢æºæ–‡ä»¶ç¼ºåˆ—æŠ¥é”™ï¼Œé»˜è®¤ç©ºå­—ç¬¦ä¸²
                    # strip() å»é™¤å¯èƒ½å­˜åœ¨çš„é¦–å°¾ç©ºæ ¼
                    val = row.get(src_col, '').strip()
                    new_row[target_col] = val
                
                # B. å¡«å……æ–°æ ‡ç­¾ (æ­£å¸¸é›†è®¾ä¸ºç©ºæˆ–ç‰¹å®šæ ‡è¯†)
                # æ‚¨è¯´åé¢å‡ ä¸ªéƒ½ä¸ç”¨æ‰“æ ‡ç­¾ï¼Œè¿™é‡Œé»˜è®¤ç•™ç©º
                new_row['fault_type'] = ''      # æˆ–è€…å¡« "normal"
                new_row['fault_instance'] = '' 
                new_row['problem_id'] = ''      # æˆ–è€…å¡« "0"
                
                # C. ç‰¹æ®Šå¤„ç† (å¯é€‰)
                # å¦‚æœ StartTime æ˜¯çº³ç§’(19ä½)ï¼Œå¯èƒ½éœ€è¦é™¤ä»¥ 1e6 è½¬æ¯«ç§’
                # è¿™é‡Œæä¾›ä¸€ä¸ªç®€å•çš„è‡ªåŠ¨è½¬æ¢é€»è¾‘ç¤ºä¾‹ï¼Œé»˜è®¤æ³¨é‡Šæ‰
                '''
                try:
                    s_ts = float(new_row['StartTimeMs'])
                    if s_ts > 1e16: # å¯èƒ½æ˜¯çº³ç§’
                        new_row['StartTimeMs'] = f"{s_ts/1e6:.3f}"
                        new_row['EndTimeMs'] = f"{float(new_row['EndTimeMs'])/1e6:.3f}"
                        new_row['DurationMs'] = f"{float(new_row['DurationMs'])/1e6:.3f}"
                except:
                    pass
                '''

                # å†™å…¥
                writer.writerow(new_row)
                success_count += 1
                
                if success_count % 10000 == 0:
                    print(f"   â³ å·²å¤„ç† {success_count} è¡Œ...", end='\r')

        print(f"\nâœ… è½¬æ¢å®Œæˆ! å…±å¤„ç† {success_count} æ¡æ•°æ®ã€‚")
        print(f"   ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_path}")

    except Exception as e:
        print(f"\nâŒ è½¬æ¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Trace CSV æ ¼å¼è½¬æ¢å·¥å…·")
    # é»˜è®¤æ–‡ä»¶åï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¿®æ”¹è¿™é‡Œ
    parser.add_argument("--input", default="E:\ZJU\AIOps\Projects\TraDNN\Trace_mcp/app/tools/trace_sv_diag\dataset/tianchi/row_old/Normal.csv", help="æº CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", default="E:\ZJU\AIOps\Projects\TraDNN\Trace_mcp/app/tools/trace_sv_diag\dataset/tianchi/row/Normal.csv", help="è¾“å‡º CSV æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    convert_csv(args.input, args.output)