# -*- coding: utf-8 -*-
"""
å…¨å±€ Trace æå–å·¥å…· (Single File Edition)
åŠŸèƒ½ï¼š
1. è¯»å– b_gt.csvï¼Œç­›é€‰æ‰€æœ‰ Service çº§æ•…éšœã€‚
2. å°†æ‰€æœ‰ Trace æ•°æ®æ±‡èšå†™å…¥åŒä¸€ä¸ª CSV æ–‡ä»¶ã€‚
3. è‡ªåŠ¨å»é‡ï¼šå¯åŠ¨æ—¶åŠ è½½å·²æœ‰ TraceIDã€‚
4. ç»Ÿè®¡è¾“å‡ºï¼šæŒ‰ fault_type ç»Ÿè®¡æ ·æœ¬æ•°é‡ã€‚
5. æ—¥å¿—è®°å½•ï¼šåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶ã€‚
"""

import os
import sys
import json
import csv
import time
import argparse
import logging
import collections
from datetime import datetime, timedelta
from aliyun.log import LogClient, GetLogsRequest
from alibabacloud_sts20150401.client import Client as StsClient
from alibabacloud_sts20150401 import models as sts_models
from alibabacloud_tea_openapi import models as open_api_models
from Tea.exceptions import TeaException

import config

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
# 1. SLS é…ç½®
PROJECT_NAME = config.SLS_PROJECT_NAME
LOGSTORE_NAME = config.SLS_LOGSTORE_NAME
REGION = config.SLS_REGION

# 2. è¾“å‡ºæ–‡ä»¶å
OUTPUT_FILENAME = "all_fault_traces.csv"
LOG_FILENAME = "trace_extraction.log"

# 3. CSV è¡¨å¤´
CSV_HEADERS = [
    'TraceID', 'SpanId', 'ParentID', 
    'ServiceName', 'NodeName', 'PodName', 
    'URL', 'SpanKind', 
    'StartTimeMs', 'EndTimeMs', 'DurationMs',
    'StatusCode', 'HttpStatusCode', 
    'fault_type', 'fault_instance', 'problem_id' # æ–°å¢ problem_id æ–¹ä¾¿å›æº¯
]

# ================= ğŸ”§ é‰´æƒé…ç½® =================
os.environ["ALIBABA_CLOUD_ROLE_SESSION_NAME"] = "service-fault-verifier"

# ===============================================

# é…ç½®æ—¥å¿—
def setup_logging():
    # åˆ›å»º Logger
    logger = logging.getLogger("TraceExtractor")
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤å·²æœ‰çš„ Handler é˜²æ­¢é‡å¤æ‰“å°
    if logger.handlers:
        logger.handlers.clear()

    # Formatter
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    # 1. File Handler (å†™å…¥æ–‡ä»¶)
    file_handler = logging.FileHandler(LOG_FILENAME, encoding='utf-8')
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    # 2. Console Handler (è¾“å‡ºåˆ°å±å¹•)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    return logger

logger = setup_logging()

def get_sts_credentials(region: str = "cn-qingdao"):
    access_key_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
    access_key_secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
    role_arn = os.getenv('ALIBABA_CLOUD_ROLE_ARN')
    session_name = 'single-csv-extractor'
    
    config = open_api_models.Config(
        access_key_id=access_key_id,
        access_key_secret=access_key_secret,
        endpoint=f'sts.{region}.aliyuncs.com'
    )
    sts_client = StsClient(config)
    try:
        response = sts_client.assume_role(sts_models.AssumeRoleRequest(
            role_arn=role_arn,
            role_session_name=session_name,
            duration_seconds=3600
        ))
        return response.body.credentials
    except TeaException as e:
        logger.error(f"STS é‰´æƒå¤±è´¥: {e.message}")
        raise

class AutoRefreshSLSClient:
    def __init__(self, region: str = "cn-qingdao"):
        self.region = region
        self.sls_endpoint = f"{region}.log.aliyuncs.com"
        self.client = None
        self._refresh_client()
    
    def _refresh_client(self):
        creds = get_sts_credentials(self.region)
        self.client = LogClient(
            endpoint=self.sls_endpoint,
            accessKeyId=creds.access_key_id,
            accessKey=creds.access_key_secret,
            securityToken=creds.security_token
        )
    
    def get_logs(self, request):
        try:
            return self.client.get_logs(request)
        except Exception as e:
            if "Unauthorized" in str(e) or "expired" in str(e).lower():
                logger.warning("Token è¿‡æœŸï¼Œæ­£åœ¨è‡ªåŠ¨åˆ·æ–°...")
                self._refresh_client()
                return self.client.get_logs(request)
            raise e

def safe_json_load(text):
    if not text: return {}
    try: return json.loads(text)
    except: return {}

class TraceExtractor:
    def __init__(self):
        self.client = AutoRefreshSLSClient(REGION)

    def _count_total_traces(self, query, start_ts, end_ts):
        """ç»Ÿè®¡é¢„ä¼°æ€»æ•°"""
        count_query = f"{query} | select count(distinct traceId) as total"
        try:
            req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=count_query, fromTime=start_ts, toTime=end_ts)
            res = self.client.get_logs(req)
            if res and res.get_logs():
                return int(res.get_logs()[0].get_contents().get('total', 0))
            return 0
        except Exception as e:
            logger.warning(f"ç»Ÿè®¡æ€»æ•°å¤±è´¥: {e}")
            return -1

    def find_trace_ids(self, query, start_ts, end_ts, limit):
        """é˜¶æ®µä¸€ï¼šæŸ¥æ‰¾ TraceID"""
        logger.info(f"   ğŸ” æ­£åœ¨æ£€ç´¢ TraceID (Query: {query})...")
        
        # 1. ç»Ÿè®¡
        total = self._count_total_traces(query, start_ts, end_ts)
        logger.info(f"      ğŸ“Š SLS ä¸­ç¬¦åˆæ¡ä»¶çš„ Trace æ€»æ•°: {total}")
        
        if total == 0:
            return []

        # 2. æ‹‰å– ID
        trace_ids = set()
        offset = 0
        while len(trace_ids) < limit:
            req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=query, fromTime=start_ts, toTime=end_ts, line=100, offset=offset)
            res = self.client.get_logs(req)
            if not res or not res.get_logs(): break
            
            logs = res.get_logs()
            for log in logs:
                tid = log.get_contents().get('traceId')
                if tid: trace_ids.add(tid)
            
            offset += len(logs)
            if len(logs) < 100: break
        
        final_ids = list(trace_ids)[:limit]
        logger.info(f"      âœ… æå– TraceID æˆåŠŸ: {len(final_ids)} ä¸ª")
        return final_ids

    def fetch_full_traces(self, trace_ids, start_ts, end_ts, meta_info, csv_writer, existing_ids):
        """é˜¶æ®µäºŒï¼šæ‹‰å–å…¨é‡ Span å¹¶å†™å…¥ CSV"""
        if not trace_ids: return 0

        # è¿‡æ»¤æ‰å·²ç»å­˜åœ¨äº CSV ä¸­çš„ TraceID
        new_ids = [tid for tid in trace_ids if tid not in existing_ids]
        skipped_count = len(trace_ids) - len(new_ids)
        
        if skipped_count > 0:
            logger.info(f"      â­ï¸  è·³è¿‡ {skipped_count} ä¸ªå·²å­˜åœ¨çš„ TraceIDï¼Œå‰©ä½™éœ€ä¸‹è½½: {len(new_ids)}")
        
        if not new_ids:
            return 0

        total_spans = 0
        batch_size = 20
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(new_ids), batch_size):
            batch = new_ids[i : i + batch_size]
            or_query = " OR ".join([f'traceId: "{tid}"' for tid in batch])
            
            offset = 0
            while True:
                req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=or_query, fromTime=start_ts, toTime=end_ts, line=100, offset=offset)
                res = self.client.get_logs(req)
                if not res or not res.get_logs(): break
                
                logs = res.get_logs()
                rows = []
                for log in logs:
                    data = log.get_contents()
                    res_obj = safe_json_load(data.get('resources', '{}'))
                    attr_obj = safe_json_load(data.get('attributes', '{}'))
                    
                    try:
                        s_ns = int(data.get('startTime', 0))
                        d_ns = int(data.get('duration', 0))
                        s_ms = s_ns / 1e6
                        e_ms = (s_ns + d_ms) / 1e6
                        d_ms = d_ns / 1e6
                    except: s_ms, e_ms, d_ms = 0, 0, 0

                    rows.append({
                        'TraceID': data.get('traceId', ''),
                        'SpanId': data.get('spanId', ''),
                        'ParentID': data.get('parentSpanId', ''),
                        'ServiceName': data.get('serviceName', ''),
                        'NodeName': res_obj.get('k8s.node.name', ''),
                        'PodName': res_obj.get('k8s.pod.name', ''),
                        'URL': data.get('spanName', ''),
                        'SpanKind': data.get('kind', ''),
                        'StartTimeMs': f"{s_ms:.3f}",
                        'EndTimeMs': f"{e_ms:.3f}",
                        'DurationMs': f"{d_ms:.3f}",
                        'StatusCode': data.get('statusCode', ''),
                        'HttpStatusCode': str(attr_obj.get('http.status_code') or attr_obj.get('rpc.grpc.status_code', '')),
                        'fault_type': meta_info['fault_type'],
                        'fault_instance': meta_info['fault_instance'],
                        'problem_id': meta_info['problem_id']
                    })
                
                # å†™å…¥æ–‡ä»¶
                if rows:
                    csv_writer.writerows(rows)
                    total_spans += len(rows)
                
                offset += len(logs)
                if len(logs) < 100: break
            
            # æ›´æ–°å…¨å±€ ID é›†åˆ
            for tid in batch:
                existing_ids.add(tid)
                
            print(f"      â³ è¿›åº¦: {min(i+batch_size, len(new_ids))}/{len(new_ids)} Traces å·²å¤„ç†...", end='\r')
        
        print("") # æ¢è¡Œ
        logger.info(f"      ğŸ“¦ å·²ä¿å­˜ {len(new_ids)} æ¡ Traceï¼Œå…± {total_spans} ä¸ª Spanã€‚")
        return len(new_ids)

class UnifiedProcessor:
    def __init__(self, args):
        self.args = args
        self.extractor = TraceExtractor()
        self.existing_trace_ids = set()
        self.stats = collections.defaultdict(int) # ç»Ÿè®¡å­—å…¸ {fault_type: trace_count}
        self.output_path = os.path.join(self.args.output_dir, OUTPUT_FILENAME)

    def _load_existing_data(self):
        """é¢„åŠ è½½å·²æœ‰çš„ TraceID ç”¨äºå»é‡"""
        if not os.path.exists(self.output_path):
            logger.info(f"ğŸ†• è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {self.output_path}")
            return False # æ–‡ä»¶ä¸å­˜åœ¨

        logger.info(f"ğŸ“¥ æ­£åœ¨è¯»å–å·²æœ‰æ•°æ®è¿›è¡Œå»é‡: {self.output_path} ...")
        try:
            with open(self.output_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                count = 0
                for row in reader:
                    tid = row.get('TraceID')
                    if tid:
                        self.existing_trace_ids.add(tid)
                        count += 1
            logger.info(f"âœ… å·²åŠ è½½ {len(self.existing_trace_ids)} ä¸ªå†å² TraceID (å…± {count} è¡Œ Span)")
            return True # æ–‡ä»¶å­˜åœ¨
        except Exception as e:
            logger.error(f"è¯»å–å†å²æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def process_all(self):
        if not os.path.exists(self.args.csv):
            logger.error(f"CSV æ–‡ä»¶ä¸å­˜åœ¨: {self.args.csv}")
            return

        # 1. å‡†å¤‡æ–‡ä»¶å¥æŸ„
        file_exists = self._load_existing_data()
        
        # ä½¿ç”¨ 'a' æ¨¡å¼è¿½åŠ 
        with open(self.output_path, 'a', newline='', encoding='utf-8') as f_out:
            writer = csv.DictWriter(f_out, fieldnames=CSV_HEADERS)
            
            # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´
            if not file_exists:
                writer.writeheader()

            # 2. è¯»å–ä»»åŠ¡åˆ—è¡¨
            with open(self.args.csv, 'r', encoding='utf-8') as f_in:
                reader = csv.DictReader(f_in)
                rows = list(reader)
            
            # è¿‡æ»¤èŒƒå›´
            target_rows = []
            if self.args.range:
                s_id, e_id = map(int, self.args.range.split(','))
                target_rows = [r for r in rows if s_id <= int(r['problem_id']) <= e_id]
            else:
                target_rows = rows # é»˜è®¤è·‘å…¨éƒ¨

            logger.info(f"ğŸ¯ å¾…å¤„ç†ä»»åŠ¡æ•°: {len(target_rows)}")

            # 3. å¾ªç¯å¤„ç†
            for row in target_rows:
                try:
                    self._process_single_row(row, writer)
                except Exception as e:
                    logger.error(f"å¤„ç† Problem {row.get('problem_id')} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                
                # é—´éš”ä¼‘æ¯
                time.sleep(0.5)

        # 4. æœ€ç»ˆç»Ÿè®¡
        self._print_final_stats()

    def _process_single_row(self, row, writer):
        pid = row['problem_id']
        p_type = row['instance_type']
        instance = row['instance']
        fault_type = row['fault_type']

        # åªå¤„ç† Service
        if p_type != 'service':
            return

        logger.info(f"\nğŸš€ [Problem {pid}] å¼€å§‹å¤„ç† | Service: {instance} | Fault: {fault_type}")

        # æ—¶é—´è®¡ç®—
        try:
            start_dt = datetime.strptime(row['start_time'], "%Y-%m-%d %H:%M:%S")
            end_dt = datetime.strptime(row['end_time'], "%Y-%m-%d %H:%M:%S")
            
            # ç¼“å†²
            s_ts = int((start_dt + timedelta(seconds=self.args.buffer)).timestamp())
            e_ts = int((end_dt - timedelta(seconds=self.args.buffer)).timestamp())
            
            # å…¨é‡æ‹‰å–èŒƒå›´
            fetch_s_ts = int(start_dt.timestamp())
            fetch_e_ts = int(end_dt.timestamp())
            
            if e_ts <= s_ts:
                s_ts, e_ts = fetch_s_ts, fetch_e_ts # ç¼“å†²æ— æ•ˆåˆ™å›é€€

        except Exception as e:
            logger.error(f"æ—¶é—´è§£æé”™è¯¯: {e}")
            return

        # æå– ID
        query = f'serviceName: "{instance}"'
        trace_ids = self.extractor.find_trace_ids(query, s_ts, e_ts, self.args.limit)
        
        if not trace_ids:
            logger.warning("   âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ Trace")
            return

        # æå–å…¨é‡æ•°æ®å¹¶å†™å…¥
        meta = {
            'fault_type': fault_type,
            'fault_instance': instance,
            'problem_id': pid
        }
        
        count = self.extractor.fetch_full_traces(trace_ids, fetch_s_ts, fetch_e_ts, meta, writer, self.existing_trace_ids)
        
        # æ›´æ–°ç»Ÿè®¡ (æŒ‰ fault_type)
        self.stats[fault_type] += count

    def _print_final_stats(self):
        logger.info("\n" + "="*40)
        logger.info("ğŸ“Š æ‰§è¡Œå®Œæˆï¼Fault Type æ ·æœ¬ç»Ÿè®¡å¦‚ä¸‹ï¼š")
        logger.info("="*40)
        
        total_traces = 0
        if not self.stats:
            logger.info("   (æ— æ–°å¢æ•°æ®)")
        else:
            # æŒ‰æ•°é‡å€’åºæ’åˆ—
            sorted_stats = sorted(self.stats.items(), key=lambda x: x[1], reverse=True)
            for f_type, count in sorted_stats:
                logger.info(f"   ğŸ”¹ {f_type:<20}: {count} Traces")
                total_traces += count
            
            logger.info("-" * 40)
            logger.info(f"   âˆ‘ æ€»è®¡æ–°å¢          : {total_traces} Traces")
        
        logger.info(f"   ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³      : {self.output_path}")
        logger.info(f"   ğŸ“ è¯¦ç»†æ—¥å¿—          : {LOG_FILENAME}")
        logger.info("="*40)

def main():
    parser = argparse.ArgumentParser(description="å…¨å±€ Trace æå–å·¥å…·")
    parser.add_argument("--csv", default="dataset/b_gt.csv", help="b_gt.csv è·¯å¾„")
    parser.add_argument("--output-dir", default="data/ServiceFault", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--limit", type=int, default=3000, help="å•æ•…éšœæå–ä¸Šé™")
    parser.add_argument("--buffer", type=int, default=60, help="ç¼“å†²æ—¶é—´(ç§’)")
    parser.add_argument("--range", help="æŒ‡å®š Problem ID èŒƒå›´ (å¦‚ 2,10)")
    
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    
    processor = UnifiedProcessor(args)
    processor.process_all()

if __name__ == "__main__":
    main()