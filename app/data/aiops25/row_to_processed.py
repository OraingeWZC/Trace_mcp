#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# åŸºäºä½ ç°æœ‰çš„ç®€æ˜“è„šæœ¬åšçš„â€œæœ€å°æ”¹åŠ¨â€ç‰ˆï¼šä¿ç•™åŸæœ‰å‡½æ•°ä¸å–æ•°æ–¹å¼ï¼Œä»…å°†å¤„ç†èŒƒå›´æ‰©å±•åˆ° aiops25 ä¸‹æ‰€æœ‰æ—¥æœŸç›®å½•ï¼Œ
# å¹¶æŒ‰å¤©è¾“å‡ºåˆ° Data/YYYY-MM-DD.csvï¼›è¡¥å……äº† 0~1 å½’ä¸€åŒ–åˆ—åå¯¹é½ä¸æ‹“æ‰‘åºåˆ—å·ã€‚
# æ¥æºï¼šä½ æä¾›çš„ dataprocess.pyï¼ˆåœ¨æ­¤åŸºç¡€ä¸Šè½»é‡æ”¹é€ ï¼‰ :contentReference[oaicite:0]{index=0}

import argparse, json, pathlib, pandas as pd, numpy as np
from typing import List

# ---------- å·¥å…·å‡½æ•°ï¼ˆå°½é‡ä¿æŒåŸæ ·ï¼‰ ----------
def ndarray2dict(obj) -> dict:
    if isinstance(obj, str):
        try:
            obj = json.loads(obj)
        except Exception:
            return {}
    if isinstance(obj, np.ndarray):
        return {d["key"]: d["value"] for d in obj if isinstance(d, dict) and "key" in d}
    if isinstance(obj, dict):
        return obj
    return {}

def pick_tag(obj, *keys):
    d = ndarray2dict(obj)
    for k in keys:
        if isinstance(d, dict) and d.get(k):
            return d[k]
    return ""

def parent_from_ref(obj) -> str:
    if isinstance(obj, str):
        data = json.loads(obj) if obj.startswith('[') else []
    elif isinstance(obj, np.ndarray):
        data = list(obj)
    elif isinstance(obj, list):
        data = obj
    else:
        data = []
    for ref in data:
        if isinstance(ref, dict) and str(ref.get("refType", "")).upper() in ("CHILD_OF", "CHILDOF", "FOLLOWS_FROM", "FOLLOWSFROM"):
            return str(ref.get("spanID") or ref.get("spanId") or "")
    return "-1"

def to_ms(x) -> float:
    """æŠŠå„ç§æ—¶é—´å•ä½ç»Ÿä¸€è½¬æˆæ¯«ç§’æ•´æ•°ï¼š
       - >1e14 è§†ä¸ºå¾®ç§’ â†’ ms
       - >1e11 è§†ä¸ºæ¯«ç§’ â†’ ms
    """
    try:
        xi = int(x)
    except Exception:
        return 0.0
    if xi > 10 ** 14:  # Î¼s
        return xi / 1_000.0
    if xi > 10 ** 11:  # ms
        return float(xi)

# ---------- å•ä¸ª parquet è¯»å–å¹¶è½¬è¡Œ ----------
def process_parquet_file(pf: pathlib.Path, rows: List[dict]):
    df = pd.read_parquet(pf)
    # å…¼å®¹ä¸åŒå¯¼å‡ºå­—æ®µå
    col_trace = "traceID" if "traceID" in df.columns else ("traceId" if "traceId" in df.columns else "trace_id")
    col_span  = "spanID"  if "spanID"  in df.columns else ("spanId"  if "spanId"  in df.columns else "span_id")
    col_oper  = "operationName" if "operationName" in df.columns else ("name" if "name" in df.columns else None)
    col_start = "startTime" if "startTime" in df.columns else ("start_time" if "start_time" in df.columns else None)
    col_dur   = "duration" if "duration" in df.columns else ("durationMs" if "durationMs" in df.columns else None)

    # process åˆ—é‡Œä¸€èˆ¬åŒ…å« tags/servicename ç­‰
    for _, r in df.iterrows():
        proc = ndarray2dict(r.get("process", {}))
        proc_tags = proc.get("tags", {})
        span_tags = ndarray2dict(r.get("tags", {}))
        status_code_str = pick_tag(span_tags, "status.code", "http.status_code")  # ä¼˜å…ˆ OTelï¼Œå†å…œåº• HTTP
        try:
            status_code = int(status_code_str) if status_code_str not in ("", None) else 0
        except Exception:
            status_code = 0
        span_kind = pick_tag(span_tags, "span.kind")  # client/server/internal ç­‰

        trace_id = str(r.get(col_trace, ""))
        span_id  = str(r.get(col_span,  ""))
        parent   = parent_from_ref(r.get("references", []))

        # ä¸‰å±‚å½’å±ï¼ˆæŒ‰ä½ çš„å£å¾„ï¼‰
        node_name    = pick_tag(proc_tags, "node_name", "nodeName", "k8s.node.name", "host.name")
        service_name = proc.get("serviceName", "") or pick_tag(proc_tags, "service.name", "serviceName")
        pod_name     = pick_tag(proc_tags, "name", "podName", "k8s.pod.name", "pod.name")

        # URL / æ“ä½œåå…œåº•
        url = str(r.get(col_oper, "")) if col_oper else ""
        if not url:
            url = pick_tag(ndarray2dict(r.get("tags", {})), "http.url", "url", "http.target", "rpc.method")

        http_code_str = pick_tag(
            span_tags,
            "http.status_code",  # å¸¸è§é”®
            "http.response.status_code"  # å…œåº•ï¼ˆæœ‰äº›å¯¼å‡ºç”¨è¿™ä¸ªï¼‰
        )
        try:
            http_status_code = int(http_code_str) if http_code_str not in ("", None) else -1
        except Exception:
            http_status_code = -1  # é HTTP span æˆ–è§£æå¤±è´¥ï¼Œç½®ä¸º -1

        # å¼€å§‹/ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        st = to_ms(r.get(col_start, 0)) if col_start else 0.0
        du = float(r.get(col_dur, 0)) if col_dur else 0.0
        ed = st + (du / 1000.0) # duration å¾®ç§’è½¬æ¯«ç§’

        rows.append({
            "TraceID"  : trace_id,
            "SpanId"   : span_id,
            "ParentID" : parent,
            "NodeName" : node_name,
            "ServiceName": service_name,
            "PodName"  : pod_name,
            "URL"      : url,
            "StatusCode": status_code,  # int, 0 è¡¨æ­£å¸¸ï¼Œé 0 è§†ä¸ºé”™è¯¯
            "HttpStatusCode": http_status_code,
            "SpanKind": span_kind,
            "StartTimeMs": st,
            "EndTimeMs"  : ed,
        })

# ---------- æŒ‰å¤©å½’ä¸€åŒ–ä¸å†™ç›˜ ----------
def finalize_and_write(rows: List[dict], out_csv: pathlib.Path):
    if not rows:
        print(f"[WARN] æ— æ•°æ®å¯å†™ï¼š{out_csv}")
        return
    spans = pd.DataFrame(rows)

    # ä»¥ Trace ä¸ºå•ä½åš 0~1 å½’ä¸€åŒ–ï¼ˆä¸ä½ ç°æœ‰ç®¡çº¿ä¸€è‡´ï¼‰
    trace_range = spans.groupby("TraceID").agg(
        start_min=("StartTimeMs", "min"),
        end_max  =("EndTimeMs",   "max")
    )
    spans = spans.merge(trace_range, left_on="TraceID", right_index=True)
    dur = (spans["end_max"] - spans["start_min"]).clip(lower=1e-6)
    spans["Normalized_StartTime"] = (spans["StartTimeMs"] - spans["start_min"]) / dur
    spans["Normalized_EndTime"]   = (spans["EndTimeMs"]   - spans["start_min"]) / dur
    spans = spans.drop(columns=["start_min", "end_max"])

    # Trace å†…æŒ‰å¼€å§‹/ç»“æŸ/SpanId ç¨³å®šæ’åºå¹¶ç”Ÿæˆæ‹“æ‰‘åºå·
    spans = spans.sort_values(["TraceID", "StartTimeMs", "EndTimeMs", "SpanId"])
    # spans["Normalized_tree_span_ids"] = spans.groupby("TraceID").cumcount()

    # è¾“å‡ºåˆ—é¡ºåºï¼ˆä¾¿äºåç»­ groundtruth å¯¹é½ä¸ç‰¹å¾æ„å»ºï¼‰
    cols = [
        "TraceID","SpanId","ParentID",
        "NodeName","ServiceName","PodName","URL",
        "HttpStatusCode", "StatusCode", "SpanKind",
        "StartTimeMs", "EndTimeMs",
        "Normalized_StartTime", "Normalized_EndTime"
    ]
    spans = spans[cols]

    out_csv.parent.mkdir(parents=True, exist_ok=True)
    spans.to_csv(out_csv, index=False, float_format="%.8f")
    print(f"â†’ å†™å®Œ {out_csv}  å…± {len(spans)} è¡Œ")


# ---------- æ‰«æ aiops25 ç›®å½•ä¸‹æ‰€æœ‰æ—¥æœŸå¹¶å¤„ç† ----------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", default="../../dataset/aiops25/row", help="aiops25 æ ¹ç›®å½•")
    ap.add_argument("--out-dir", default="../dataset/aiops25/processed", help="æŒ‰å¤©è¾“å‡ºç›®å½•ï¼ˆData/YYYY-MM-DD.csvï¼‰")
    ap.add_argument("--pattern", default="2025-??-??", help="æ—¥æœŸç›®å½•åŒ¹é…æ¨¡å¼ï¼ˆé»˜è®¤ 2025-??-??ï¼‰")
    args = ap.parse_args()

    root = pathlib.Path(args.root)
    out_root = pathlib.Path(args.out_dir)
    # ä¸€å±‚æ—¥æœŸç›®å½•ï¼ˆå¦‚ aiops25/2025-06-06ï¼‰ï¼Œå…¶ä¸‹ä¸€å±‚åŒåç›®å½•ï¼ˆaiops25/2025-06-06/2025-06-06/trace-parquetï¼‰
    date_dirs = sorted(root.glob(args.pattern))

    if not date_dirs:
        print(f"[WARN] åœ¨ {root} ä¸‹æœªæ‰¾åˆ°ç¬¦åˆ {args.pattern} çš„æ—¥æœŸ ç›®å½•")
        return

    for d in date_dirs:
        inner = d / d.name / "trace-parquet"
        if not inner.exists():
            print(f"[WARN] è·³è¿‡ï¼š{inner} ä¸å­˜åœ¨")
            continue

        # import shutil
        # # ğŸ”¥ æ¸…ç†åŒçº§é trace-parquet ç›®å½•
        # parent_dir = inner.parent
        # for item in parent_dir.iterdir():
        #     if item.is_dir() and item.name != "trace-parquet":
        #         print(f"[CLEAN] åˆ é™¤ç›®å½•ï¼š{item}")
        #         shutil.rmtree(item, ignore_errors=True)

        # æœ¬æ—¥è¾“å‡ºæ–‡ä»¶
        out_csv = out_root / f"{d.name}.csv"
        print(f"\n=== å¤„ç†æ—¥æœŸ {d.name} ===")
        print(f"æ‰«æç›®å½•ï¼š{inner}")

        # æ”¶é›†æ‰€æœ‰ parquet / parguet
        pq_files = sorted(list(inner.glob("*.parquet")))
        if not pq_files:
            print(f"[WARN] {inner} ä¸‹æ²¡æœ‰ parquet/parguet æ–‡ä»¶ï¼Œè·³è¿‡")
            continue

        rows: List[dict] = []
        for i, pf in enumerate(pq_files, 1):
            print(f"[{i}/{len(pq_files)}] Read {pf.name}")
            try:
                process_parquet_file(pf, rows)
            except Exception as e:
                print(f"[ERROR] è¯»å– {pf.name} å¤±è´¥ï¼š{e}")

        finalize_and_write(rows, out_csv)


if __name__ == "__main__":
    main()
