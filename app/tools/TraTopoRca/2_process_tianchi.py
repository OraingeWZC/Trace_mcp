import os
import argparse
import shutil
import sqlite3
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm

# å¼•å…¥é¡¹ç›®ä¾èµ–
from tracegnn.data.trace_graph import df_to_trace_graphs, TraceGraphIDManager
from tracegnn.data.trace_graph_db import TraceGraphDB, BytesSqliteDB
from tracegnn.utils.host_state import host_state_vector

# ================= é…ç½®åŒºåŸŸ =================
DEFAULT_DATASET_ROOT = 'dataset/tianchi/0112' 
# æŒ‡æ ‡æ–‡ä»¶åï¼Œè¯·ç¡®ä¿è¿™ä¸ä½  3_allfault_nodeMetric.py ç”Ÿæˆçš„æ–‡ä»¶åä¸€è‡´
INFRA_FILENAME = 'all_metrics_10s.csv' 

# Host Sequence é…ç½® (éœ€ä¸ dataset.py / config.py ä¿æŒä¸€è‡´)
SEQ_WINDOW = 15
# [ä¿®æ”¹] æ‰©å……åºåˆ—æŒ‡æ ‡åˆ«åï¼Œä»¥åŒ…å«ç½‘ç»œå’Œç£ç›˜
SEQ_METRICS = ['cpu', 'mem', 'disk', 'net', 'tcp'] 

# [æ–°å¢] å¤©æ± æ•°æ®çš„çœŸå®æŒ‡æ ‡åˆ—å (è¯·ç¡®ä¿ä¸ CSV è¡¨å¤´ä¸€è‡´)
TIANCHI_METRICS = [
    "aggregate_node_cpu_usage",
    "aggregate_node_memory_usage",
    "aggregate_node_disk_io_usage",
    "aggregate_node_net_receive_packages_errors_per_minute",
    "aggregate_node_tcp_alloc_total_num",
    "aggregate_node_tcp_inuse_total_num"
]
# ===========================================

def flexible_load_trace_csv(input_path: str) -> pd.DataFrame:
    if not os.path.exists(input_path):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return pd.DataFrame()
    try:
        df = pd.read_csv(input_path)
        if 'Duration' in df.columns: df['Duration'] = pd.to_numeric(df['Duration'], errors='coerce')
        if 'StartTimeMs' in df.columns: df['StartTime'] = pd.to_numeric(df['StartTimeMs'], errors='coerce')
        if 'Anomaly' in df.columns: df['Anomaly'] = df['Anomaly'].astype(bool)
        return df
    except Exception as e:
        print(f"åŠ è½½CSVå‡ºé”™ {input_path}: {e}")
        return pd.DataFrame()

def load_infra_data_from_parent(dataset_root: str):
    """
    åŠ è½½å¹¶è§£ææŒ‡æ ‡æ•°æ®ï¼Œé€‚é…å¤©æ± æ•°æ®æ ¼å¼
    """
    parent_dir = os.path.dirname(dataset_root.rstrip(os.path.sep))
    infra_path = os.path.join(parent_dir, INFRA_FILENAME)
    
    # å°è¯•åœ¨ä¸åŒä½ç½®æŸ¥æ‰¾æ–‡ä»¶
    if not os.path.exists(infra_path):
        infra_path_alt = os.path.join(parent_dir, 'infra', INFRA_FILENAME)
        if os.path.exists(infra_path_alt): infra_path = infra_path_alt
    
    # å°è¯•æŸ¥æ‰¾ data ç›®å½• (é€šå¸¸æ˜¯ step 2/3 çš„è¾“å‡ºç›®å½•)
    if not os.path.exists(infra_path):
        infra_path_alt2 = os.path.join(parent_dir, 'data', INFRA_FILENAME)
        if os.path.exists(infra_path_alt2): infra_path = infra_path_alt2

    if not os.path.exists(infra_path):
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æŒ‡æ ‡æ•°æ®æ–‡ä»¶ã€‚æœŸæœ›è·¯å¾„: {infra_path}")
        return None
    
    print(f"âœ… å·²åŠ è½½æŒ‡æ ‡æ•°æ®: {infra_path}")
    try:
        df = pd.read_csv(infra_path)
        
        # [å…³é”®ä¿®æ”¹] 1. æ£€æŸ¥å…³é”®åˆ—
        if 'timestamp' not in df.columns or 'instance_id' not in df.columns:
            print("âŒ é”™è¯¯: CSVç¼ºå°‘å…³é”®åˆ— 'timestamp' æˆ– 'instance_id'ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")
            return None

        # [å…³é”®ä¿®æ”¹] 2. æ—¶é—´æˆ³è½¬æ¢: çº³ç§’ (19ä½) -> æ¯«ç§’ (13ä½)
        # å¤©æ±  timestamp e.g., 1758036033000000000
        df['timeMs'] = df['timestamp'].astype(np.int64) // 1000000
        
        # [å…³é”®ä¿®æ”¹] 3. èŠ‚ç‚¹IDæ˜ å°„: instance_id -> kubernetes_node
        # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†è®©åç»­ GNN ä»£ç èƒ½é€šè¿‡ 'kubernetes_node' è¿™ä¸ªæ ‡å‡†å­—æ®µæ‰¾åˆ°æ•°æ®
        df['kubernetes_node'] = df['instance_id'].astype(str)

        # è¿‡æ»¤éœ€è¦çš„æŒ‡æ ‡åˆ—ï¼Œå¹¶å¡«å……ç¼ºå¤±å€¼
        metric_cols = [c for c in TIANCHI_METRICS if c in df.columns]
        for m in metric_cols:
            df[m] = pd.to_numeric(df[m], errors='coerce').fillna(0.0)
        
        # æ„å»ºç´¢å¼• (æŒ‰èŠ‚ç‚¹åˆ†ç»„ï¼ŒæŒ‰æ—¶é—´æ’åº)
        # åªä¿ç•™éœ€è¦çš„åˆ—
        cols = ['timeMs', 'kubernetes_node'] + metric_cols
        df = df[cols].dropna(subset=['timeMs', 'kubernetes_node'])
        
        host_idx = {}
        for host, g in tqdm(df.groupby('kubernetes_node'), desc="æ„å»ºæŒ‡æ ‡ç´¢å¼•"):
            lg = g.sort_values('timeMs')
            # å»é‡ï¼šé˜²æ­¢åŒä¸€æ¯«ç§’æœ‰å¤šæ¡æ•°æ® (å–æœ€åä¸€æ¡)
            lg = lg.drop_duplicates(subset=['timeMs'], keep='last')
            
            host_idx[str(host)] = {
                'timeMs': lg['timeMs'].to_numpy(dtype=np.int64),
                'metrics': {m: lg[m].to_numpy(dtype=np.float64) for m in metric_cols}
            }
        return host_idx
    except Exception as e:
        print(f"è§£ææŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

# === 1. HostState é¢„è®¡ç®— (GNN) ===
def precompute_host_states(trace_graphs, infra_index, id_manager, W=3):
    if infra_index is None: return
    
    # [ä¿®æ”¹] å¼ºåˆ¶ä½¿ç”¨å¤©æ± æŒ‡æ ‡åˆ—è¡¨ä½œä¸º GNN çš„è¾“å…¥ç‰¹å¾
    metrics = TIANCHI_METRICS
    per_metric_dims = 4  # mean, std, max, min

    for graph in tqdm(trace_graphs, desc="é¢„è®¡ç®— HostState (GNN)"):
        try:
            st = graph.root.spans[0].start_time if (graph.root and graph.root.spans) else None
            if isinstance(st, (int, float)):
                v = float(st)
                t0_ms = int(v if v > 1e12 else v * 1000.0)
            else:
                t0_ms = 0
            t0_min_ms = (t0_ms // 60000) * 60000
            
            host_ids = set(node.host_id for _, node in graph.iter_bfs() if node.host_id and node.host_id > 0)
            host_state_map = {}
            for hid in host_ids:
                hname = id_manager.host_id.rev(int(hid))
                if hname:
                    # host_state_vector ä¼šå» metrics å­—å…¸é‡Œæ‰¾å¯¹åº”çš„ key
                    vec = host_state_vector(hname, infra_index, t0_min_ms, metrics=metrics, W=W, per_metric_dims=per_metric_dims)
                    if vec is not None:
                        host_state_map[hid] = vec
            if host_state_map:
                graph.data['precomputed_host_state'] = host_state_map
        except Exception:
            continue

# === 2. HostSequence é¢„è®¡ç®— (OmniAnomaly) ===
def precompute_host_sequences(trace_graphs, infra_index, id_manager):
    """é¢„å…ˆè®¡ç®—ç”¨äº OmniAnomaly çš„æ—¶é—´åºåˆ—æ•°æ® [Window, Metrics]"""
    if infra_index is None: return

    # [ä¿®æ”¹] æ˜ å°„é…ç½®é‡Œçš„åˆ«å -> å¤©æ± çœŸå®åˆ—å
    def _map_metric(alias: str) -> str:
        alias = str(alias).lower().strip()
        
        # CPU
        if alias in ('cpu',): 
            return 'aggregate_node_cpu_usage'
            
        # Memory
        if alias in ('mem', 'memory'): 
            return 'aggregate_node_memory_usage'
            
        # Disk (å¯¹åº” node diskchaos)
        if alias in ('fs', 'filesystem', 'disk', 'io'): 
            return 'aggregate_node_disk_io_usage'
            
        # Network (å¯¹åº” node networkchaos, ä¸»è¦æ˜¯ä¸¢åŒ…/é”™åŒ…)
        if alias in ('net', 'network'): 
            return 'aggregate_node_net_receive_packages_errors_per_minute'
            
        # TCP (è¾…åŠ©ç½‘ç»œç‰¹å¾)
        if alias in ('tcp',):
            return 'aggregate_node_tcp_inuse_total_num'
            
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œé»˜è®¤è¿”å›åŸåï¼ˆä»¥é˜²ç›´æ¥ä½¿ç”¨äº†çœŸå®åˆ—åï¼‰
        return alias
    
    metrics_cols = [_map_metric(a) for a in SEQ_METRICS]
    W = SEQ_WINDOW

    def _robust_norm(x):
        med = np.nanmedian(x)
        q1, q3 = np.nanpercentile(x, 25), np.nanpercentile(x, 75)
        iqr = q3 - q1
        stdv = np.nanstd(x)
        denom = iqr if (iqr is not None and iqr > 1e-6) else (stdv if stdv > 1e-6 else 1.0)
        z = (x - med) / denom
        return np.nan_to_num(z, nan=0.0, posinf=0.0, neginf=0.0)

    for graph in tqdm(trace_graphs, desc="é¢„è®¡ç®— HostSeq (OmniAnomaly)"):
        try:
            # è®¡ç®— t0 (åˆ†é’Ÿå¯¹é½)
            st = graph.root.spans[0].start_time if (graph.root and graph.root.spans) else None
            if isinstance(st, (int, float)):
                v = float(st)
                t0_ms = int(v if v > 1e12 else v * 1000.0)
            else:
                t0_ms = 0
            t0_min = (t0_ms // 60000) * 60000

            host_ids = set(node.host_id for _, node in graph.iter_bfs() if node.host_id and node.host_id > 0)
            host_seq_map = {}

            for hid in host_ids:
                hname = id_manager.host_id.rev(int(hid))
                if not hname: continue
                
                rec = infra_index.get(str(hname))
                if not rec: continue
                
                t_arr = rec.get('timeMs', [])
                if len(t_arr) == 0: continue
                
                per_metric = []
                for mcol in metrics_cols:
                    # å°è¯•è·å–æŒ‡æ ‡æ•°æ®ï¼Œå¦‚æœåˆ—åä¸å¯¹åˆ™è¿”å›ç©ºåˆ—è¡¨
                    vals = rec.get('metrics', {}).get(mcol, [])
                    if len(vals) == 0:
                        # å®¹é”™ï¼šå¦‚æœæŒ‡æ ‡ä¸å­˜åœ¨ï¼Œå¡«å……å…¨0åºåˆ—
                        seq_vals_np = np.zeros(W, dtype=np.float64)
                    else:
                        seq_vals = []
                        for k in range(W):
                            target = t0_min - (W - 1 - k) * 60000
                            # æ‰¾åˆ° <= target çš„æœ€åä¸€ä¸ªç‚¹
                            pos = int(np.searchsorted(t_arr, target, side='right')) - 1
                            if pos >= 0:
                                seq_vals.append(float(vals[pos]))
                            else:
                                seq_vals.append(np.nan)
                        seq_vals_np = np.array(seq_vals, dtype=np.float64)
                    
                    norm_vals = _robust_norm(seq_vals_np)
                    per_metric.append(norm_vals.astype(np.float32))
                
                if per_metric:
                    # shape: [Window, Metrics] e.g. [15, 5]
                    mat = np.stack(per_metric, axis=1)
                    host_seq_map[int(hid)] = torch.from_numpy(mat)
            
            if host_seq_map:
                graph.data['precomputed_host_seq'] = host_seq_map

        except Exception:
            continue

def process_split(split_name, dataset_root, id_manager, infra_index, processed_df=None):
    raw_csv = os.path.join(dataset_root, 'raw', f'{split_name}.csv')
    out_dir = os.path.join(dataset_root, 'processed', split_name)
    if not os.path.exists(raw_csv) and processed_df is None: return

    print(f"\n=== å¤„ç† {split_name} é›† ===")
    os.makedirs(out_dir, exist_ok=True)
    
    if processed_df is not None: df = processed_df
    else: df = flexible_load_trace_csv(raw_csv)

    if df.empty: return

    trace_graphs = df_to_trace_graphs(df=df, id_manager=id_manager, min_node_count=2, max_node_count=100, summary_file=None, merge_spans=False)
    if not trace_graphs: return

    # === æ‰§è¡Œä¸¤é¡¹é¢„è®¡ç®— ===
    precompute_host_states(trace_graphs, infra_index, id_manager)    # GNN ç”¨
    precompute_host_sequences(trace_graphs, infra_index, id_manager) # OmniAnomaly ç”¨
    # ====================

    db_path = os.path.join(out_dir, "_bytes.db")
    if not os.path.exists(db_path): open(db_path, 'a').close()
    db = TraceGraphDB(BytesSqliteDB(out_dir, write=True))
    try:
        with db.write_batch():
            for graph in trace_graphs:
                if hasattr(graph, 'root_cause') and graph.root_cause is None: graph.root_cause = 0
                if hasattr(graph, 'fault_category') and graph.fault_category is None: graph.fault_category = 0
                db.add(graph)
        db.commit()
        print(f"  âœ… æˆåŠŸå†™å…¥ {len(trace_graphs)} ä¸ªå›¾åˆ° {split_name} æ•°æ®åº“")
    finally:
        db.close()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--root', type=str, default=DEFAULT_DATASET_ROOT)
    args = parser.parse_args()
    
    dataset_root = args.root
    print(f"ğŸš€ å¼€å§‹å¤„ç†æ•°æ®æµ (é€‚é…å¤©æ± æŒ‡æ ‡)ï¼Œæ ¹ç›®å½•: {dataset_root}")
    processed_root = os.path.join(dataset_root, 'processed')
    os.makedirs(processed_root, exist_ok=True)
    
    # 1. åŠ è½½æŒ‡æ ‡æ•°æ®
    infra_index = load_infra_data_from_parent(dataset_root)
    
    # 2. å»ºç«‹ ID æ˜ å°„
    print("\n[æ­¥éª¤ 1/4] å»ºç«‹ç»Ÿä¸€ ID æ˜ å°„...")
    combined_dfs = []
    for split in ['train', 'val', 'test']:
        path = os.path.join(dataset_root, 'raw', f'{split}.csv')
        df = flexible_load_trace_csv(path)
        if not df.empty: combined_dfs.append(df)
    if not combined_dfs: return
    temp_id_dir = os.path.join(dataset_root, 'temp_ids')
    os.makedirs(temp_id_dir, exist_ok=True)
    id_manager = TraceGraphIDManager(temp_id_dir)
    with id_manager:
        full_df = pd.concat(combined_dfs, ignore_index=True)
        for row in tqdm(full_df.itertuples(), total=len(full_df), desc="ç”Ÿæˆ ID"):
            id_manager.service_id.get_or_assign(getattr(row, 'ServiceName', '') or '')
            id_manager.operation_id.get_or_assign(getattr(row, 'OperationName', '') or '')
            id_manager.status_id.get_or_assign(str(getattr(row, 'StatusCode', '')) or '')
    id_manager.dump_to(processed_root)
    id_manager = TraceGraphIDManager(processed_root)
    if os.path.exists(temp_id_dir): shutil.rmtree(temp_id_dir)

    # 3. å¤„ç†æ•°æ®
    process_split('train', dataset_root, id_manager, infra_index)
    process_split('val', dataset_root, id_manager, infra_index)

    print("\n[æ­¥éª¤ 3/4] å¤„ç†æµ‹è¯•é›†...")
    test_csv_path = os.path.join(dataset_root, 'raw', 'test.csv')
    test_df = flexible_load_trace_csv(test_csv_path)
    if not test_df.empty:
        for col in ['RootCause', 'FaultCategory']:
            if col not in test_df.columns: test_df[col] = ''
        for idx, row in test_df.iterrows():
            if row.get('Anomaly'):
                rc_text = str(row.get('RootCause', '')).strip()
                fc_text = str(row.get('FaultCategory', '')).strip()
                mapped_id = None
                if fc_text.lower().startswith('node'):
                    rc_text = rc_text.replace('_', '-')
                    mapped_id = id_manager.host_id.get(rc_text)
                else:
                    rc_svc = rc_text.split('-')[0] if '-' in rc_text else rc_text
                    mapped_id = id_manager.service_id.get(rc_svc)
                test_df.at[idx, 'RootCause'] = mapped_id if mapped_id is not None else 0
                test_df.at[idx, 'FaultCategory'] = id_manager.fault_category.get_or_assign(fc_text) if fc_text else 0
        process_split('test', dataset_root, id_manager, infra_index, processed_df=test_df)

    id_manager.dump_to(processed_root)
    print(f"\nâœ¨ æ‰€æœ‰å¤„ç†å®Œæˆï¼")

if __name__ == '__main__':
    main()