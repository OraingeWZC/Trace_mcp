#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import sys
import json
import csv
import time
import argparse
import ast
from datetime import datetime
import pandas as pd  # æ ¸å¿ƒï¼šå¼•å…¥ pandas è¿›è¡Œæ•°æ®èšåˆ
import numpy as np

import config

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ================= ğŸ”§ 1. åœ¨è¿™é‡Œå®šä¹‰ä½ éœ€è¦çš„â€œç²¾å‡†æŒ‡æ ‡åˆ—è¡¨â€ =================
TARGET_METRICS = [
    # --- ç½‘ç»œå…³é”®æŒ‡æ ‡ ---
    "aggregate_node_net_receive_packages_errors_per_minute", # æ ¸å¿ƒï¼šç½‘ç»œé”™åŒ…
    "aggregate_node_tcp_inuse_total_num",                    # æ ¸å¿ƒï¼šTCPè¿æ¥æ•°
    "aggregate_node_tcp_alloc_total_num",
    
    # --- åŸºç¡€èµ„æº ---
    "aggregate_node_cpu_usage",
    "aggregate_node_memory_usage",
    "aggregate_node_disk_io_usage"
]
# =======================================================================

# é‰´æƒé…ç½®
os.environ.setdefault("ALIBABA_CLOUD_ROLE_SESSION_NAME", "my-sls-access")

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from tools.paas_entity_tools import umodel_get_entities
from tools.paas_data_tools import umodel_get_golden_metrics
from tools.common import create_cms_client, execute_cms_query
from tools.constants import REGION_ID, WORKSPACE_NAME

class BatchCustomMetricFetcher:
    def __init__(self, csv_path, output_dir=None, unified_mode=False, interval=None):
        self.csv_path = csv_path
        self.unified_mode = unified_mode
        self.interval = interval # ç›®æ ‡æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
        self.client = create_cms_client(REGION_ID)
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if output_dir:
            self.data_dir = output_dir
        else:
            self.data_dir = os.path.join(project_root, "output_datasets" if unified_mode else "data")
            
        os.makedirs(self.data_dir, exist_ok=True)

        if self.unified_mode:
            self.global_csv_path = os.path.join(self.data_dir, "all_metrics.csv")
            self.global_headers = ['problem_id', 'fault_type', 'instance_id', 'timestamp'] + sorted(TARGET_METRICS)
            
            if not os.path.exists(self.global_csv_path):
                with open(self.global_csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=self.global_headers)
                    writer.writeheader()
            print(f"âœ… [ç»Ÿä¸€æ¨¡å¼] ç»“æœå°†è¿½åŠ è‡³: {self.global_csv_path}")
            if self.interval:
                print(f"â±ï¸  [é‡é‡‡æ ·] å·²å¯ç”¨æ•°æ®èšåˆ: æ¯ {self.interval} ç§’ä¸€æ¡")
        else:
            print(f"âœ… [åˆ†æ•£æ¨¡å¼] ç»“æœå°†ä¿å­˜è‡³: {self.data_dir}/problem_XXX/")

    def _parse_time(self, t_str):
        if not t_str: return int(time.time())
        try:
            return int(datetime.strptime(t_str, '%Y-%m-%d %H:%M:%S').timestamp())
        except ValueError:
            try:
                return int(t_str)
            except:
                return int(time.time())

    def fetch_metrics_for_problem(self, row):
        problem_id = row['problem_id']
        fault_type = row.get('fault_type', 'unknown')
        start_ts = self._parse_time(row['start_time']) - 180 # æå‰3åˆ†é’Ÿï¼Œé˜²æ­¢æ•°æ®ç¼ºå¤±
        end_ts = self._parse_time(row['end_time'])
        
        print(f"\nğŸš€ [Problem {problem_id}] å¤„ç†ä¸­... ({row['start_time']} ~ {row['end_time']})")

        # 1. æŸ¥æ‰¾æ´»è·ƒ ECS
        entity_query = {
            "domain": "acs",
            "entity_set_name": "acs.ecs.instance",
            "from_time": start_ts,
            "to_time": end_ts,
            "limit": 100 
        }
        nodes_result = umodel_get_entities.invoke(entity_query)
        
        if not nodes_result or not nodes_result.data:
            print(f"   âš ï¸ æ­¤æ—¶æ®µæœªå‘ç°æ´»è·ƒ ECS èŠ‚ç‚¹")
            return 0, 0

        nodes = nodes_result.data
        print(f"   ğŸ” äº‘ç«¯æŸ¥è¯¢åˆ°æ´»è·ƒèŠ‚ç‚¹: {len(nodes)} ä¸ª")

        valid_node_metrics = {}
        
        # 2. è·å–æŒ‡æ ‡
        for node in nodes:
            node_name = node.get('instance_id')
            entity_id = node.get('__entity_id__')
            if not entity_id: continue

            node_data = {}

            # ç­–ç•¥ A: Golden Metrics
            gm_res = umodel_get_golden_metrics.invoke({
                "domain": "acs",
                "entity_set_name": "acs.ecs.instance",
                "entity_ids": [entity_id],
                "from_time": start_ts,
                "to_time": end_ts
            })
            
            if gm_res and gm_res.data:
                for item in gm_res.data:
                    m_name = item.get('metric')
                    if m_name in TARGET_METRICS:
                        self._extract_value(item, m_name, node_data)

            # ç­–ç•¥ B: è¡¥ç¼º
            missing = [m for m in TARGET_METRICS if m not in node_data]
            if missing:
                for m in missing:
                    query = f".entity_set with(domain='acs', name='acs.ecs.instance', ids=['{entity_id}']) | entity-call get_metric('{m}')"
                    try:
                        res = execute_cms_query(self.client, WORKSPACE_NAME, query, start_ts, end_ts)
                        if res and res.data:
                            vals, ts = [], []
                            for r in res.data:
                                v = r.get('value') or r.get(m)
                                t = r.get('timestamp') or r.get('ts')
                                if v is not None: vals.append(v); ts.append(t)
                            if vals:
                                node_data[m] = {"values": vals, "timestamps": ts}
                    except:
                        pass

            if node_data:
                valid_node_metrics[node_name] = node_data

        if not valid_node_metrics:
            print("   âš ï¸ æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæŒ‡æ ‡æ•°æ®")
            return 0, 0

        # ğŸ”¥ã€å…³é”®ã€‘åœ¨è¿™é‡Œè°ƒç”¨é‡é‡‡æ ·
        if self.interval and self.interval > 0:
            valid_node_metrics = self._resample_data(valid_node_metrics, self.interval)

        # ç»Ÿè®¡
        batch_records = 0
        print(f"   ğŸ“‰ èŠ‚ç‚¹æ•°æ®è¯¦æƒ…" + (f" (å·²èšåˆè‡³ {self.interval}s)" if self.interval else "") + ":")
        for nid, m_data in valid_node_metrics.items():
            unique_ts = set()
            for metrics in m_data.values():
                unique_ts.update(metrics.get('timestamps', []))
            cnt = len(unique_ts)
            batch_records += cnt
            print(f"      ğŸ”¹ èŠ‚ç‚¹ {nid:<20}: è·å– {cnt:>4} æ¡è®°å½•")

        # 3. ä¿å­˜
        if self.unified_mode:
            self._append_to_global_csv(problem_id, fault_type, valid_node_metrics)
        else:
            self._save_separate_files(problem_id, valid_node_metrics)
            
        return len(valid_node_metrics), batch_records

    def _resample_data(self, node_metrics, interval_sec):
        """æ ¸å¿ƒå‡½æ•°ï¼šä½¿ç”¨ Pandas å°†åŸå§‹æ•°æ®é‡é‡‡æ ·ä¸ºæŒ‡å®šé—´éš”"""
        resampled_metrics = {}
        
        for node_id, metrics_dict in node_metrics.items():
            df_all = pd.DataFrame()
            
            for metric_name, data in metrics_dict.items():
                ts_list = data.get('timestamps', [])
                val_list = data.get('values', [])
                
                if not ts_list: continue
                
                # åˆ›å»ºä¸´æ—¶ DF
                df_temp = pd.DataFrame({'ts': ts_list, metric_name: val_list})
                # çº³ç§’è½¬ datetime
                df_temp['ts'] = pd.to_datetime(df_temp['ts'], unit='ns')
                df_temp.set_index('ts', inplace=True)
                
                # Outer Join åˆå¹¶
                if df_all.empty:
                    df_all = df_temp
                else:
                    df_all = df_all.join(df_temp, how='outer')
            
            if df_all.empty: continue
            
            # é‡é‡‡æ · (å–å¹³å‡å€¼)
            # ğŸ”¥ [ä¿®æ­£ç‚¹] ä½¿ç”¨å°å†™ 's' æ›¿ä»£å¤§å†™ 'S' ä»¥æ¶ˆé™¤ FutureWarning
            df_resampled = df_all.resample(f'{interval_sec}s').mean()
            
            # è¿˜åŸä¸ºå­—å…¸
            node_resul = {}
            new_timestamps = df_resampled.index.astype(np.int64).tolist()
            
            for col in df_resampled.columns:
                vals = df_resampled[col].fillna(0.0).tolist()
                node_resul[col] = {
                    "values": vals,
                    "timestamps": new_timestamps
                }
            
            resampled_metrics[node_id] = node_resul
            
        return resampled_metrics

    def _save_separate_files(self, problem_id, data):
        output_dir = os.path.join(self.data_dir, f"problem_{problem_id}")
        os.makedirs(output_dir, exist_ok=True)
        # JSON é‡Œçš„ int64 å¯èƒ½ä¼šåœ¨æŸäº›æŸ¥çœ‹å™¨æŠ¥é”™ï¼Œä½†Pythonè¯»å–æ²¡é—®é¢˜
        with open(os.path.join(output_dir, "custom_ecs_metrics.json"), 'w', encoding='utf-8') as f:
            # default=str ç”¨äºå¤„ç† numpy ç±»å‹
            json.dump(data, f, indent=2, ensure_ascii=False, default=str) 
        self._save_as_csv(output_dir, "custom_ecs_metrics.csv", data)
        print(f"   âœ… å·²ä¿å­˜è‡³ {output_dir}")

    def _append_to_global_csv(self, problem_id, fault_type, data):
        rows_to_write = []
        for instance_id, metrics in data.items():
            time_map = {}
            for metric_name, metric_data in metrics.items():
                values = metric_data.get('values', [])
                timestamps = metric_data.get('timestamps', [])
                for v, t in zip(values, timestamps):
                    if t not in time_map:
                        time_map[t] = {
                            'problem_id': problem_id,
                            'fault_type': fault_type,
                            'instance_id': instance_id,
                            'timestamp': t
                        }
                    time_map[t][metric_name] = v
            for ts in sorted(time_map.keys()):
                row = time_map[ts]
                for m in TARGET_METRICS:
                    if m not in row: row[m] = "" 
                rows_to_write.append(row)
        if rows_to_write:
            try:
                with open(self.global_csv_path, 'a', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=self.global_headers)
                    writer.writerows(rows_to_write)
            except Exception as e:
                print(f"   âŒ å†™å…¥å¤±è´¥: {e}")

    def _save_as_csv(self, output_dir, filename, data):
        if not data: return
        all_metrics = set()
        for instance_data in data.values():
            all_metrics.update(instance_data.keys())
        headers = ['instance_id', 'timestamp'] + sorted(list(all_metrics))
        rows = []
        for instance_id, metrics in data.items():
            time_map = {}
            for metric_name, metric_data in metrics.items():
                values = metric_data.get('values', [])
                timestamps = metric_data.get('timestamps', [])
                for v, t in zip(values, timestamps):
                    if t not in time_map:
                        time_map[t] = {'instance_id': instance_id, 'timestamp': t}
                    time_map[t][metric_name] = v
            for ts in sorted(time_map.keys()):
                rows.append(time_map[ts])
        try:
            with open(os.path.join(output_dir, filename), 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=headers)
                writer.writeheader()
                writer.writerows(rows)
        except Exception as e:
            print(f"   âŒ CSV ä¿å­˜å¤±è´¥: {e}")

    def _extract_value(self, item, metric_name, target_dict):
        try:
            vals = ast.literal_eval(item.get('__value__', '[]'))
            ts = ast.literal_eval(item.get('__ts__', '[]'))
            if vals:
                target_dict[metric_name] = {"values": vals, "timestamps": ts}
        except:
            pass

    def run(self):
        print(f"ğŸ“‚ è¯»å–ä»»åŠ¡åˆ—è¡¨: {self.csv_path}")
        if not os.path.exists(self.csv_path):
            print("âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨")
            return
        total_problems, total_nodes, total_records = 0, 0, 0
        with open(self.csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            print(f"ğŸ“‹ å…±å‘ç° {len(rows)} ä¸ªé—®é¢˜å¾…å¤„ç†...")
            start_time = time.time()
            for row in rows:
                try:
                    n_nodes, n_records = self.fetch_metrics_for_problem(row)
                    if n_nodes > 0:
                        total_problems += 1; total_nodes += n_nodes; total_records += n_records
                except Exception as e:
                    print(f"âŒ å¤„ç† Problem {row.get('problem_id')} æ—¶å‡ºé”™: {e}")
            end_time = time.time()
            print("\n" + "="*50 + f"\nğŸ“Š æ‰§è¡Œå®Œæˆæ€»ç»“ report\n" + "="*50)
            print(f"â±ï¸  æ€»è€—æ—¶       : {end_time - start_time:.2f} ç§’")
            print(f"âœ… æˆåŠŸå¤„ç†é—®é¢˜ : {total_problems} ä¸ª")
            print(f"ğŸ’» æ¶‰åŠèŠ‚ç‚¹æ€»æ•° : {total_nodes} ä¸ª")
            print(f"ğŸ“ˆ è·å–æ•°æ®è®°å½• : {total_records} æ¡")
            if self.unified_mode:
                print(f"ğŸ’¾ ç»“æœæ–‡ä»¶     : {self.global_csv_path}")
            else:
                print(f"ğŸ’¾ ç»“æœç›®å½•     : {self.data_dir}")
            print("="*50 + "\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", default="dataset/b_gt.csv", help="è·¯å¾„æŒ‡å‘ b_gt.csv")
    parser.add_argument("--unified", action="store_true", help="å¯ç”¨ç»Ÿä¸€æ¨¡å¼")
    parser.add_argument("--output-dir", default="data/NodeMetric", help="è‡ªå®šä¹‰è¾“å‡ºç›®å½•")
    
    # ğŸ”¥ æ–°å¢å‚æ•°
    parser.add_argument("--interval", type=int, default=30, help="é‡é‡‡æ ·æ—¶é—´é—´éš”(ç§’)ï¼Œä¾‹å¦‚ 60 è¡¨ç¤ºæ¯åˆ†é’Ÿä¸€æ¡")
    
    args = parser.parse_args()
    fetcher = BatchCustomMetricFetcher(args.csv, args.output_dir, args.unified, args.interval)
    fetcher.run()