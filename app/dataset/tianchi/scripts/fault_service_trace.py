# -*- coding: utf-8 -*-
"""
æ‰¹é‡ Trace æå–è„šæœ¬ (æœåŠ¡çº§æ•…éšœä¸“ç”¨) - ä¿®æ­£é‰´æƒ & å¢åŠ ç»Ÿè®¡ç‰ˆ
åŠŸèƒ½ï¼šè¯»å– b_gt.csv -> è¿‡æ»¤æœåŠ¡æ•…éšœ -> æ—¶é—´ç¼“å†² -> ç»Ÿè®¡å‘½ä¸­æ•° -> æå– TraceID -> æ‹‰å–å…¨é‡é“¾è·¯ -> ä¿å­˜ CSV
"""

import os
import json
import csv
import time
import argparse
from datetime import datetime, timedelta
from aliyun.log import LogClient, GetLogsRequest
from alibabacloud_sts20150401.client import Client as StsClient
from alibabacloud_sts20150401 import models as sts_models
from alibabacloud_tea_openapi import models as open_api_models
from Tea.exceptions import TeaException

import app.dataset.tianchi.config as config

# ================= ğŸ”§ åŸºç¡€é…ç½® =================
PROJECT_NAME = config.SLS_PROJECT_NAME
LOGSTORE_NAME = config.SLS_LOGSTORE_NAME
REGION = config.SLS_REGION

# è¾“å‡º CSV è¡¨å¤´
CSV_HEADERS = [
    'TraceID', 'SpanId', 'ParentID', 
    'ServiceName', 'NodeName', 'PodName', 
    'URL', 'SpanKind', 
    'StartTimeMs', 'EndTimeMs', 'DurationMs',
    'StatusCode', 'HttpStatusCode', 
    'fault_type', 'fault_instance'
]

# ===============================================

def get_sts_credentials(region: str = "cn-qingdao"):
    """è·å– STS ä¸´æ—¶å‡­è¯"""
    access_key_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
    access_key_secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
    role_arn = os.getenv('ALIBABA_CLOUD_ROLE_ARN')
    session_name = 'batch-trace-extractor'
    
    config = open_api_models.Config(
        access_key_id=access_key_id,
        access_key_secret=access_key_secret,
        endpoint=f'sts.{region}.aliyuncs.com'
    )
    sts_client = StsClient(config)
    
    try:
        response = sts_client.assume_role(sts_models.AssumeRoleRequest(
            role_arn=role_arn,
            role_session_name=session_name,
            duration_seconds=3600
        ))
        return response.body.credentials
    except TeaException as e:
        print(f"âŒ è·å– STS å‡­è¯å¤±è´¥: {e.message}")
        raise

class AutoRefreshSLSClient:
    """è‡ªåŠ¨åˆ·æ–° Token çš„ SLS å®¢æˆ·ç«¯"""
    def __init__(self, region: str = "cn-qingdao"):
        self.region = region
        self.sls_endpoint = f"{region}.log.aliyuncs.com"
        self.client = None
        self._refresh_client()
    
    def _refresh_client(self):
        creds = get_sts_credentials(self.region)
        self.client = LogClient(
            endpoint=self.sls_endpoint,
            accessKeyId=creds.access_key_id,
            accessKey=creds.access_key_secret,
            securityToken=creds.security_token
        )
    
    def get_logs(self, request):
        try:
            return self.client.get_logs(request)
        except Exception as e:
            if "Unauthorized" in str(e) or "expired" in str(e).lower():
                print(f"âš ï¸ Token è¿‡æœŸï¼Œæ­£åœ¨åˆ·æ–°...")
                self._refresh_client()
                return self.client.get_logs(request)
            raise e

def safe_json_load(text):
    if not text: return {}
    try: return json.loads(text)
    except: return {}

class TraceExtractor:
    def __init__(self):
        self.client = AutoRefreshSLSClient(REGION)

    def _count_total_traces(self, query, start_ts, end_ts):
        """è¾…åŠ©æ–¹æ³•ï¼šç»Ÿè®¡æ€»æ•°"""
        # æ„é€ èšåˆæŸ¥è¯¢
        count_query = f"{query} | select count(distinct traceId) as total"
        try:
            req = GetLogsRequest(
                project=PROJECT_NAME, 
                logstore=LOGSTORE_NAME, 
                query=count_query, 
                fromTime=start_ts, 
                toTime=end_ts
            )
            res = self.client.get_logs(req)
            if res and res.get_logs():
                return int(res.get_logs()[0].get_contents().get('total', 0))
            return 0
        except Exception as e:
            print(f"      âš ï¸ ç»Ÿè®¡æ€»æ•°å¤±è´¥: {e}")
            return -1

    def find_trace_ids(self, query, start_ts, end_ts, limit):
        """é˜¶æ®µä¸€ï¼šæŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„ TraceID"""
        print(f"   ğŸ” æ£€ç´¢ TraceID...")
        print(f"      æŸ¥è¯¢è¯­å¥: {query}")
        
        # 1. å…ˆç»Ÿè®¡æ€»é‡
        total_count = self._count_total_traces(query, start_ts, end_ts)
        print(f"      ğŸ“Š è¯¥æ—¶æ®µç¬¦åˆæ¡ä»¶çš„ Trace æ€»æ•°: {total_count}")
        
        if total_count == 0:
            return []

        # 2. æ‹‰å– ID (å— Limit é™åˆ¶)
        print(f"      ğŸ”„ æ­£åœ¨æå– TraceID (è®¾å®šä¸Šé™: {limit})...")
        trace_ids = set()
        offset = 0
        
        while len(trace_ids) < limit:
            req = GetLogsRequest(
                project=PROJECT_NAME, 
                logstore=LOGSTORE_NAME, 
                query=query, 
                fromTime=start_ts, 
                toTime=end_ts, 
                line=100, 
                offset=offset
            )
            res = self.client.get_logs(req)
            
            if not res or not res.get_logs():
                break
                
            logs = res.get_logs()
            for log in logs:
                tid = log.get_contents().get('traceId')
                if tid:
                    trace_ids.add(tid)
            
            offset += len(logs)
            # å¦‚æœå•æ¬¡è·å–ä¸è¶³ 100 æ¡ï¼Œè¯´æ˜ç¿»é¡µç»“æŸ
            if len(logs) < 100: 
                break
        
        final_ids = list(trace_ids)[:limit]
        print(f"      âœ… å®é™…æå– TraceID: {len(final_ids)} (Coverage: {len(final_ids)}/{total_count})")
        return final_ids

    def fetch_and_save_traces(self, trace_ids, start_ts, end_ts, output_path, fault_info):
        """é˜¶æ®µäºŒï¼šæ‹‰å–å…¨é‡é“¾è·¯å¹¶ä¿å­˜"""
        if not trace_ids:
            print("      âš ï¸ æ—  TraceIDï¼Œè·³è¿‡å¯¼å‡º")
            return

        print(f"   ğŸ“¦ æ‹‰å–å…¨é‡ Span å¹¶ä¿å­˜è‡³: {output_path}")
        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
        
        total_spans = 0
        batch_size = 20 # æ¯æ¬¡å¤„ç† 20 ä¸ª TraceIDï¼Œé˜²æ­¢ Query è¿‡é•¿
        
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=CSV_HEADERS)
            writer.writeheader()
            
            for i in range(0, len(trace_ids), batch_size):
                batch = trace_ids[i : i + batch_size]
                # æ„é€  OR æŸ¥è¯¢åæŸ¥æ‰€æœ‰ Span
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªé€šè¿‡ traceId è¿‡æ»¤ï¼Œä¸åŠ ä»»ä½•å…¶ä»–æ¡ä»¶ï¼Œç¡®ä¿æ‹¿å›å®Œæ•´çš„ Trace
                or_query = " OR ".join([f'traceId: "{tid}"' for tid in batch])
                
                offset = 0
                while True:
                    req = GetLogsRequest(
                        project=PROJECT_NAME, 
                        logstore=LOGSTORE_NAME, 
                        query=or_query, 
                        fromTime=start_ts, 
                        toTime=end_ts, 
                        line=100, 
                        offset=offset
                    )
                    res = self.client.get_logs(req)
                    
                    if not res or not res.get_logs():
                        break
                    
                    logs = res.get_logs()
                    rows = []
                    
                    for log in logs:
                        data = log.get_contents()
                        res_obj = safe_json_load(data.get('resources', '{}'))
                        attr_obj = safe_json_load(data.get('attributes', '{}'))
                        
                        # æ—¶é—´è®¡ç®—
                        try:
                            s_ns = int(data.get('startTime', 0))
                            d_ns = int(data.get('duration', 0))
                            s_ms = s_ns / 1e6
                            d_ms = d_ns / 1e6
                            e_ms = s_ms + d_ms
                        except:
                            s_ms, d_ms, e_ms = 0, 0, 0

                        # å¡«å……è¡Œæ•°æ®
                        row = {
                            'TraceID': data.get('traceId', ''),
                            'SpanId': data.get('spanId', ''),
                            'ParentID': data.get('parentSpanId', ''),
                            'ServiceName': data.get('serviceName', ''),
                            'NodeName': res_obj.get('host.id') or res_obj.get('k8s.node.name', ''),
                            'PodName': res_obj.get('k8s.pod.name', ''),
                            'URL': data.get('spanName', ''),
                            'SpanKind': data.get('kind', ''),
                            'StartTimeMs': f"{s_ms:.3f}",
                            'EndTimeMs': f"{e_ms:.3f}",
                            'DurationMs': f"{d_ms:.3f}",
                            'StatusCode': data.get('statusCode', ''),
                            'HttpStatusCode': str(attr_obj.get('http.status_code') or attr_obj.get('rpc.grpc.status_code', '')),
                            'fault_type': fault_info['fault_type'],       
                            'fault_instance': fault_info['fault_instance'] 
                        }
                        rows.append(row)
                    
                    writer.writerows(rows)
                    count = len(logs)
                    total_spans += count
                    offset += count
                    if count < 100: break
                
                print(f"      è¿›åº¦: {min(i+batch_size, len(trace_ids))}/{len(trace_ids)} Traces...", end='\r')
        
        print(f"\n      âœ… å®Œæˆ. å…±å¯¼å‡º {total_spans} Spans.")

class BatchProcessor:
    def __init__(self, args):
        self.args = args
        self.extractor = TraceExtractor()

    def process_row(self, row):
        pid = row['problem_id']
        p_type = row['instance_type']
        instance = row['instance']
        
        # 1. åªå¤„ç†æœåŠ¡çº§æ•…éšœ
        if p_type != 'service':
            # print(f"â­ï¸  [Problem {pid}] è·³è¿‡ (ç±»å‹: {p_type}, é Service)")
            return

        print(f"\nğŸš€ [Problem {pid}] å¤„ç†ä¸­... (æœåŠ¡: {instance}, æ•…éšœ: {row['fault_type']})")
        
        # 2. è®¡ç®—æ—¶é—´çª—å£ (å¸¦ç¼“å†²)
        try:
            start_dt = datetime.strptime(row['start_time'], "%Y-%m-%d %H:%M:%S")
            end_dt = datetime.strptime(row['end_time'], "%Y-%m-%d %H:%M:%S")
            
            # ç¼“å†²é€»è¾‘ï¼šé¦–å°¾å„å»æ‰ buffer ç§’
            search_start = start_dt + timedelta(seconds=self.args.buffer)
            search_end = end_dt - timedelta(seconds=self.args.buffer)
            
            if search_end <= search_start:
                print("   âš ï¸ ç¼“å†²åæ—¶é—´çª—å£æ— æ•ˆï¼Œä½¿ç”¨åŸå§‹æ—¶é—´")
                search_start, search_end = start_dt, end_dt
            
            print(f"   ğŸ•’ åŸå§‹æ—¶é—´: {row['start_time']} ~ {row['end_time']}")
            print(f"   ğŸ•’ ç¼“å†²æŸ¥è¯¢: {search_start} ~ {search_end} (-{self.args.buffer}s)")
            
            s_ts = int(search_start.timestamp())
            e_ts = int(search_end.timestamp())
            
            # ä¸ºäº†åæŸ¥å®Œæ•´é“¾è·¯ï¼ŒPhase 2 çš„æ‹‰å–éœ€è¦è¦†ç›–ç¨å¤§çš„èŒƒå›´
            fetch_start_ts = int(start_dt.timestamp())
            fetch_end_ts = int(end_dt.timestamp())

        except Exception as e:
            print(f"   âŒ æ—¶é—´è§£æé”™è¯¯: {e}")
            return

        # 3. æ„é€ æŸ¥è¯¢ & æå–
        # é€»è¾‘ï¼šserviceName == æ•…éšœå®ä¾‹å
        query = f'serviceName: "{instance}"'
        
        # é˜¶æ®µä¸€ï¼šæ‰¾ TraceID
        trace_ids = self.extractor.find_trace_ids(query, s_ts, e_ts, self.args.limit)
        
        if not trace_ids:
            print("   âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ Trace")
            return

        # é˜¶æ®µäºŒï¼šä¿å­˜ CSV
        output_file = os.path.join(self.args.output_dir, f"problem_{pid}", "trace_fusion.csv")
        
        fault_info = {
            'fault_type': row['fault_type'],
            'fault_instance': instance
        }
        
        self.extractor.fetch_and_save_traces(trace_ids, fetch_start_ts, fetch_end_ts, output_file, fault_info)

    def run(self):
        if not os.path.exists(self.args.csv):
            print(f"âŒ æ‰¾ä¸åˆ° CSV æ–‡ä»¶: {self.args.csv}")
            return

        with open(self.args.csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            rows = list(reader)
            
            print(f"ğŸ“‹ åŠ è½½ä»»åŠ¡åˆ—è¡¨: {len(rows)} æ¡")
            
            target_rows = []
            
            # æ¨¡å¼ 1: æŒ‡å®šå•ä¸ª ID
            if self.args.problem_id:
                target_rows = [r for r in rows if r['problem_id'] == self.args.problem_id]
                
            # æ¨¡å¼ 2: æŒ‡å®š ID èŒƒå›´ (ä¾‹å¦‚ 002-005)
            elif self.args.range:
                try:
                    start_id, end_id = map(int, self.args.range.split(','))
                    target_rows = [r for r in rows if start_id <= int(r['problem_id']) <= end_id]
                except:
                    print("âŒ èŒƒå›´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨: start_id,end_id (ä¾‹å¦‚: 2,5)")
                    return
            
            # æ¨¡å¼ 3: å…¨éƒ¨
            else:
                print("âš ï¸ æœªæŒ‡å®š --problem-id æˆ– --rangeï¼Œå°†å¤„ç† CSV ä¸­æ‰€æœ‰æœåŠ¡çº§æ•…éšœ...")
                target_rows = rows

            print(f"ğŸ¯ å‘½ä¸­ä»»åŠ¡æ•°: {len(target_rows)}")
            
            for row in target_rows:
                self.process_row(row)
                time.sleep(1) # é¿å…è¯·æ±‚è¿‡å¿«

def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡ Trace æå–å·¥å…·")
    parser.add_argument("--csv", default="dataset/b_gt.csv", help="b_gt.csv è·¯å¾„")
    parser.add_argument("--output-dir", default="output_datasets", help="æ•°æ®ä¿å­˜æ ¹ç›®å½•")
    
    # ç­›é€‰æ¨¡å¼
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--problem-id", help="æŒ‡å®šå•ä¸ª Problem ID (å¦‚: 002)")
    group.add_argument("--range", help="æŒ‡å®š ID èŒƒå›´ (å¦‚: 2,5)")
    
    # å‚æ•°å¾®è°ƒ
    parser.add_argument("--limit", type=int, default=2000, help="æ¯ä¸ªæ•…éšœæå– TraceID ä¸Šé™")
    parser.add_argument("--buffer", type=int, default=60, help="æ—¶é—´çª—å£é¦–å°¾åˆ‡é™¤ç§’æ•° (é»˜è®¤ 60s)")
    
    args = parser.parse_args()
    
    processor = BatchProcessor(args)
    processor.run()

if __name__ == "__main__":
    main()