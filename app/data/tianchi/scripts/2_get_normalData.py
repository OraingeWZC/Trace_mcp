#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ­£å¸¸æ—¶æ®µæ•°æ®è·å–å·¥å…· (Baseline Data Fetcher) - æœ€ç»ˆç‰ˆ
- æ”¯æŒ --window-hours è‡ªå®šä¹‰æ—¶é—´çª—
- æ”¯æŒ --file-name è‡ªå®šä¹‰æ–‡ä»¶ååç¼€ (é˜²æ­¢è¦†ç›–)
- åŒ…å«æ‚¬æµ®èŠ‚ç‚¹/æ–­é“¾ä¸¥æ ¼æ£€æŸ¥
"""

import os
import sys
import json
import csv
import time
import argparse
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

import config

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
# 1. æŒ‡æ ‡å®šä¹‰
TARGET_METRICS = [
    "aggregate_node_net_receive_packages_errors_per_minute",
    "aggregate_node_tcp_inuse_total_num",
    "aggregate_node_tcp_alloc_total_num",
    "aggregate_node_cpu_usage",
    "aggregate_node_memory_usage",
    "aggregate_node_disk_io_usage"
]

# 2. SLS é…ç½®
PROJECT_NAME = config.SLS_PROJECT_NAME
LOGSTORE_NAME = config.SLS_LOGSTORE_NAME
REGION = config.SLS_REGION

# 3. é‰´æƒé…ç½®
os.environ.setdefault("ALIBABA_CLOUD_ROLE_SESSION_NAME", "normal-data-fetcher")

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# å¯¼å…¥å·¥å…·åº“
try:
    from tools.paas_entity_tools import umodel_get_entities
    from tools.paas_data_tools import umodel_get_golden_metrics
    from tools.common import create_cms_client, execute_cms_query
    from aliyun.log import LogClient, GetLogsRequest
    from alibabacloud_sts20150401.client import Client as StsClient
    from alibabacloud_sts20150401 import models as sts_models
    from alibabacloud_tea_openapi import models as open_api_models
    from tools.constants import REGION_ID, WORKSPACE_NAME
except ImportError as e:
    print(f"âŒ ä¾èµ–ç¼ºå¤±: {e}")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === æ‚¬æµ®èŠ‚ç‚¹æ£€æŸ¥ ===
def check_orphan_root(spans: list) -> bool:
    """
    æ£€æŸ¥ Trace æ˜¯å¦å­˜åœ¨æ–­é“¾ä¸å¤šæ ¹
    æ‰¾å‡ºæ‰€æœ‰â€œæ‹“æ‰‘æ ¹â€ï¼šå³ ParentID ä¸æŒ‡å‘å½“å‰ Trace ä¸­ä»»ä½•å·²çŸ¥ Span çš„èŠ‚ç‚¹ã€‚(åŒ…å«ä¸‰ç§æƒ…å†µï¼šParentIDä¸ºç©ºã€ParentIDä¸º-1ã€ParentIDæŒ‡å‘ä¸å­˜åœ¨çš„ID)
       """
    if not spans: return False
    
    # 1. å»ºç«‹å½“å‰ Trace æ‰€æœ‰ SpanID çš„é›†åˆ (ç™½åå•)
    span_ids = set()
    for s in spans:
        # å…¼å®¹ä¸åŒå­—æ®µåï¼Œç¡®ä¿è½¬ä¸ºå­—ç¬¦ä¸²å¹¶å»ç©º
        sid = str(s.get('SpanId', '')).strip()
        if sid: 
            span_ids.add(sid)
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆ Span IDï¼Œç›´æ¥è§†ä¸ºæ— æ•ˆ
    if not span_ids: return False

    root_count = 0
    
    # 2. éå†æ‰€æœ‰ Spanï¼Œç»Ÿè®¡â€œæ‹“æ‰‘æ ¹â€çš„æ•°é‡
    for s in spans:
        pid = str(s.get('ParentID', '')).strip()
        
        # æ ¸å¿ƒåˆ¤å®šï¼šåªè¦ ParentID ä¸åœ¨ span_ids é‡Œï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªâ€œæ ¹â€
        # (è¿™è‡ªåŠ¨æ¶µç›–äº†: pidä¸º -1, pidä¸º nan, pidä¸º null, ä»¥åŠ pid æŒ‡å‘ç¼ºå¤±èŠ‚ç‚¹çš„æƒ…å†µ)
        if pid not in span_ids:
            root_count += 1
            
    # 3. ä¸¥æ ¼é™åˆ¶ï¼šåªèƒ½æœ‰ 1 ä¸ªæ ¹
    return root_count == 1

class NormalDataFetcher:
    def __init__(self, args):
        self.args = args
        self.output_dir = args.output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.cms_client = create_cms_client(REGION_ID)
        self.sls_client = self._init_sls_client()

    def _init_sls_client(self):
        """åˆå§‹åŒ– SLS å®¢æˆ·ç«¯ (å¸¦ STS)"""
        config = open_api_models.Config(
            access_key_id=os.environ["ALIBABA_CLOUD_ACCESS_KEY_ID"],
            access_key_secret=os.environ["ALIBABA_CLOUD_ACCESS_KEY_SECRET"],
            endpoint=f'sts.{REGION}.aliyuncs.com'
        )
        sts_client = StsClient(config)
        resp = sts_client.assume_role(sts_models.AssumeRoleRequest(
            role_arn=os.environ["ALIBABA_CLOUD_ROLE_ARN"],
            role_session_name="normal-fetcher",
            duration_seconds=3600
        ))
        creds = resp.body.credentials
        return LogClient(
            endpoint=f"{REGION}.log.aliyuncs.com",
            accessKeyId=creds.access_key_id,
            accessKey=creds.access_key_secret,
            securityToken=creds.security_token
        )

    def determine_time_window(self):
        """æ­¥éª¤ 1: ç¡®å®šæ­£å¸¸æ—¶é—´æ®µ"""
        logger.info(f"ğŸ“… æ­£åœ¨æ‰«æ {self.args.csv} è®¡ç®—åŸºå‡†æ—¶é—´...")
        min_ts = float('inf')
        
        if os.path.exists(self.args.csv):
            with open(self.args.csv, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    try:
                        ts = int(datetime.strptime(row['start_time'], '%Y-%m-%d %H:%M:%S').timestamp())
                        if ts < min_ts: min_ts = ts
                    except: continue
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {self.args.csv}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºåŸºå‡†")
            min_ts = int(time.time())
        
        # å®šä¹‰ï¼šæœ€æ—©æ•…éšœå‰ window_secondså°æ—¶ ~ å‰ 1å°æ—¶
        end_time = min_ts - 60 * 60
        window_seconds = int(self.args.window_hours * 3600)
        start_time = end_time - window_seconds
        
        logger.info(f"âœ… é€‰å®šæ­£å¸¸æ—¶æ®µ: {datetime.fromtimestamp(start_time)} ~ {datetime.fromtimestamp(end_time)}")
        logger.info(f"   (çª—å£: {self.args.window_hours}h, åŸºå‡†æ•…éšœå‰ç¼“å†²: 1h)")
        return start_time, end_time

    def fetch_metrics(self, start_ts, end_ts):
        """
        è·å–èŠ‚ç‚¹æŒ‡æ ‡ï¼š
        1. ä½¿ç”¨åˆ†ç‰‡æŸ¥è¯¢ (Chunking) é˜²æ­¢ API è‡ªåŠ¨é™é‡‡æ · (è§£å†³ 60s ç²’åº¦é—®é¢˜)
        2. ç»Ÿä¸€æ—¶é—´æˆ³å•ä½ä¸ºçº³ç§’ (é˜²æ­¢ç´¢å¼•æŠ¥é”™)
        3. ç»“åˆ Golden Metrics å’Œ CMS åŸå§‹æ¥å£ (è¡¥å…¨ç¼ºå¤±æŒ‡æ ‡)
        4. ä½¿ç”¨ ffill+fillna ç­–ç•¥ (è§£å†³ç©ºæ´/æ–­å±‚é—®é¢˜)
        """
        logger.info(f"ğŸš€ [Metric] å¼€å§‹è·å–æ­£å¸¸æ—¶æ®µçš„èŠ‚ç‚¹æŒ‡æ ‡ ({start_ts} -> {end_ts})...")
        
        # 1. è·å–æ´»è·ƒèŠ‚ç‚¹åˆ—è¡¨
        entity_query = {
            "domain": "acs",
            "entity_set_name": "acs.ecs.instance",
            "from_time": start_ts,
            "to_time": end_ts,
            "limit": 200
        }
        nodes_res = umodel_get_entities.invoke(entity_query)
        if not nodes_res or not nodes_res.data:
            logger.warning("   âš ï¸ æœªå‘ç°æ´»è·ƒèŠ‚ç‚¹")
            return

        nodes = nodes_res.data
        logger.info(f"   å‘ç° {len(nodes)} ä¸ªæ´»è·ƒèŠ‚ç‚¹ï¼Œæ­£åœ¨åˆ†ç‰‡æ‹‰å–é«˜ç²¾åº¦æŒ‡æ ‡...")
        
        # å‡†å¤‡ CSV è¾“å‡º
        filename = f"normal_metrics_{self.args.file_name}.csv"
        csv_path = os.path.join(self.output_dir, filename)
        headers = ['problem_id', 'fault_type', 'instance_id', 'timestamp'] + sorted(TARGET_METRICS)
        
        rows_to_write = []
        
        # [å…³é”®è®¾ç½®] åˆ†ç‰‡å¤§å°è®¾ä¸º 30åˆ†é’Ÿ (1800s)
        # æ—¶é—´è·¨åº¦çŸ­æ—¶ï¼ŒAPI ä¼šè¿”å›åŸå§‹é«˜ç²¾åº¦æ•°æ® (å¦‚ 10s/15s)ï¼›è·¨åº¦é•¿æ—¶ä¼šè‡ªåŠ¨èšåˆä¸º 60s
        CHUNK_SIZE = 1800

        for i, node in enumerate(nodes):
            instance_id = node.get('instance_id')
            entity_id = node.get('__entity_id__')
            if not entity_id: continue

            # node_data: { timestamp_ns: { metric_name: value } }
            node_data = {} 

            # === [æ ¸å¿ƒé€»è¾‘] åˆ†ç‰‡å¾ªç¯æŸ¥è¯¢ ===
            current_chunk_start = start_ts
            while current_chunk_start < end_ts:
                current_chunk_end = min(current_chunk_start + CHUNK_SIZE, end_ts)
                
                # è®°å½•æœ¬è½®åˆ†ç‰‡ä¸­æ‰¾åˆ°çš„æŒ‡æ ‡ï¼Œç”¨äºå†³å®šæ˜¯å¦éœ€è¦ CMS è¡¥ç¼º
                chunk_found_metrics = set()

                # --- ç­–ç•¥ A: Golden Metrics (é¦–é€‰) ---
                try:
                    gm_res = umodel_get_golden_metrics.invoke({
                        "domain": "acs",
                        "entity_set_name": "acs.ecs.instance",
                        "entity_ids": [entity_id],
                        "from_time": current_chunk_start,
                        "to_time": current_chunk_end
                    })
                    
                    if gm_res and gm_res.data:
                        for item in gm_res.data:
                            m_name = item.get('metric')
                            if m_name in TARGET_METRICS:
                                chunk_found_metrics.add(m_name)
                                import ast
                                vals = ast.literal_eval(item.get('__value__', '[]'))
                                tss = ast.literal_eval(item.get('__ts__', '[]'))
                                for v, t in zip(vals, tss):
                                    # [ä¿®å¤] å¼ºåˆ¶è½¬æ¢ä¸ºçº³ç§’ (19ä½)ï¼Œé˜²æ­¢ä¸ CMS æ¯«ç§’æ··ç”¨å¯¼è‡´ Pandas å´©æºƒ
                                    t_int = int(t)
                                    t_ns = t_int * 1000000 if t_int < 1e14 else t_int
                                    
                                    if t_ns not in node_data: node_data[t_ns] = {}
                                    node_data[t_ns][m_name] = v
                except Exception as e:
                    # logger.warning(f"GM Error: {e}")
                    pass

                # --- ç­–ç•¥ B: CMS åŸå§‹æ¥å£è¡¥ç¼º (å¤‡é€‰) ---
                # å¦‚æœ Golden Metrics æ²¡æ‹¿åˆ°æŸäº›æŒ‡æ ‡ï¼Œå°è¯•å»æŸ¥åº•å±‚æ¥å£
                missing = [m for m in TARGET_METRICS if m not in chunk_found_metrics]
                if missing:
                    for m in missing:
                        query = f".entity_set with(domain='acs', name='acs.ecs.instance', ids=['{entity_id}']) | entity-call get_metric('{m}')"
                        try:
                            # æ³¨æ„ï¼šCMS æŸ¥è¯¢æ¯”è¾ƒæ…¢ï¼Œè¿™é‡ŒåªæŸ¥ç¼ºå¤±çš„éƒ¨åˆ†
                            res = execute_cms_query(self.cms_client, WORKSPACE_NAME, query, current_chunk_start, current_chunk_end)
                            if res and res.data:
                                for r in res.data:
                                    v = r.get('value') or r.get(m)
                                    t = r.get('timestamp') or r.get('ts')
                                    if v is not None and t is not None:
                                        t_int = int(t)
                                        t_ns = t_int * 1000000 if t_int < 1e14 else t_int
                                        
                                        if t_ns not in node_data: node_data[t_ns] = {}
                                        node_data[t_ns][m] = v
                        except: 
                            pass
                
                # æ¨è¿›åˆ°ä¸‹ä¸€ä¸ªåˆ†ç‰‡
                current_chunk_start = current_chunk_end
            
            # === ç»Ÿä¸€é‡é‡‡æ ·ä¸å¡«å…… ===
            if self.args.interval and self.args.interval > 0 and node_data:
                try:
                    df = pd.DataFrame.from_dict(node_data, orient='index')
                    df.index = pd.to_datetime(df.index, unit='ns')
                    
                    # 1. æ•°æ®æ¸…æ´—ï¼šå¼ºåˆ¶è½¬ä¸ºæ•°å­—ï¼Œå¤„ç†ç©ºå­—ç¬¦ä¸²
                    for col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    # 2. é‡é‡‡æ · + å¡«å……ç­–ç•¥
                    df_resampled = df.resample(f'{self.args.interval}s').mean().ffill().fillna(0.0)
                    
                    # 3. å›å¡«
                    new_timestamps = df_resampled.index.astype(np.int64).tolist()
                    for idx, ts_val in enumerate(new_timestamps):
                        row_vals = df_resampled.iloc[idx].to_dict()
                        
                        # æ„é€  CSV è¡Œ
                        row = {
                            'problem_id': 'normal_000',
                            'fault_type': 'normal',
                            'instance_id': instance_id,
                            'timestamp': ts_val
                        }
                        # å¡«å…¥æŒ‡æ ‡å€¼ï¼Œç¼ºå¤±çš„è¡¥ç©ºå­—ç¬¦ä¸²(æˆ–0)
                        for m in TARGET_METRICS:
                            row[m] = row_vals.get(m, 0.0)
                        
                        rows_to_write.append(row)
                        
                except Exception as e:
                    logger.error(f"   [Node {instance_id}] é‡é‡‡æ ·/å¤„ç†å¤±è´¥: {e}")
                    # å‡ºé”™æ—¶é™çº§æ–¹æ¡ˆï¼šå†™å…¥åŸå§‹æ•°æ®ï¼ˆé˜²æ­¢æ•°æ®å…¨ä¸¢ï¼‰
                    for ts, metrics in node_data.items():
                        row = {
                            'problem_id': 'normal_000',
                            'fault_type': 'normal',
                            'instance_id': instance_id,
                            'timestamp': ts
                        }
                        for m in TARGET_METRICS:
                            row[m] = metrics.get(m, 0.0)
                        rows_to_write.append(row)

            if (i+1) % 5 == 0: print(f"   å·²å¤„ç† {i+1}/{len(nodes)} ä¸ªèŠ‚ç‚¹...", end='\r')

        # ç»Ÿä¸€å†™å…¥æ–‡ä»¶
        try:
            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=headers)
                writer.writeheader()
                writer.writerows(rows_to_write)
            logger.info(f"\nâœ… [Metric] å·²ä¿å­˜ {len(rows_to_write)} æ¡æŒ‡æ ‡æ•°æ®è‡³ {csv_path}")
        except Exception as e:
            logger.error(f"âŒ å†™å…¥ CSV å¤±è´¥: {e}")

    def fetch_traces(self, start_ts, end_ts):
        """æ­¥éª¤ 3: è·å– Trace æ•°æ® (å«ä¸¥æ ¼è¿‡æ»¤)"""
        logger.info("ğŸš€ [Trace] å¼€å§‹è·å–æ­£å¸¸æ—¶æ®µçš„ Trace...")
        
        # 1. åˆç­›: è·å–åŒ…å«è‡³å°‘ä¸€ä¸ªæ­£å¸¸Spançš„å€™é€‰TraceID
        # (è¿™é‡Œè¿˜æ˜¯ç”¨å®½æ³›æŸ¥è¯¢ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šåœ¨æœ¬åœ°åšäºŒæ¬¡ä¸¥æ ¼æ£€æŸ¥)
        query = "* | where try_cast(statusCode as bigint) <= 1"
        limit = self.args.trace_limit
        
        candidate_trace_ids = set()
        offset = 0
        target_candidates = int(limit * 2.0) 
        
        logger.info(f"   ç›®æ ‡: è·å– {limit} æ¡çº¯å‡€ Traceï¼Œé¢„è®¡éœ€æ‰«æ {target_candidates} ä¸ªå€™é€‰ ID...")
        
        while len(candidate_trace_ids) < target_candidates:
            req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=query, fromTime=start_ts, toTime=end_ts, line=100, offset=offset)
            try:
                res = self.sls_client.get_logs(req)
                if not res or not res.get_logs(): break
                logs = res.get_logs()
                for log in logs:
                    candidate_trace_ids.add(log.get_contents().get('traceId'))
                offset += len(logs)
                print(f"   å·²æ‰«æ {offset} æ¡æ—¥å¿—ï¼Œå‘ç° {len(candidate_trace_ids)} ä¸ªå€™é€‰ TraceID...", end='\r')
                if len(logs) < 100: break
            except Exception as e:
                logger.error(f"SLS Query Error: {e}")
                break
        
        logger.info(f"\n   æ‰«æç»“æŸã€‚å¼€å§‹æ‹‰å–å¹¶ä¸¥æ ¼æ¸…æ´— {len(candidate_trace_ids)} ä¸ª Trace...")
        
        # [ä¿®æ”¹] ä½¿ç”¨åç¼€æ„é€ æ–‡ä»¶å
        filename = f"normal_traces{self.args.file_name}.csv"
        csv_path = os.path.join(self.output_dir, filename)
        
        csv_headers = [
            'TraceID', 'SpanId', 'ParentID', 'ServiceName', 'NodeName', 'PodName', 
            'URL', 'SpanKind', 'StartTimeMs', 'EndTimeMs', 'DurationMs',
            'StatusCode', 'HttpStatusCode', 'fault_type', 'fault_instance', 'problem_id'
        ]
        
        batch_list = list(candidate_trace_ids)
        valid_trace_count = 0
        total_spans = 0
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=csv_headers)
            writer.writeheader()
            
            # æ¯æ¬¡å¤„ç† 20 ä¸ª TraceID
            for i in range(0, len(batch_list), 20):
                if valid_trace_count >= limit: break
                
                batch = batch_list[i:i+20]
                or_query = " OR ".join([f'traceId: "{tid}"' for tid in batch])
                
                # === å†…å­˜èšåˆ: å°†è¿™20ä¸ªTraceçš„æ‰€æœ‰Spanå…ˆå­˜èµ·æ¥ ===
                trace_buffer = {tid: [] for tid in batch} 
                
                sub_offset = 0
                while True:
                    req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=or_query, fromTime=start_ts, toTime=end_ts, line=100, offset=sub_offset)
                    try:
                        res = self.sls_client.get_logs(req)
                        if not res or not res.get_logs(): break
                        logs = res.get_logs()
                        
                        for log in logs:
                            d = log.get_contents()
                            tid = d.get('traceId')
                            if tid in trace_buffer:
                                # è§£æ Span æ•°æ®
                                try: res_obj = json.loads(d.get('resources', '{}'))
                                except: res_obj = {}
                                try: att_obj = json.loads(d.get('resources', '{}'))
                                except: att_obj = {}
                                try:
                                    s_ms = int(d.get('startTime', 0)) / 1e6
                                    d_ms = int(d.get('duration', 0)) / 1e6
                                except: s_ms, d_ms = 0, 0
                                
                                # æš‚å­˜åŸå§‹æ•°æ®å¯¹è±¡
                                span_obj = {
                                    'TraceID': tid,
                                    'SpanId': d.get('spanId'),
                                    'ParentID': d.get('parentSpanId'),
                                    'ServiceName': d.get('serviceName'),
                                    'NodeName': res_obj.get('k8s.node.name'),
                                    'PodName': res_obj.get('k8s.pod.name'),
                                    'URL': d.get('spanName'),
                                    'SpanKind': d.get('kind'),
                                    'StartTimeMs': f"{s_ms:.3f}",
                                    'EndTimeMs': f"{s_ms + d_ms:.3f}",
                                    'DurationMs': f"{d_ms:.3f}",
                                    'StatusCode': d.get('statusCode'),
                                    'HttpStatusCode': str(att_obj.get('http.status_code') or att_obj.get('rpc.grpc.status_code', '')),
                                    'fault_type': 'normal',
                                    'fault_instance': "",
                                    'problem_id': 'normal_000'
                                }
                                trace_buffer[tid].append(span_obj)
                        
                        sub_offset += len(logs)
                        if len(logs) < 100: break
                    except: break
                
                # === ä¸¥æ ¼è¿‡æ»¤: æ£€æŸ¥æ¯ä¸ªTraceæ˜¯å¦çœŸæ­£â€œçº¯å‡€â€ ===
                rows_to_save = []
                for tid, spans in trace_buffer.items():
                    if not spans: continue
                    if len(spans) < 2: continue

                    is_error = False
                    is_out_of_window = False # [æ–°å¢] æ ‡è®°æ˜¯å¦è¶…å‡ºæ—¶é—´çª—

                    for span in spans:
                        # 1. æ£€æŸ¥é”™è¯¯çŠ¶æ€
                        try:
                            # å…¼å®¹å¤„ç†ï¼šæœ‰äº›statusCodeå¯èƒ½æ˜¯ç©ºæˆ–éæ•°å­—ï¼Œè§†ä½œ0
                            sc = int(span['StatusCode']) if span['StatusCode'] and span['StatusCode'].isdigit() else 0
                            if sc > 1: is_error = True; break
                        except: pass
                        
                        # 2. æ£€æŸ¥å¼€å§‹æ—¶é—´æ˜¯å¦æ—©äºçª—å£èµ·å§‹æ—¶é—´
                        # start_ts æ˜¯ç§’çº§ï¼ŒStartTimeMs æ˜¯å­—ç¬¦ä¸²æ¯«ç§’ï¼Œéœ€è½¬æ¢
                        try:
                            span_start_ms = float(span['StartTimeMs'])
                            if span_start_ms < start_ts * 1000:
                                is_out_of_window = True
                                break # åªè¦æœ‰ä¸€ä¸ª Span æ—©äºçª—å£ï¼Œæ•´æ¡ Trace ä¸¢å¼ƒ
                        except: pass

                    if is_error: continue
                    if is_out_of_window: continue # [æ–°å¢] ä¸¢å¼ƒè·¨çª—å£çš„ Trace

                    # 3. ä¸¥æ ¼æ‚¬æµ®æ£€æŸ¥
                    if not check_orphan_root(spans): continue

                    rows_to_save.extend(spans)
                    valid_trace_count += 1
                
                # å†™å…¥æ–‡ä»¶
                if rows_to_save:
                    writer.writerows(rows_to_save)
                    total_spans += len(rows_to_save)
                
                print(f"   è¿›åº¦: å·²è·å– {valid_trace_count}/{limit} æ¡çº¯å‡€ Trace...", end='\r')

        logger.info(f"\nâœ… [Trace] å·²ä¿å­˜ {valid_trace_count} æ¡çº¯å‡€ Trace ({total_spans} Spans) è‡³ {csv_path}")

    def run(self):
        s_ts, e_ts = self.determine_time_window()
        # è·å–æŒ‡æ ‡æ—¶ï¼Œé¢å¤–å¤šå¾€å‰æ‹‰ 3 åˆ†é’Ÿ
        self.fetch_metrics(s_ts - 180, e_ts)
        self.fetch_traces(s_ts, e_ts)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", default="dataset/b_gt.csv", help="æ•…éšœåˆ—è¡¨è·¯å¾„")
    parser.add_argument("--output-dir", default="data/NormalData", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--trace-limit", type=int, default=400000, help="è·å–å¤šå°‘æ¡æ­£å¸¸ Trace")
    parser.add_argument("--interval", type=int, default=30, help="æŒ‡æ ‡é‡é‡‡æ ·é—´éš”(ç§’)")
    
    # [æ–°å¢] å‚æ•°
    parser.add_argument("--window-hours", type=float, default=4.0, help="è·å–æ•…éšœå‰å¤šå°‘å°æ—¶çš„æ•°æ®")
    parser.add_argument("--file-name", type=str, default="4e5_30s_4h_new", help="è¾“å‡ºæ–‡ä»¶ååç¼€ (ä¾‹å¦‚ '_v1')")
    
    args = parser.parse_args()

    fetcher = NormalDataFetcher(args)
    fetcher.run()