import os
import argparse
import shutil
import sqlite3
import pandas as pd
import numpy as np
import torch
import pickle  # [æ–°å¢] ç”¨äºä¿å­˜ç´¢å¼•æ–‡ä»¶
from tqdm import tqdm

# å¼•å…¥é¡¹ç›®ä¾èµ–
from tracegnn.data.trace_graph import df_to_trace_graphs, TraceGraphIDManager
from tracegnn.data.trace_graph_db import TraceGraphDB, BytesSqliteDB
from tracegnn.utils.host_state import host_state_vector

# ================= é…ç½®åŒºåŸŸ =================
DEFAULT_DATASET_ROOT = 'dataset/tianchi/2e5_1622' 

# [ä¿®æ”¹] ç°åœ¨ç»Ÿä¸€ä½¿ç”¨åˆå¹¶åçš„æ–‡ä»¶å
INFRA_FILENAME = 'merged_all_infra.csv'

# Host Sequence é…ç½® (éœ€ä¸ config.py ä¿æŒä¸€è‡´)
SEQ_WINDOW = 15
SEQ_METRICS = ['cpu', 'mem', 'disk', 'net', 'tcp'] 

# å¤©æ± æ•°æ®çš„çœŸå®æŒ‡æ ‡åˆ—å (ç”¨äºä» CSV ä¸­æå–æ•°æ®)
# è„šæœ¬ä¼šå» CSV é‡Œæ‰¾è¿™äº›åˆ—ï¼Œå¦‚æœä½ çš„åˆå¹¶è„šæœ¬æ”¹åäº†ï¼Œè¿™é‡Œä¹Ÿè¦å¯¹åº”ä¿®æ”¹
TIANCHI_METRICS = [
    "aggregate_node_cpu_usage",
    "aggregate_node_memory_usage",
    "aggregate_node_disk_io_usage",
    "aggregate_node_net_receive_packages_errors_per_minute",
    "aggregate_node_tcp_alloc_total_num",
    "aggregate_node_tcp_inuse_total_num"
]
# ===========================================

def flexible_load_trace_csv(input_path: str) -> pd.DataFrame:
    if not os.path.exists(input_path):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return pd.DataFrame()
    try:
        df = pd.read_csv(input_path)
        if 'Duration' in df.columns: df['Duration'] = pd.to_numeric(df['Duration'], errors='coerce')
        if 'StartTimeMs' in df.columns: df['StartTime'] = pd.to_numeric(df['StartTimeMs'], errors='coerce')
        if 'Anomaly' in df.columns: df['Anomaly'] = df['Anomaly'].astype(bool)
        return df
    except Exception as e:
        print(f"åŠ è½½CSVå‡ºé”™ {input_path}: {e}")
        return pd.DataFrame()

def load_infra_data(dataset_root: str, filename: str):
    """åŠ è½½æŒ‡æ ‡æ•°æ®ï¼Œæ”¯æŒä» dataset_root æˆ–å…¶çˆ¶ç›®å½•æŸ¥æ‰¾"""
    
    # å®šä¹‰æŸ¥æ‰¾è·¯å¾„ä¼˜å…ˆçº§
    paths_to_try = [
        os.path.join(dataset_root, 'processed', filename), # ä¼˜å…ˆæ‰¾ processed
        os.path.join(dataset_root, filename),              # å…¶æ¬¡æ‰¾ root
        os.path.join(os.path.dirname(dataset_root.rstrip('/')), filename), # æ‰¾çˆ¶ç›®å½• dataset/tianchi
    ]

    infra_path = None
    for p in paths_to_try:
        if os.path.exists(p):
            infra_path = p
            break

    if not infra_path:
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°æŒ‡æ ‡æ•°æ®æ–‡ä»¶: {filename}")
        print(f"   è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œäº†åˆå¹¶è„šæœ¬ï¼Œå¹¶å°†æ–‡ä»¶æ”¾åœ¨ {dataset_root} æˆ–å…¶çˆ¶ç›®å½•ä¸‹")
        return None
    
    print(f"âœ… å·²åŠ è½½æŒ‡æ ‡æ•°æ®: {infra_path}")
    try:
        df = pd.read_csv(infra_path)
        
        # 1. æ£€æŸ¥å…³é”®åˆ—
        if 'timeMs' not in df.columns:
            if 'timestamp' in df.columns:
                df['timeMs'] = df['timestamp'].astype(np.int64) // 1000000
            else:
                print("âŒ é”™è¯¯: CSVç¼ºå°‘ 'timeMs' æˆ– 'timestamp' åˆ—")
                return None
                
        if 'kubernetes_node' not in df.columns:
            if 'instance_id' in df.columns:
                df['kubernetes_node'] = df['instance_id'].astype(str)
            else:
                print("âŒ é”™è¯¯: CSVç¼ºå°‘ 'kubernetes_node' æˆ– 'instance_id' åˆ—")
                return None

        # 2. è¿‡æ»¤éœ€è¦çš„æŒ‡æ ‡åˆ—
        # å…¼å®¹é€»è¾‘ï¼šå¦‚æœ CSV é‡Œå·²ç»æ˜¯æ ‡å‡†å(node_cpu...)å°±ç”¨æ ‡å‡†åï¼Œå¦åˆ™ç”¨å¤©æ± å
        valid_cols = []
        for m in TIANCHI_METRICS:
            if m in df.columns:
                valid_cols.append(m)
                df[m] = pd.to_numeric(df[m], errors='coerce').fillna(0.0)
            # è¿™é‡Œå¯ä»¥åŠ ä¸ª else æ£€æŸ¥æ ‡å‡†åï¼Œè§†ä½ åˆå¹¶è„šæœ¬çš„é€»è¾‘è€Œå®š
        
        if not valid_cols:
            print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„æŒ‡æ ‡åˆ—ï¼Œè¯·æ£€æŸ¥ CSV è¡¨å¤´")
            return None

        # 3. æ„å»ºç´¢å¼• (æŒ‰èŠ‚ç‚¹åˆ†ç»„)
        cols = ['timeMs', 'kubernetes_node'] + valid_cols
        df = df[cols].dropna(subset=['timeMs', 'kubernetes_node'])
        
        host_idx = {}
        for host, g in tqdm(df.groupby('kubernetes_node'), desc="æ„å»ºå†…å­˜ç´¢å¼•"):
            lg = g.sort_values('timeMs')
            lg = lg.drop_duplicates(subset=['timeMs'], keep='last')
            
            host_idx[str(host)] = {
                'timeMs': lg['timeMs'].to_numpy(dtype=np.int64),
                'metrics': {m: lg[m].to_numpy(dtype=np.float64) for m in valid_cols}
            }
        return host_idx
    except Exception as e:
        print(f"è§£ææŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
        return None

# ... (precompute_host_states å’Œ precompute_host_sequences å‡½æ•°é€»è¾‘æ— éœ€ä¿®æ”¹ï¼Œä¿æŒåŸæ ·å³å¯) ...
# ä¸ºäº†å®Œæ•´æ€§ï¼Œè¿™é‡Œç®€å†™ä¿ç•™ç»“æ„ï¼Œå®é™…è¿è¡Œæ—¶è¯·ç¡®ä¿è¿™ä¸¤ä¸ªå‡½æ•°åœ¨ä»£ç ä¸­
def precompute_host_states(trace_graphs, infra_index, id_manager, W=3):
    if infra_index is None: return
    metrics = TIANCHI_METRICS # ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„åˆ—è¡¨
    per_metric_dims = 4
    feature_dim = len(metrics) * per_metric_dims
    zero_vec = np.zeros(feature_dim, dtype=np.float32)

    for graph in tqdm(trace_graphs, desc="é¢„è®¡ç®— HostState (GNN)"):
        try:
            st = graph.root.spans[0].start_time if (graph.root and graph.root.spans) else None
            if isinstance(st, (int, float)):
                v = float(st)
                t0_ms = int(v if v > 1e12 else v * 1000.0)
            else:
                t0_ms = 0
            t0_min_ms = (t0_ms // 60000) * 60000
            
            nodes_in_graph = [node for _, node in graph.iter_bfs() if node.host_id and node.host_id > 0]
            host_ids = set(node.host_id for node in nodes_in_graph)
            host_state_map = {}
            
            for hid in host_ids:
                hname = id_manager.host_id.rev(int(hid))
                if not hname or str(hname).lower() == 'nan':
                    host_state_map[hid] = zero_vec.copy()
                    continue

                vec = host_state_vector(hname, infra_index, t0_min_ms, metrics=metrics, W=W, per_metric_dims=per_metric_dims)
                if vec is not None:
                    host_state_map[hid] = vec
            
            if host_state_map:
                graph.data['precomputed_host_state'] = host_state_map
        except Exception:
            continue

def precompute_host_sequences(trace_graphs, infra_index, id_manager):
    if infra_index is None: return
    # ç®€å•çš„åˆ—åæ˜ å°„ï¼Œå¦‚æœ CSV åˆ—åå·²ç»æ˜¯ aggregate_...ï¼Œè¿™é‡Œæ˜ å°„éœ€è¦æ³¨æ„
    # å¦‚æœä½ çš„ CSV åˆ—åæ˜¯ aggregate_...ï¼Œä¸‹é¢è¿™ä¸ªæ˜ å°„è¦ç¡®ä¿èƒ½æ‰¾åˆ°
    def _map_metric(alias: str) -> str:
        alias = str(alias).lower().strip()
        mapping = {
            'cpu': 'aggregate_node_cpu_usage',
            'mem': 'aggregate_node_memory_usage',
            'disk': 'aggregate_node_disk_io_usage',
            'net': 'aggregate_node_net_receive_packages_errors_per_minute',
            'tcp': 'aggregate_node_tcp_inuse_total_num'
        }
        # å¦‚æœ alias åœ¨ mapping é‡Œï¼Œè¿”å›å¯¹åº”çš„ aggregate åï¼›å¦åˆ™è¿”å› alias æœ¬èº«ï¼ˆé˜²æ­¢ alias å·²ç»æ˜¯çœŸå®åï¼‰
        return mapping.get(alias, alias)
    
    metrics_cols = [_map_metric(a) for a in SEQ_METRICS]
    W = SEQ_WINDOW
    
    # ... (Robust norm logic) ...
    def _robust_norm(x):
        med = np.nanmedian(x)
        iqr = np.nanpercentile(x, 75) - np.nanpercentile(x, 25)
        denom = iqr if iqr > 1e-6 else (np.nanstd(x) if np.nanstd(x) > 1e-6 else 1.0)
        return np.nan_to_num((x - med) / denom, nan=0.0)

    for graph in tqdm(trace_graphs, desc="é¢„è®¡ç®— HostSeq (OmniAnomaly)"):
        try:
            st = graph.root.spans[0].start_time if (graph.root and graph.root.spans) else None
            if isinstance(st, (int, float)):
                v = float(st)
                t0_ms = int(v if v > 1e12 else v * 1000.0)
            else:
                t0_ms = 0
            t0_min = (t0_ms // 60000) * 60000

            host_ids = set(node.host_id for _, node in graph.iter_bfs() if node.host_id and node.host_id > 0)
            host_seq_map = {}

            for hid in host_ids:
                hname = id_manager.host_id.rev(int(hid))
                if not hname: continue
                rec = infra_index.get(str(hname))
                if not rec: continue
                t_arr = rec.get('timeMs', [])
                if len(t_arr) == 0: continue
                
                per_metric = []
                for mcol in metrics_cols:
                    vals = rec.get('metrics', {}).get(mcol, [])
                    if len(vals) == 0:
                        seq_vals_np = np.zeros(W, dtype=np.float64)
                    else:
                        seq_vals = []
                        for k in range(W):
                            target = t0_min - (W - 1 - k) * 60000
                            pos = int(np.searchsorted(t_arr, target, side='right')) - 1
                            seq_vals.append(float(vals[pos]) if pos >= 0 else np.nan)
                        seq_vals_np = np.array(seq_vals, dtype=np.float64)
                    per_metric.append(_robust_norm(seq_vals_np).astype(np.float32))
                
                if per_metric:
                    host_seq_map[int(hid)] = torch.from_numpy(np.stack(per_metric, axis=1))
            
            if host_seq_map:
                graph.data['precomputed_host_seq'] = host_seq_map
        except Exception:
            continue

def process_split(split_name, dataset_root, id_manager, infra_index, processed_df=None):
    raw_csv = os.path.join(dataset_root, 'raw', f'{split_name}.csv')
    out_dir = os.path.join(dataset_root, 'processed', split_name)
    if not os.path.exists(raw_csv) and processed_df is None: return

    print(f"\n=== å¤„ç† {split_name} é›† ===")
    os.makedirs(out_dir, exist_ok=True)
    
    if processed_df is not None: df = processed_df
    else: df = flexible_load_trace_csv(raw_csv)

    if df.empty: return

    trace_graphs = df_to_trace_graphs(df=df, id_manager=id_manager, min_node_count=2, max_node_count=100, summary_file=None, merge_spans=False)
    if not trace_graphs: return

    # === æ‰§è¡Œä¸¤é¡¹é¢„è®¡ç®— ===
    # ä¼ å…¥åŒä¸€ä¸ª infra_index
    precompute_host_states(trace_graphs, infra_index, id_manager)    
    precompute_host_sequences(trace_graphs, infra_index, id_manager) 
    # ====================

    db_path = os.path.join(out_dir, "_bytes.db")
    if not os.path.exists(db_path): open(db_path, 'a').close()
    db = TraceGraphDB(BytesSqliteDB(out_dir, write=True))
    try:
        with db.write_batch():
            for graph in trace_graphs:
                if hasattr(graph, 'root_cause') and graph.root_cause is None: graph.root_cause = 0
                if hasattr(graph, 'fault_category') and graph.fault_category is None: graph.fault_category = 0
                db.add(graph)
        db.commit()
        print(f"  âœ… æˆåŠŸå†™å…¥ {len(trace_graphs)} ä¸ªå›¾åˆ° {split_name} æ•°æ®åº“")
    finally:
        db.close()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--root', type=str, default=DEFAULT_DATASET_ROOT)
    args = parser.parse_args()
    
    dataset_root = args.root
    print(f"ğŸš€ å¼€å§‹å¤„ç†æ•°æ®æµ (å•æ–‡ä»¶æ¨¡å¼)ï¼Œæ ¹ç›®å½•: {dataset_root}")
    processed_root = os.path.join(dataset_root, 'processed')
    os.makedirs(processed_root, exist_ok=True)
    
    # 1. åŠ è½½å”¯ä¸€çš„æŒ‡æ ‡æ–‡ä»¶
    print(f"\n[æ­¥éª¤ 0/4] åŠ è½½æŒ‡æ ‡æ–‡ä»¶ {INFRA_FILENAME}...")
    global_infra_index = load_infra_data(dataset_root, INFRA_FILENAME)
    
    # 2. å»ºç«‹ ID æ˜ å°„ (ä¿æŒä¸å˜)
    print("\n[æ­¥éª¤ 1/4] å»ºç«‹ç»Ÿä¸€ ID æ˜ å°„...")
    combined_dfs = []
    for split in ['train', 'val', 'test']:
        path = os.path.join(dataset_root, 'raw', f'{split}.csv')
        df = flexible_load_trace_csv(path)
        if not df.empty: combined_dfs.append(df)
    if not combined_dfs: return
    temp_id_dir = os.path.join(dataset_root, 'temp_ids')
    os.makedirs(temp_id_dir, exist_ok=True)
    id_manager = TraceGraphIDManager(temp_id_dir)
    with id_manager:
        full_df = pd.concat(combined_dfs, ignore_index=True)
        for row in tqdm(full_df.itertuples(), total=len(full_df), desc="ç”Ÿæˆ ID"):
            id_manager.service_id.get_or_assign(getattr(row, 'ServiceName', '') or '')
            id_manager.operation_id.get_or_assign(getattr(row, 'OperationName', '') or '')
            id_manager.status_id.get_or_assign(str(getattr(row, 'StatusCode', '')) or '')
    id_manager.dump_to(processed_root)
    id_manager = TraceGraphIDManager(processed_root)
    if os.path.exists(temp_id_dir): shutil.rmtree(temp_id_dir)

    # 3. å¤„ç†æ•°æ® (Train/Val/Test ç»Ÿä¸€ä½¿ç”¨ global_infra_index)
    process_split('train', dataset_root, id_manager, global_infra_index)
    process_split('val', dataset_root, id_manager, global_infra_index)

    print("\n[æ­¥éª¤ 3/4] å¤„ç†æµ‹è¯•é›†...")
    test_csv_path = os.path.join(dataset_root, 'raw', 'test.csv')
    test_df = flexible_load_trace_csv(test_csv_path)
    if not test_df.empty:
        # ID æ˜ å°„é€»è¾‘...
        for col in ['RootCause', 'FaultCategory']:
            if col not in test_df.columns: test_df[col] = ''
        for idx, row in test_df.iterrows():
            if row.get('Anomaly'):
                rc_text = str(row.get('RootCause', '')).strip()
                fc_text = str(row.get('FaultCategory', '')).strip()
                mapped_id = None
                if fc_text.lower().startswith('node'):
                    rc_text = rc_text.replace('_', '-')
                    mapped_id = id_manager.host_id.get(rc_text)
                else:
                    rc_svc = rc_text.split('-')[0] if '-' in rc_text else rc_text
                    mapped_id = id_manager.service_id.get(rc_svc)
                test_df.at[idx, 'RootCause'] = mapped_id if mapped_id is not None else 0
                test_df.at[idx, 'FaultCategory'] = id_manager.fault_category.get_or_assign(fc_text) if fc_text else 0
        
        process_split('test', dataset_root, id_manager, global_infra_index, processed_df=test_df)

    # 4. [å…³é”®] ä¿å­˜ç´¢å¼•æ–‡ä»¶åˆ°ç£ç›˜ï¼
    if global_infra_index:
        pkl_path = os.path.join(processed_root, 'host_infra_index.pkl')
        print(f"\n[æ­¥éª¤ 4/4] ğŸ’¾ ä¿å­˜æŒ‡æ ‡ç´¢å¼•åˆ° PKL: {pkl_path}")
        try:
            with open(pkl_path, 'wb') as f:
                pickle.dump(global_infra_index, f)
            print("  âœ… ç´¢å¼•ä¿å­˜æˆåŠŸ (è¯„ä¼°è„šæœ¬å¯ä»¥ç›´æ¥è¯»å–äº†)")
        except Exception as e:
            print(f"  âŒ ç´¢å¼•ä¿å­˜å¤±è´¥: {e}")

    id_manager.dump_to(processed_root)
    print(f"\nâœ¨ æ‰€æœ‰å¤„ç†å®Œæˆï¼")

if __name__ == '__main__':
    main()