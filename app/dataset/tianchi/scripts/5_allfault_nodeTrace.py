# -*- coding: utf-8 -*-
"""
èŠ‚ç‚¹çº§æ•…éšœ Trace æå–å·¥å…· (äºŒæ¬¡æ ¡éªŒç‰ˆ)
æ ¸å¿ƒé€»è¾‘ï¼š
1. åˆç­›ï¼šåˆ©ç”¨ SQL Like è¯­å¥ä» SLS æ‹‰å–æ½œåœ¨ Traceã€‚
2. äºŒæ¬¡æ ¡éªŒï¼šåœ¨æœ¬åœ°æ£€æŸ¥æ¯æ¡ Trace çš„æ‰€æœ‰ Spanã€‚
   - è§„åˆ™ï¼šTrace ä¸­è‡³å°‘æœ‰ä¸€ä¸ª Span çš„ NodeName åŒ…å«ç›®æ ‡ IP æˆ–ç­‰äº ECS IDã€‚
   - åŠ¨ä½œï¼šä¸æ»¡è¶³åˆ™ä¸¢å¼ƒã€‚
3. ç»Ÿè®¡ï¼šå®æ—¶è¾“å‡ºä¸¢å¼ƒæ•°é‡ã€‚
"""

import os
import sys
import json
import csv
import time
import argparse
import logging
from collections import defaultdict
from datetime import datetime, timedelta

import config

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ================= ğŸ”§ é‰´æƒé…ç½® =================
os.environ["ALIBABA_CLOUD_ROLE_SESSION_NAME"] = "node-fault-verifier"

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

try:
    from aliyun.log import LogClient, GetLogsRequest
    from alibabacloud_sts20150401.client import Client as StsClient
    from alibabacloud_sts20150401 import models as sts_models
    from alibabacloud_tea_openapi import models as open_api_models
    from tools.paas_entity_tools import umodel_get_entities
except ImportError as e:
    print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# ================= ğŸ”§ åŸºç¡€é…ç½® =================
PROJECT_NAME = config.SLS_PROJECT_NAME
LOGSTORE_NAME = config.SLS_LOGSTORE_NAME
REGION = config.SLS_REGION
OUTPUT_FILENAME = "all_fault_traces.csv"

CSV_HEADERS = [
    'TraceID', 'SpanId', 'ParentID', 
    'ServiceName', 'NodeName', 'PodName', 
    'URL', 'SpanKind', 
    'StartTimeMs', 'EndTimeMs', 'DurationMs',
    'StatusCode', 'HttpStatusCode', 
    'fault_type', 'fault_instance', 'instance_type', 'problem_id'
]

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ================= ğŸ› ï¸ å·¥å…·ç±»å®šä¹‰ =================

class ECSInfoProvider:
    def get_instance_ips(self, instance_id, start_ts, end_ts):
        query_start = start_ts - 3600
        query_end = end_ts
        query = {
            "domain": "acs",
            "entity_set_name": "acs.ecs.instance",
            "from_time": query_start,
            "to_time": query_end,
            "limit": 500
        }
        ips = set()
        try:
            res = umodel_get_entities.invoke(query)
            if not res or not res.data: return []
            for node in res.data:
                if node.get('instance_id') == instance_id:
                    raw_ip = node.get('instance_ip') or node.get('privateIpAddress')
                    if isinstance(raw_ip, list):
                        for ip in raw_ip: ips.add(ip)
                    elif isinstance(raw_ip, str):
                        for ip in raw_ip.split(','):
                            if ip.strip(): ips.add(ip.strip())
        except Exception as e:
            logger.error(f"   âŒ ECS Query Error: {e}")
        return list(ips)

class AutoRefreshSLSClient:
    def __init__(self, region=REGION):
        self.region = region
        self.client = None
        self._refresh_client()
    
    def _refresh_client(self):
        config = open_api_models.Config(
            access_key_id=os.environ["ALIBABA_CLOUD_ACCESS_KEY_ID"],
            access_key_secret=os.environ["ALIBABA_CLOUD_ACCESS_KEY_SECRET"],
            endpoint=f'sts.{self.region}.aliyuncs.com'
        )
        sts_client = StsClient(config)
        resp = sts_client.assume_role(sts_models.AssumeRoleRequest(
            role_arn=os.environ["ALIBABA_CLOUD_ROLE_ARN"],
            role_session_name=os.environ["ALIBABA_CLOUD_ROLE_SESSION_NAME"],
            duration_seconds=3600
        ))
        creds = resp.body.credentials
        self.client = LogClient(
            endpoint=f"{self.region}.log.aliyuncs.com",
            accessKeyId=creds.access_key_id,
            accessKey=creds.access_key_secret,
            securityToken=creds.security_token
        )
    
    def get_logs(self, req):
        try:
            return self.client.get_logs(req)
        except Exception:
            self._refresh_client()
            return self.client.get_logs(req)

def safe_json_load(text):
    if not text: return {}
    try: return json.loads(text)
    except: return {}

class TraceExtractor:
    def __init__(self):
        self.client = AutoRefreshSLSClient()

    def find_trace_ids(self, query, start_ts, end_ts, limit):
        logger.info(f"      ğŸ” Query: {query}")
        trace_ids = set()
        offset = 0
        while len(trace_ids) < limit:
            req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=query, fromTime=start_ts, toTime=end_ts, line=100, offset=offset)
            res = self.client.get_logs(req)
            if not res or not res.get_logs(): break
            
            logs = res.get_logs()
            for log in logs:
                tid = log.get_contents().get('traceId')
                if tid: trace_ids.add(tid)
            
            offset += len(logs)
            if len(logs) < 100: break
            
        # logger.info(f"      âœ… Found IDs: {len(trace_ids)}") # ç§»åˆ°å¤–å±‚ç»Ÿä¸€æ‰“å°
        return list(trace_ids)[:limit]

    def fetch_and_verify_traces(self, trace_ids, start_ts, end_ts, meta, writer, existing_ids, target_ips):
        """
        æ‹‰å–è¯¦æƒ… -> æœ¬åœ°æ ¡éªŒ -> å†™å…¥ CSV
        è¿”å›: (saved_count, discarded_count)
        """
        # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥çš„ trace_ids å·²ç»æ˜¯å‰”é™¤è¿‡å·²å­˜åœ¨ ID çš„åˆ—è¡¨äº†ï¼ˆåœ¨ process_single_row é‡Œå¤„ç†ï¼‰
        new_ids = trace_ids 
        if not new_ids: return 0, 0
        
        target_instance = meta['fault_instance']
        
        saved_count = 0
        discarded_count = 0
        batch_size = 20
        
        for i in range(0, len(new_ids), batch_size):
            batch = new_ids[i:i+batch_size]
            or_query = " OR ".join([f'traceId: "{tid}"' for tid in batch])
            
            batch_buffer = defaultdict(list)
            
            offset = 0
            while True:
                req = GetLogsRequest(PROJECT_NAME, LOGSTORE_NAME, query=or_query, fromTime=start_ts, toTime=end_ts, line=100, offset=offset)
                res = self.client.get_logs(req)
                if not res or not res.get_logs(): break
                
                logs = res.get_logs()
                for log in logs:
                    d = log.get_contents()
                    tid = d.get('traceId')
                    if not tid: continue

                    res_obj = safe_json_load(d.get('resources', '{}'))
                    attr_obj = safe_json_load(d.get('attributes', '{}'))
                    
                    node_name = res_obj.get('k8s.node.name', '')
                    
                    try:
                        s_ms = int(d.get('startTime', 0)) / 1e6
                        d_ms = int(d.get('duration', 0)) / 1e6
                    except: s_ms, d_ms = 0, 0

                    row = {
                        'TraceID': tid,
                        'SpanId': d.get('spanId', ''),
                        'ParentID': d.get('parentSpanId', ''),
                        'ServiceName': d.get('serviceName', ''),
                        'NodeName': node_name,  
                        'PodName': res_obj.get('k8s.pod.name', ''),
                        'URL': d.get('spanName', ''),
                        'SpanKind': d.get('kind', ''),
                        'StartTimeMs': f"{s_ms:.3f}",
                        'EndTimeMs': f"{s_ms + d_ms:.3f}",
                        'DurationMs': f"{d_ms:.3f}",
                        'StatusCode': d.get('statusCode', ''),
                        'HttpStatusCode': str(attr_obj.get('http.status_code') or attr_obj.get('rpc.grpc.status_code', '')),
                        'fault_type': meta['fault_type'],
                        'fault_instance': meta['fault_instance'],
                        'instance_type': meta['instance_type'],
                        'problem_id': meta['problem_id']
                    }
                    batch_buffer[tid].append(row)
                
                offset += len(logs)
                if len(logs) < 100: break
            
            # æœ¬åœ°äºŒæ¬¡æ ¡éªŒ
            for tid in batch:
                spans = batch_buffer.get(tid, [])
                if not spans: continue

                is_valid = False
                for span in spans:
                    n_name = span['NodeName']
                    if not n_name: continue
                    
                    if n_name == target_instance:
                        is_valid = True
                        break
                    
                    for ip in target_ips:
                        if ip in n_name:
                            is_valid = True
                            break
                    if is_valid: break
                
                if is_valid:
                    writer.writerows(spans)
                    existing_ids.add(tid)
                    saved_count += 1
                else:
                    discarded_count += 1
            
            print(f"      â³ Batch Processed: {min(i+batch_size, len(new_ids))}/{len(new_ids)} (Discarded so far: {discarded_count})...", end='\r')
            
        print("") 
        return saved_count, discarded_count

class NodeFaultProcessor:
    def __init__(self, args):
        self.args = args
        self.extractor = TraceExtractor()
        self.ecs_provider = ECSInfoProvider()
        self.existing_ids = set()
        self.out_path = os.path.join(args.output_dir, OUTPUT_FILENAME)
        
        if os.path.exists(self.out_path):
            with open(self.out_path, 'r', encoding='utf-8') as f:
                for row in csv.DictReader(f):
                    if row.get('TraceID'): self.existing_ids.add(row['TraceID'])
            logger.info(f"ğŸ“š Loaded {len(self.existing_ids)} existing TraceIDs from local file")

    def process(self):
        file_exists = os.path.exists(self.out_path)
        with open(self.out_path, 'a', newline='', encoding='utf-8') as f_out:
            writer = csv.DictWriter(f_out, fieldnames=CSV_HEADERS)
            if not file_exists:
                writer.writeheader()
            
            with open(self.args.csv, 'r', encoding='utf-8') as f_in:
                rows = list(csv.DictReader(f_in))
                if self.args.range:
                    s, e = map(int, self.args.range.split(','))
                    rows = [r for r in rows if s <= int(r['problem_id']) <= e]
                
                total_discarded = 0
                for row in rows:
                    total_discarded += self._process_single_row(row, writer)
                
                logger.info(f"\nğŸ›‘ All Done. Total Traces Discarded: {total_discarded}")

    def _process_single_row(self, row, writer):
        pid = row['problem_id']
        i_type = row['instance_type']
        instance = row['instance']
        
        if i_type != 'node': return 0

        logger.info(f"\nğŸš€ [Problem {pid}] Node Fault | Instance: {instance}")
        try:
            s_dt = datetime.strptime(row['start_time'], "%Y-%m-%d %H:%M:%S")
            e_dt = datetime.strptime(row['end_time'], "%Y-%m-%d %H:%M:%S")
            s_ts = int((s_dt + timedelta(seconds=self.args.buffer)).timestamp())
            e_ts = int((e_dt - timedelta(seconds=self.args.buffer)).timestamp())
            fetch_s_ts = int(s_dt.timestamp())
            fetch_e_ts = int(e_dt.timestamp())
        except:
            logger.error("   âŒ Time Format Error")
            return 0

        # 1. è·å– IPs
        logger.info(f"   ğŸ” Fetching IPs for validation...")
        ips = self.ecs_provider.get_instance_ips(instance, s_ts, e_ts)
        
        # 2. æ„é€ æŸ¥è¯¢
        conditions = [f"resources like '%{instance}%'"]
        if ips:
            logger.info(f"   âœ… Valid IPs for Check: {ips}")
            for ip in ips:
                node_target = f"cn-qingdao.{ip}"
                kv_compact = f'\\"k8s.node.name\\":\\"{node_target}\\"'
                conditions.append(f"resources like '%{kv_compact}%'")
                conditions.append(f"resources like '%{node_target}%'")
        else:
            logger.warning(f"   âš ï¸ No IPs found. Strict check will only use InstanceID.")

        query = "* | where " + " OR ".join(conditions)

        # 3. è·å– TraceID List
        tids = self.extractor.find_trace_ids(query, s_ts, e_ts, self.args.limit)
        
        # ==========================================
        # ğŸ”¥ æ–°å¢ï¼šæ¸…æ™°çš„ç»Ÿè®¡æ—¥å¿— ğŸ”¥
        # ==========================================
        num_found = len(tids)
        # è®¡ç®—å“ªäº›æ˜¯æ–°çš„
        new_tids = [t for t in tids if t not in self.existing_ids]
        num_new = len(new_tids)
        num_existing = num_found - num_new
        
        logger.info(f"   ğŸ“Š ç»Ÿè®¡: äº‘ç«¯å‘½ä¸­ {num_found} æ¡ | æœ¬åœ°å·²å­˜ {num_existing} æ¡ | â¬‡ï¸ å¾…ä¸‹è½½ {num_new} æ¡")
        # ==========================================

        meta = {
            'fault_type': row['fault_type'],
            'fault_instance': instance,
            'instance_type': i_type,
            'problem_id': pid
        }
        
        if new_tids:
            # ç›´æ¥ä¼ å…¥ new_tidsï¼Œé¿å…å‡½æ•°å†…å†æ¬¡é‡å¤è®¡ç®—
            saved, discarded = self.extractor.fetch_and_verify_traces(
                new_tids, fetch_s_ts, fetch_e_ts, meta, writer, self.existing_ids, ips
            )
            logger.info(f"      ğŸ“‰ æœ¬æ‰¹æ¬¡ç»“æœ: å…¥åº“ {saved} æ¡, æ ¡éªŒä¸¢å¼ƒ {discarded} æ¡")
            return discarded
        else:
            logger.info(f"      âœ¨ æ‰€æœ‰æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
            return 0

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv", default="dataset/b_gt.csv", help="b_gt.csv path")
    parser.add_argument("--output-dir", default="data/NodeFault", help="output directory")
    parser.add_argument("--limit", type=int, default=20000)
    parser.add_argument("--buffer", type=int, default=60)
    parser.add_argument("--range", help="Problem ID range (e.g. 1,100)")
    args = parser.parse_args()
    
    os.makedirs(args.output_dir, exist_ok=True)
    NodeFaultProcessor(args).process()

if __name__ == "__main__":
    main()